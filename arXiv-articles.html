<a href='http://arxiv.org/pdf/1911.08265.pdf'>1911.08265</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 32.0201баллов, №1</br>
<b>Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model</b></br>
Authors: , Schrittwieser, Julian, Antonoglou, Ioannis, Hubert, Thomas, <font color="red">Simonyan, Ka</font>ren, Sifre, Laurent, Schmitt, Simon, Guez, Arthur, Lockhart, Edward, <font color="red">Hassabis</font>, Demis, Graepel, Thore, <font color="red">Lillicrap</font>, Timothy, Silver, David</br>
  Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in <font color="#009600">real-world</font> problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new <font color="red">state of the art</font>. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the <font color="red">AlphaZero</font> algorithm that was supplied with the game rules. </br></br>

<a href='http://arxiv.org/pdf/1911.09665.pdf'>1911.09665</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 21.9689баллов, №2</br>
<b>Adversarial Examples Improve Image Recognition</b></br>
Authors: , Xie, Cihang, Tan, Mingxing, Gong, Boqing, Wang, Jiang, <font color="red">Yuille</font>, Alan, Le, <font color="red">Quoc</font> V.</br>
  Adversarial examples are commonly viewed as a threat to ConvNets. Here we present an opposite perspective: adversarial examples can be used to improve image recognition models if harnessed in the right manner. We propose AdvProp, an enhanced adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to our method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples.   We show that AdvProp improves a wide range of models on various image recognition tasks and performs better when the models are bigger. For instance, by applying AdvProp to the latest EfficientNet-B7 [28] on ImageNet, we achieve significant improvements on ImageNet (+0.7%), ImageNet-C (+6.5%), ImageNet-A (+7.0%), Stylized-ImageNet (+4.8%). With an enhanced EfficientNet-B8, our method achieves the <font color="red">state-of-the-art</font> 85.5% ImageNet top-1 accuracy without extra data. This result even surpasses the best model in [20] which is trained with 3.5B Instagram images (~3000X more than ImageNet) and ~9.4X more parameters. Models are available at <font color="#006400">http</font>s://<font color="red">github</font>.com/tensorflow/tpu/tree/master/models/official/efficientnet. </br></br>

<a href='http://arxiv.org/pdf/1911.08571.pdf'>1911.08571</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 10.9180баллов, №3</br>
<b>Localizing Occluders with Compositional Convolutional Networks</b></br>
Authors: , Kortylewski, Adam, Liu, Qing, Wang, Huiyu, Zhang, Zhishuai, <font color="red">Yuille</font>, Alan</br>
  Compositional convolutional networks are generative compositional models of neural network features, that achieve <font color="red">state of the art</font> results when classifying partially occluded objects, even when they have not been exposed to occluded objects during training. In this work, we study the performance of CompositionalNets at localizing occluders in images. We show that the original model is not able to localize occluders well. We propose to overcome this limitation by modeling the feature activations as a mixture of von-Mises-Fisher distributions, which also allows for an end-to-end training of CompositionalNets. Our experimental results demonstrate that the proposed extensions increase the model\'s performance at localizing occluders as well as at classifying partially occluded objects. </br></br>

<a href='http://arxiv.org/pdf/1911.06930.pdf'>1911.06930</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp 10.4971баллов, №4</br>
<b>Inverse Reinforcement Learning with Missing Data</b></br>
Authors: , Mai, Tien, Nguyen, <font color="red">Quoc</font> Phong, Low, Kian Hsiang, Jaillet, Patrick</br>
  We consider the problem of recovering an expert\'s reward function with inverse <font color="#00be00">reinforcement learning</font> (IRL) when there are missing/incomplete state-action pairs or observations in the demonstrated trajectories. This issue of missing trajectory data or information occurs in many situations, e.g., GPS signals from vehicles moving on a road network are intermittent. In this paper, we propose a tractable approach to directly compute the log-likelihood of demonstrated trajectories with incomplete/missing data. Our algorithm is efficient in handling a large number of missing segments in the demonstrated trajectories, as it performs the training with incomplete data by solving a sequence of systems of linear equations, and the number of such systems to be solved does not depend on the number of missing segments. Empirical evaluation on a <font color="#009600">real-world</font> dataset shows that our training algorithm <font color="#00be00">outperform</font>s other conventional techniques. </br></br>

<a href='http://arxiv.org/pdf/1911.09539.pdf'>1911.09539</a> &nbsp&nbsp (cs:AI, stat:ML) &nbsp&nbsp 10.4694баллов, №5</br>
<b>Neural Large Neighborhood Search for the Capacitated Vehicle Routing\n  Problem</b></br>
Authors: , Hottu<font color="red">ng, Andr</font>&#xe9;, Tierney, Kevin</br>
  Learning how to automatically solve optimization problems has the potential to provide the next big leap in optimization technology. The performance of automatically learned heuristics on routing problems has been steadily improving in recent years, but approaches based purely on machine learning are still <font color="#00be00">outperform</font>ed by <font color="red">state-of-the-art</font> optimization methods. To close this performance gap, we propose a novel large neighborhood search (LNS) framework for vehicle routing that integrates learned heuristics for generating new solutions. The learning mechanism is based on a deep neural network with an attention mechanism and has been especially designed to be integrated into an LNS search setting. We evaluate our approach on the capacitated vehicle routing problem (CVRP) and the split delivery vehicle routing problem (SDVRP). On CVRP instances with up to 297 <font color="#be00be">customer</font>s our approach significantly outperforms an LNS that uses only handcrafted heuristics and a well-known heuristic from the literature. Furthermore, we show for the CVRP and the SDVRP that our approach surpasses the performance of existing machine learning approaches and comes close to the performance of state-of-the-art optimization approaches. </br></br>

<a href='http://arxiv.org/pdf/1911.08453.pdf'>1911.08453</a> &nbsp&nbsp (cs:ML, cs:AI, cs:RO, stat:ML) &nbsp&nbsp 10.4132баллов, №6</br>
<b>Planning with Goal-Conditioned Policies</b></br>
Authors: , Nasiriany, Soroush, Pong, Vitchyr H., Lin, Steven, <font color="red">Levine, Sergey</font></br>
  Planning methods can solve temporally extended sequential decision making problems by composing simple behaviors. However, planning requires suitable abstractions for the states and transitions, which typically need to be designed by hand. In contrast, model-free <font color="#00be00">reinforcement learning</font> (RL) can acquire behaviors from low-level inputs directly, but often struggles with temporally extended tasks. Can we utilize reinforcement learning to automatically form the abstractions needed for planning, thus obtaining the best of both approaches? We show that goal-conditioned policies learned with RL can be incorporated into planning, so that a planner can focus on which states to reach, rather than how those states are reached. However, with complex state observations such as images, not all inputs represent valid states. We therefore also propose using a latent variable model to compactly represent the set of valid states for the planner, so that the policies provide an abstraction of actions, and the latent variable model provides an abstraction of states. We compare our method with planning-based and model-free methods and find that our method significantly <font color="#00be00">outperform</font>s prior work when evaluated on image-based robot navigation and manipulation tasks that require non-greedy, multi-staged behavior. </br></br>

<a href='http://arxiv.org/pdf/1911.07421.pdf'>1911.07421</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 10.1447баллов, №7</br>
<b>Deep Verifier Networks: Verification of Deep Discriminative Models with\n  Deep Generative Models</b></br>
Authors: , Che, Tong, Liu, Xiaofeng, Li, Site, Ge, Yubin, Zhang, Ruixiang, Xiong, Caiming, <font color="red">Bengio, Yoshua</font></br>
  AI Safety is a major concern in many deep learning applications such as autonomous driving. Given a trained deep learning model, an important natural problem is how to reliably verify the model\'s prediction. In this paper, we propose a novel framework --- deep verifier networks (DVN) to verify the inputs and outputs of deep discriminative models with deep generative models. Our proposed model is based on conditional variational auto-encoders with disentanglement constraints. We give both intuitive and <font color="blue">theor</font>etical justifications of the model. Our verifier network is trained independently with the prediction model, which eliminates the need of retraining the verifier network for a new model. We test the verifier network on out-of-distribution detection and adversarial example detection problems, as well as <font color="#be00be">anomal</font>y detection problems in structured prediction tasks such as image caption generation. We achieve <font color="red">state-of-the-art</font> results in all of these problems. </br></br>

<a href='http://arxiv.org/pdf/1911.08585.pdf'>1911.08585</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 10.0842баллов, №8</br>
<b>Ghost Units Yield Biologically Plausible Backprop in Deep Neural\n  Networks</b></br>
Authors: , Mesnard, Thomas, Vignoud, Gaetan, Sacramento, Joao, Senn, Walter, <font color="red">Bengio, Yoshua</font></br>
  In the past few years, deep learning has transformed artificial intelligence research and led to impressive performance in various difficult tasks. However, it is still unclear how the <font color="#00be00">brain</font> can perform credit assignment across many areas as efficiently as backpropagation does in deep neural networks. In this paper, we introduce a model that relies on a new role for a neuronal inhibitory machinery, referred to as ghost units. By cancelling the feedback coming from the upper layer when no target signal is provided to the top layer, the ghost units enables the network to backpropagate errors and do efficient credit assignment in deep structures. While considering one-compartment neurons and requiring very few biological assumptions, it is able to approximate the error gradient and achieve good performance on classification tasks. Error backpropagation occurs through the recurrent dynamics of the network and thanks to biologically plausible local learning rules. In particular, it does not require separate feedforward and feedback circuits. Different mechanisms for cancelling the feedback were studied, ranging from complete duplication of the connectivity by long term processes to online replication of the feedback activity. This reduced system combines the essential elements to have a working biologically abstracted analogue of backpropagation with a simple formulation and proofs of the associated results. Therefore, this model is a step towards understanding how learning and memory are implemented in cortical multilayer structures, but it also raises interesting perspectives for neuromorphic hardware. </br></br>

<a href='http://arxiv.org/pdf/1911.09194.pdf'>1911.09194</a> &nbsp&nbsp (cs:AI, cs:CL, cs:ML) &nbsp&nbsp 10.0116баллов, №9</br>
<b>Generating Interactive Worlds with Text</b></br>
Authors: , Fan, Angela, Urbanek, Jack, Ringshia, Pratik, Dinan, Emily, Qian, Emma, Karamcheti, Siddharth, Prabhumoye, Shrimai, Kiela, Douwe, Rocktaschel, Tim, Szlam, Arthur, <font color="red">Weston, Jason</font></br>
  Procedurally generating cohesive and interesting game environments is challenging and time-consuming. In order for the relationships between the game elements to be natural, common-sense has to be encoded into arrangement of the elements. In this work, we investigate a machine learning approach for world creation using content from the multi-player text adventure game environment LIGHT. We introduce neural network based models to compositionally arrange locations, characters, and objects into a coherent whole. In addition to creating worlds based on existing elements, our models can generate new game content. Humans can also leverage our models to interactively aid in worldbuilding. We show that the game environments created with our approach are cohesive, diverse, and preferred by human evaluators compared to other machine learning based world construction algorithms. </br></br>

<a href='http://arxiv.org/pdf/1911.08795.pdf'>1911.08795</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 9.7749баллов, №10</br>
<b>On Node Features for Graph Neural Networks</b></br>
Authors: , Duong, Chi Thang, Hoang, Thanh Dat, Dang, Ha The Hien, Nguyen, <font color="red">Quoc</font> Viet Hung, Aberer, Karl</br>
  Graph neural network (GNN) is a deep model for graph representation learning. One advantage of graph neural network is its ability to incorporate node features into the learning process. However, this prevents graph neural network from being applied into featureless graphs. In this paper, we first analyze the effects of node features on the performance of graph neural network. We show that GNNs work well if there is a strong correlation between node features and node labels. Based on these results, we propose new feature initialization methods that allows to apply graph neural network to non-attributed graphs. Our experimental results show that the artificial features are highly <font color="#960096">competitive</font> with real features. </br></br>

<a href='http://arxiv.org/pdf/1911.02752.pdf'>1911.02752</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 9.6245баллов, №11</br>
<b>Sequence-Aware Factorization Machines for Temporal Predictive Analytics</b></br>
Authors: , Chen, Tong, Yin, Hongzhi, Nguyen, <font color="red">Quoc</font> Viet Hung, Peng, Wen-Chih, Li, Xue, Zhou, Xiaofang</br>
  In various web applications like targeted advertising and recommender systems, the available categorical features (e.g., product type) are often of great importance but sparse. As a widely adopted solution, models based on Factorization Machines (FMs) are capable of modelling high-order interactions among features for effective sparse predictive analytics. As the volume of web-scale data grows exponentially over time, sparse predictive analytics inevitably involves dynamic and sequential features. However, existing FM-based models assume no temporal orders in the data, and are unable to capture the sequential dependencies or patterns within the dynamic features, impeding the performance and adaptivity of these methods. Hence, in this paper, we propose a novel Sequence-Aware Factorization Machine (SeqFM) for temporal predictive analytics, which models feature interactions by fully investigating the effect of sequential dependencies. As static features (e.g., user gender) and dynamic features (e.g., user interacted items) express different semantics, we innovatively devise a multi-view self-attention scheme that separately models the effect of static features, dynamic features and the mutual interactions between static and dynamic features in three different views. In SeqFM, we further map the learned representations of feature interactions to the desired output with a shared residual network. To showcase the versatility and generalizability of SeqFM, we test SeqFM in three popular application scenarios for FM-based models, namely ranking, classification and <font color="#be00be">regression</font> tasks. Extensive experimental results on six large-scale datasets demonstrate the superior effectiveness and efficiency of SeqFM. </br></br>

<a href='http://arxiv.org/pdf/1911.09070.pdf'>1911.09070</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 9.5910баллов, №12</br>
<b>EfficientDet: Scalable and Efficient Object Detection</b></br>
Authors: , Tan, Mingxing, Pang, Ruoming, Le, <font color="red">Quoc</font> V.</br>
  Model efficiency has become increasingly important in computer vision. In this paper, we systematically study various neural network architecture design choices for <font color="#be00be">object detection</font> and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations, we have developed a new family of object detectors, called EfficientDet, which consistently achieve an order-of-magnitude better efficiency than prior art across a wide spectrum of resource constraints. In particular, without bells and whistles, our EfficientDet-D7 achieves stateof-the-art 51.0 mAP on COCO dataset with 52M parameters and 326B FLOPS1 , being 4x smaller and using 9.3x fewer FLOPS yet still more accurate (+0.3% mAP) than the best previous detector. </br></br>

<a href='http://arxiv.org/pdf/1911.08905.pdf'>1911.08905</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 8.9520баллов, №13</br>
<b>FeCaffe: FPGA-enabled Caffe with OpenCL for Deep Learning Training and\n  Inference on Intel Stratix 10</b></br>
Authors: , He, Ke, Liu, Bo, Zhang, Yu, Li<font color="red">ng, Andr</font>ew, Gu, Dian</br>
  Deep learning and Convolutional Neural Network (CNN) have becoming increasingly more popular and important in both academic and industrial areas in recent years cause they are able to provide better accuracy and result in classification, detection and recognition areas, compared to traditional approaches. Currently, there are many popular frameworks in the <font color="#be00be">market</font> for deep learning development, such as Caffe, TensorFlow, Pytorch, and most of frameworks natively support CPU and consider GPU as the mainline accelerator by default. <font color="#be00be">FPGA</font> device, viewed as a potential heterogeneous platform, still cannot provide a comprehensive support for CNN development in popular frameworks, in particular to the training phase. In this paper, we firstly propose the FeCaffe, i.e. FPGA-enabled Caffe, a <font color="#00be00">hierarchical</font> software and hardware design methodology based on the Caffe to enable FPGA to support mainline deep learning development features, e.g. training and inference with Caffe. Furthermore, we provide some benchmarks with FeCaffe by taking some classical CNN networks as examples, and further analysis of <font color="blue">kernel</font> execution time in details accordingly. Finally, some optimization directions including FPGA kernel design, system pipeline, network architecture, user case application and heterogeneous platform levels, have been proposed gradually to improve FeCaffe performance and efficiency. The result demonstrates the proposed FeCaffe is capable of supporting almost full features during CNN network training and inference respectively with high degree of design flexibility, expansibility and reusability for deep learning development. Compared to prior studies, our architecture can support more network and training settings, and current configuration can achieve 6.4x and 8.4x average execution time improvement for forward and backward respectively for LeNet. </br></br>

<a href='http://arxiv.org/pdf/1911.08098.pdf'>1911.08098</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 2.9711баллов, №14</br>
<b>HighEr-Resolution Network for Image Demosaicing and Enhancing</b></br>
Authors: , Mei, Kangfu, Li, Juncheng, Zhang, Jiajie, Wu, Haoyu, Li, Jie, Huang, Rui</br>
  Neural-networks based image restoration methods tend to use low-resolution image patches for training. Although higher-resolution image patches can provide more global information, <font color="red">state-of-the-art</font> methods cannot utilize them due to their huge GPU memory usage, as well as the instable training process. However, plenty of studies have shown that global information is crucial for image restoration tasks like image demosaicing and enhancing. In this work, we propose a HighEr-Resolution Network (HERN) to fully learning global information in high-resolution image patches. To achieve this, the HERN employs two parallel paths to learn image features in two different resolutions, respectively. By combining global-aware features and multi-scale features, our HERN is able to learn global information with feasible GPU memory usage. Besides, we introduce a progressive training method to solve the instability issue and accelerate model convergence. On the task of image demosaicing and enhancing, our HERN achieves state-of-the-art performance on the AIM2019 RAW to RGB mapping challenge. The <font color="red">source code</font> of our implementation is available at <font color="#006400">http</font>s://<font color="red">github</font>.com/MKFMIKU/RAW2RGBNet. </br></br>

<a href='http://arxiv.org/pdf/1911.08287.pdf'>1911.08287</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 2.4505баллов, №15</br>
<b>Distance-IoU Loss: Faster and Better Learning for Bounding Box\n  Regression</b></br>
Authors: , Zheng, Zhaohui, Wang, Ping, Liu, Wei, Li, Jinze, Ye, Rongguang, Ren, Dongwei</br>
  Bounding box <font color="#be00be">regression</font> is the crucial step in <font color="#be00be">object detection</font>. In existing methods, while $\\ell_n$-norm loss is widely adopted for bounding box regression, it is not tailored to the evaluation metric, i.e., Intersection over Union (IoU). Recently, IoU loss and generalized IoU (GIoU) loss have been proposed to benefit the IoU metric, but still suffer from the problems of slow convergence and inaccurate regression. In this paper, we propose a Distance-IoU (DIoU) loss by incorporating the normalized distance between the predicted box and the target box, which converges much faster in training than IoU and GIoU losses. Furthermore, this paper summarizes three geometric factors in bounding box regression, \\ie, overlap area, central point distance and aspect ratio, based on which a Complete IoU (CIoU) loss is proposed, thereby leading to faster convergence and better performance. By incorporating DIoU and CIoU losses into <font color="red">state-of-the-art</font> object detection algorithms, e.g., YOLO v3, SSD and Faster RCNN, we achieve notable performance gains in terms of not only IoU metric but also GIoU metric. Moreover, DIoU can be easily adopted into non-maximum suppression (NMS) to act as the criterion, further boosting performance improvement. The <font color="red">source code</font> and trained models are available at <font color="#006400">http</font>s://<font color="red">github</font>.com/Zzh-tju/DIoU. </br></br>

<a href='http://arxiv.org/pdf/1911.08718.pdf'>1911.08718</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 2.3297баллов, №16</br>
<b>Towards Ghost-free Shadow Removal via Dual Hierarchical Aggregation\n  Network and Shadow Matting GAN</b></br>
Authors: , Cun, Xiaodong, Pun, Chi-Man, Shi, Cheng</br>
  Shadow removal is an essential task for scene understanding. Many studies consider only matching the image contents, which often causes two types of ghosts: color in-consistencies in shadow regions or artifacts on shadow boundaries. In this paper, we tackle these issues in two ways. First, to carefully learn the border artifacts-free image, we propose a novel network structure named the dual <font color="#00be00">hierarchical</font>ly aggregation network~(DHAN). It contains a series of growth dilated convolutions as the backbone without any down-samplings, and we hierarchically aggregate multi-context features for attention and prediction, respectively. Second, we argue that training on a limited dataset restricts the textural understanding of the network, which leads to the shadow region color in-consistencies. Currently, the largest dataset contains 2k+ shadow/shadow-free image pairs. However, it has only 0.1k+ unique scenes since many samples share exactly the same background with different shadow positions. Thus, we design a shadow matting generative adversarial network~(SMGAN) to synthesize realistic shadow mattings from a given shadow mask and shadow-free image. With the help of novel masks or scenes, we enhance the current datasets using synthesized shadow images. Experiments show that our DHAN can erase the shadows and produce high-quality ghost-free images. After training on the synthesized and real datasets, our network <font color="#00be00">outperform</font>s other <font color="red">state-of-the-art</font> methods by a large margin. The code is available: <font color="#006400">http</font>://<font color="red">github</font>.com/vinthony/ghost-free-shadow-removal/ </br></br>

<a href='http://arxiv.org/pdf/1911.09280.pdf'>1911.09280</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp 2.2773баллов, №17</br>
<b>Integrated Motion Planner for Real-time Aerial Videography with a Drone\n  in a Dense Environment</b></br>
Authors: , Jeon, Boseong, Kim, H. Jin</br>
  This letter suggests an integrated approach for a drone (or multirotor) to perform an autonomous videography task in a 3-D obstacle environment by following a moving object. The proposed system includes 1) a target motion prediction module which can be applied to dense environments and 2) a <font color="#00be00">hierarchical</font> chasing planner based on a proposed metric for visibility. In the prediction module, we minimize observation error given that the target object itself does not collide with obstacles. The estimated future trajectory of target is obtained by covariant optimization. The other module, chasing planner, is in a bi-level structure composed of preplanner and smooth planner. In the first phase, we leverage a graph-search method to preplan a chasing corridor which incorporates safety and visibility of target during a time window. In the subsequent phase, we generate a smooth and dynamically feasible path within the corridor using quadratic programming (QP). We validate our approach with multiple complex scenarios and actual experiments. The <font color="red">source code</font> can be found in <font color="#006400">http</font>s://<font color="red">github</font>.com/icsl-Jeon/traj_gen_vis </br></br>

<a href='http://arxiv.org/pdf/1911.08511.pdf'>1911.08511</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 2.1748баллов, №18</br>
<b>Action Recognition Using Volumetric Motion Representations</b></br>
Authors: , Peven, Michael, Hager, Gregory D., Reiter, Austin</br>
  Traditional action recognition models are constructed around the paradigm of 2D perspective imagery. Though sophisticated time-series models have pushed the field forward, much of the information is still not exploited by confining the domain to 2D. In this work, we introduce a novel representation of motion as a voxelized 3D vector field and demonstrate how it can be used to improve performance of action recognition networks. This volumetric representation is a natural fit for 3D CNNs, and allows out-of-plane data augmentation techniques during training of these networks. Both the construction of this representation from RGB-D video and inference can be run in real time. We demonstrate superior results using this representation with our network design on the open-source NTU RGB+D dataset where it <font color="#00be00">outperform</font>s <font color="red">state-of-the-art</font> on both of the defined evaluation metrics. Furthermore, we experimentally show how the out-of-plane augmentation techniques create viewpoint invariance and allow the model trained using this representation to generalize to unseen camera angles. Code is available here: <font color="#006400">http</font>s://<font color="red">github</font>.com/mpeven/ntu_rgb. </br></br>

<a href='http://arxiv.org/pdf/1911.09042.pdf'>1911.09042</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 2.1643баллов, №19</br>
<b>Learning Cross-modal Context Graph for Visual Grounding</b></br>
Authors: , Liu, Yongfei, Wan, Bo, Zhu, Xiaodan, He, Xuming</br>
  Visual grounding is a ubiquitous building block in many vision-language tasks and yet remains challenging due to large variations in visual and linguistic features of grounding entities, strong context effect and the resulting semantic ambiguities. Prior works typically focus on learning representations of individual phrases with limited context information. To address their limitations, this paper proposes a language-guided graph representation to capture the global context of grounding entities and their relations, and develop a cross-modal graph matching strategy for the multiple-phrase visual grounding task. In particular, we introduce a modular graph neural network to compute context-aware representations of phrases and object proposals respectively via message propagation, followed by a graph-based matching module to generate globally consistent localization of grounding phrases. We train the entire graph neural network jointly in a two-stage strategy and evaluate it on the Flickr30K Entities benchmark. Extensive experiments show that our method <font color="#00be00">outperform</font>s the prior <font color="red">state of the art</font>s by a sizable margin, evidencing the efficacy of our grounding framework. Code is available at <font color="#006400">http</font>s://<font color="red">github</font>.com/youngfly11/LCMCG-PyTorch. </br></br>

<a href='http://arxiv.org/pdf/1910.02339.pdf'>1910.02339</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp 2.1632баллов, №20</br>
<b>Natural- to formal-language generation using Tensor Product\n  Representations</b></br>
Authors: , Chen, Kezhen, Huang, Qiuyuan, Palangi, Hamid, Smolensky, Paul, Forbus, Kenneth D., Gao, Jianfeng</br>
  Generating formal-language represented by relational tuples, such as Lisp programs or mathematical expressions, from a natural-language input is an extremely challenging task because it requires to explicitly capture discrete symbolic structural information from the input to generate the output. Most <font color="red">state-of-the-art</font> neural sequence models do not explicitly capture such structure information, and thus do not perform well on these tasks. In this paper, we propose a new encoder-decoder model based on Tensor Product Representations (TPRs) for Natural- to Formal-language generation, called TP-N2F. The encoder of TP-N2F employs TPR \'binding\' to encode natural-language symbolic structure in vector space and the decoder uses TPR \'unbinding\' to generate a sequence of relational tuples, each consisting of a relation (or operation) and a number of arguments, in symbolic space. TP-N2F considerably <font color="#00be00">outperform</font>s LSTM-based Seq2Seq models, creating a new <font color="red">state of the art</font> results on two benchmarks: the MathQA dataset for math problem solving, and the AlgoList dataset for program synthesis. Ablation studies show that improvements are mainly attributed to the use of TPRs in both the encoder and decoder to explicitly capture relational structure information for symbolic reasoning. </br></br>

<a href='http://arxiv.org/pdf/1911.09265.pdf'>1911.09265</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 2.1024баллов, №21</br>
<b>EnAET: Self-Trained Ensemble AutoEncoding Transformations for\n  Semi-Supervised Learning</b></br>
Authors: , Wang, Xiao, Kihara, Daisuke, Luo, Jiebo, Qi, Guo-Jun</br>
  Deep neural networks have been successfully applied to many <font color="#009600">real-world</font> applications. However, these successes rely heavily on large amounts of labeled data, which is expensive to obtain. Recently, Auto-Encoding Transformation (AET) and MixMatch have been proposed and achieved <font color="red">state-of-the-art</font> results for unsupervised and semi-supervised learning, respectively. In this study, we train an Ensemble of Auto-Encoding Transformations (EnAET) to learn from both labeled and unlabeled data based on the embedded representations by decoding both spatial and non-spatial transformations. This distinguishes EnAET from conventional semi-supervised methods that focus on improving prediction consistency and confidence by different models on both unlabeled and labeled examples. In contrast, we propose to explore the role of self-supervised representations in semi-supervised learning under a rich family of transformations. Experiment results on CIFAR-10, CIFAR-100, SVHN and STL10 demonstrate that the proposed EnAET <font color="#00be00">outperform</font>s the state-of-the-art semi-supervised methods by significant margins. In particular, we apply the proposed method to extremely challenging scenarios with only 10 images per class, and show that EnAET can achieve an error rate of 9.35% on CIFAR-10 and 16.92% on SVHN. In addition, EnAET achieves the best result when compared with fully supervised learning using all labeled data with the same network architecture. The performance on CIFAR-10, CIFAR-100 and SVHN with a smaller network is even more <font color="#960096">competitive</font> than the state-of-the-art of supervised learning methods based on a larger network. We also set a new performance record with an error rate of 1.99% on CIFAR-10 and 4.52% on STL10. The code and experiment records are released at <font color="#006400">http</font>s://<font color="red">github</font>.com/maple-research-lab/EnAET. </br></br>

<a href='http://arxiv.org/pdf/1911.07605.pdf'>1911.07605</a> &nbsp&nbsp (cs:AI, cs:ML) &nbsp&nbsp 2.0504баллов, №22</br>
<b>Commit2Vec: Learning Distributed Representations of Code Changes</b></br>
Authors: , Lozoya, Roc&#xec;o Cabrera, Baumann, Arnaud, Sabetta, Antonino, Bezzi, Michele</br>
  Deep learning methods, which have found successful applications in fields like image classification and natural language processing, have recently been applied to <font color="red">source code</font> analysis too, due to the enormous amount of freely available source code (e.g., from open-source software repositories).   In this work, we elaborate upon a <font color="red">state-of-the-art</font> approach to the representation of source code that uses information about its syntactic structure, and we adapt it to represent source changes (i.e., commits). We use this representation to classify security-relevant commits.   Because our method uses transfer learning (that is, we train a network on a &quot;pretext task&quot; for which abundant labeled data is available, and then we use such network for the target task of commit classification, for which fewer labeled instances are available), we studied the impact of pre-training the network using two different pretext tasks versus a randomly initialized model.   Our results indicate that representations that leverage the structural information obtained through code syntax <font color="#00be00">outperform</font> token-based representations. Furthermore, the performance metrics obtained when pre-training on a loosely related pretext task with a very large dataset ($&gt;10^6$ samples) were surpassed when pretraining on a smaller dataset ($&gt;10^4$ samples) but for a pretext task that is more closely related to the target task. </br></br>

<a href='http://arxiv.org/pdf/1911.09309.pdf'>1911.09309</a> &nbsp&nbsp (cs:ML, cs:NE) &nbsp&nbsp 1.9818баллов, №23</br>
<b>Decoding Spiking Mechanism with Dynamic Learning on Neuron Population</b></br>
Authors: , Chen, Zhijie, Yan, Junchi, Li, Longyuan, Yang, Xiaokang</br>
  A main concern in cognitive neuroscience is to decode the overt neural spike train observations and infer latent representations under neural circuits. However, traditional methods entail strong prior on network structure and hardly meet the demand for real spike data. Here we propose a novel neural network approach called Neuron Activation Network that extracts neural information explicitly from single trial neuron population spike trains. Our proposed method consists of a spatiotemporal learning procedure on sensory environment and a message passing mechanism on population graph, followed by a neuron activation process in a recursive fashion. Our model is aimed to reconstruct neuron information while inferring representations of neuron spiking states. We apply our model to retinal ganglion cells and the experimental results suggest that our model holds a more potent capability in generating neural spike sequences with high fidelity than the <font color="red">state-of-the-art</font> methods, as well as being more expressive and having potential to disclose latent spiking mechanism. The <font color="red">source code</font> will be released with the final paper. </br></br>

<a href='http://arxiv.org/pdf/1911.07620.pdf'>1911.07620</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 1.9784баллов, №24</br>
<b>Exploiting Token and Path-based Representations of Code for Identifying\n  Security-Relevant Commits</b></br>
Authors: , Ram, Achyudh, Xin, Ji, Nagappan, Meiyappan, Yu, Yaoliang, Lozoya, Roc&#xed;o Cabrera, Sabetta, Antonino, Lin, Jimmy</br>
  Public vulnerability databases such as CVE and NVD account for only 60% of security vulnerabilities present in open-source projects, and are known to suffer from inconsistent quality. Over the last two years, there has been considerable growth in the number of known vulnerabilities across projects available in various repositories such as NPM and Maven Central. Such an increasing risk calls for a mechanism to infer the presence of security threats in a timely manner. We propose novel <font color="#00be00">hierarchical</font> deep learning models for the identification of security-relevant commits from either the commit diff or the <font color="red">source code</font> for the Java classes. By comparing the performance of our model against code2vec, a <font color="red">state-of-the-art</font> model that learns from path-based representations of code, and a logistic <font color="#be00be">regression</font> baseline, we show that deep learning models show promising results in identifying security-related commits. We also conduct a comparative analysis of how various deep learning models learn across different input representations and the effect of regularization on the generalization of our models. </br></br>

<a href='http://arxiv.org/pdf/1910.00164.pdf'>1910.00164</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp 1.9529баллов, №25</br>
<b>Predicting with High Correlation Features</b></br>
Authors: , Arpit, Devansh, Xiong, Caiming, Socher, Richard</br>
  It has been shown that instead of learning actual object features, deep networks tend to exploit non-robust (spurious) discriminative features that are shared between training and test sets. Therefore, while they achieve <font color="red">state of the art</font> performance on such test sets, they achieve poor generalization on out of distribution (OOD) samples where the IID (independent, identical distribution) assumption breaks and the distribution of non-robust features shifts. In this paper, we consider distribution shift as a shift in the distribution of input features during test time that exhibit low correlation with targets in the training set. Under this definition, we evaluate existing robust feature learning methods and regularization methods and compare them against a baseline designed to specifically capture high correlation features in training set. As a controlled test-bed, we design a colored MNIST (C-MNIST) dataset and find that existing methods trained on this set fail to generalize well on an OOD version this dataset, showing that they overfit the low correlation color features. This is avoided by the baseline method trained on the same C-MNIST data, which is designed to learn high correlation features, and is able to generalize on the test sets of vanilla MNIST, MNIST-M and SVHN datasets. Our code is available at \\url{<font color="#006400">http</font>s://<font color="red">github</font>.com/salesforce/corr_based_prediction}. </br></br>

<a href='http://arxiv.org/pdf/1911.08896.pdf'>1911.08896</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.9126баллов, №26</br>
<b>Shift Convolution Network for Stereo Matching</b></br>
Authors: , Xie, Jian</br>
  In this paper, we present Shift Convolution Network (ShiftConvNet) to provide matching capability between two feature maps for stereo estimation. The proposed method can speedily produce a highly accurate disparity map from stereo images. A module called shift convolution layer is proposed to replace the traditional correlation layer to perform patch comparisons between two feature maps. By using a novel architecture of convolutional network to learn the matching process, ShiftConvNet can produce better results than DispNet-C[1], also running faster with 5 fps. Moreover, with a proposed auto shift convolution refine part, further improvement is obtained. The proposed approach was evaluated on FlyingThings 3D. It achieves <font color="red">state-of-the-art</font> results on the benchmark dataset. Codes will be made available at <font color="red">github</font>. </br></br>

<a href='http://arxiv.org/pdf/1911.00219.pdf'>1911.00219</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 1.9095баллов, №27</br>
<b>InteractE: Improving Convolution-based Knowledge Graph Embeddings by\n  Increasing Feature Interactions</b></br>
Authors: , Vashishth, Shikhar, Sanyal, Soumya, Nitin, Vikram, Agrawal, Nilesh, Talukdar, Partha</br>
  Most existing <font color="#960096">knowledge graph</font>s suffer from incompleteness, which can be alleviated by inferring missing links based on known facts. One popular way to accomplish this is to generate low-dimensional embeddings of entities and relations, and use these to make inferences. ConvE, a recently proposed approach, applies convolutional filters on 2D reshapings of entity and relation embeddings in order to capture rich interactions between their components. However, the number of interactions that ConvE can capture is limited. In this paper, we analyze how increasing the number of these interactions affects link prediction performance, and utilize our observations to propose InteractE. InteractE is based on three key ideas -- feature permutation, a novel feature reshaping, and circular convolution. Through extensive experiments, we find that InteractE <font color="#00be00">outperform</font>s <font color="red">state-of-the-art</font> convolutional link prediction baselines on FB15k-237. Further, InteractE achieves an MRR score that is 9%, 7.5%, and 23% better than ConvE on the FB15k-237, WN18RR and YAGO3-10 datasets respectively. The results validate our central hypothesis -- that increasing feature interaction is beneficial to link prediction performance. We make the <font color="red">source code</font> of InteractE available to encourage reproducible research. </br></br>

<a href='http://arxiv.org/pdf/1911.07559.pdf'>1911.07559</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.8376баллов, №28</br>
<b>FFA-Net: Feature Fusion Attention Network for Single Image Dehazing</b></br>
Authors: , Qin, Xu, Wang, Zhilin, Bai, Yuanchao, Xie, Xiaodong, Jia, Huizhu</br>
  In this paper, we propose an end-to-end feature fusion at-tention network (FFA-Net) to directly restore the haze-free image. The FFA-Net architecture consists of three key components:   1) A novel Feature Attention (FA) module combines Channel Attention with Pixel Attention mechanism, considering that different channel-wise features contain totally different weighted information and haze distribution is uneven on the different image pixels. FA treats different features and pixels unequally, which provides additional flexibility in dealing with different types of information, expanding the representational ability of CNNs. 2) A basic block structure consists of Local Residual Learning and Feature Attention, Local Residual Learning allowing the less important information such as thin haze region or low-frequency to be bypassed through multiple local residual connections, let main network architecture focus on more effective information. 3) An Attention-based different levels Feature Fusion (FFA) structure, the feature weights are adaptively learned from the Feature Attention (FA) module, giving more weight to important features. This structure can also retain the information of shallow layers and pass it into deep layers.   The experimental results demonstrate that our proposed FFA-Net surpasses previous <font color="red">state-of-the-art</font> single image dehazing methods by a very large margin both quantitatively and qualitatively, boosting the best published PSNR metric from 30.23db to 35.77db on the SOTS indoor test dataset.   Code has been made available at <font color="red">GitHub</font>. </br></br>

<a href='http://arxiv.org/pdf/1911.08947.pdf'>1911.08947</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.7074баллов, №29</br>
<b>Real-time Scene Text Detection with Differentiable Binarization</b></br>
Authors: , Liao, Minghui, Wan, Zhaoyi, Yao, Cong, Chen, Kai, Bai, Xiang</br>
  Recently, <font color="#be00be">segmentation</font>-based methods are quite popular in scene text detection, as the segmentation results can more accurately describe scene text of various shapes such as curve text. However, the post-processing of binarization is essential for segmentation-based detection, which converts probability maps produced by a segmentation method into bounding boxes/regions of text. In this paper, we propose a module named Differentiable Binarization (DB), which can perform the binarization process in a segmentation network. Optimized along with a DB module, a segmentation network can adaptively set the thresholds for binarization, which not only simplifies the post-processing but also enhances the performance of text detection. Based on a simple segmentation network, we validate the performance improvements of DB on five benchmark datasets, which consistently achieves <font color="red">state-of-the-art</font> results, in terms of both detection accuracy and speed. In particular, with a light-weight backbone, the performance improvements by DB are significant so that we can look for an ideal tradeoff between detection accuracy and efficiency. Specifically, with a backbone of ResNet-18, our detector achieves an F-measure of 82.8, running at 62 FPS, on the MSRA-TD500 dataset. Code is available at: <font color="#006400">http</font>s://<font color="red">github</font>.com/MhLiao/DB </br></br>

<a href='http://arxiv.org/pdf/1911.08564.pdf'>1911.08564</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.6576баллов, №30</br>
<b>Enhancing Generic Segmentation with Learned Region Representations</b></br>
Authors: , Isaacs, Or, Shayer, Oran, Lindenbaum, Michael</br>
  Current successful approaches for generic (non-semantic) <font color="#be00be">segmentation</font> rely mostly on edge detection and have leveraged the strengths of deep learning mainly by improving the edge detection stage in the algorithmic pipeline. This is in contrast to semantic and instance segmentation, where deep learning has made a dramatic affect and DNNs are applied directly to generate pixel-wise segment representations. We propose a new method for learning a pixelwise representation that reflects segment relatedness. This representation is combined with an edge map to yield a new segmentation algorithm. We show that the representations themselves achieve <font color="red">state-of-the-art</font> segment similarity scores. Moreover, the proposed, combined segmentation algorithm provides results that are either the <font color="red">state of the art</font> or improve it, for most quality measures. </br></br>

<a href='http://arxiv.org/pdf/1911.06997.pdf'>1911.06997</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 1.6530баллов, №31</br>
<b>Self-supervised GAN: Analysis and Improvement with Multi-class Minimax\n  Game</b></br>
Authors: , Tran, Ngoc-Trung, Tran, Viet-Hung, Nguyen, Ngoc-Bao, Yang, Linxiao, Cheung, Ngai-Man</br>
  Self-supervised (SS) learning is a powerful approach for representation learning using unlabeled data. Recently, it has been applied to Generative Adversarial Networks (GAN) training. Specifically, SS tasks were proposed to address the catastrophic forgetting issue in the GAN discriminator. In this work, we perform an in-depth analysis to understand how SS tasks interact with learning of generator. From the analysis, we identify issues of SS tasks which allow a severely mode-collapsed generator to excel the SS tasks. To address the issues, we propose new SS tasks based on a multi-class minimax game. The competition between our proposed SS tasks in the game encourages the generator to learn the data distribution and generate diverse samples. We provide both <font color="blue">theor</font>etical and empirical analysis to support that our proposed SS tasks have better convergence property. We conduct experiments to incorporate our proposed SS tasks into two different GAN baseline models. Our approach establishes <font color="red">state-of-the-art</font> FID scores on CIFAR-10, CIFAR-100, STL-10, CelebA, Imagenet $32\\times32$ and Stacked-MNIST datasets, <font color="#00be00">outperform</font>ing existing works by considerable margins in some cases. Our unconditional GAN model approaches performance of conditional GAN without using labeled data. Our code: \\url{<font color="#006400">http</font>s://<font color="red">github</font>.com/tntrung/msgan} </br></br>

<a href='http://arxiv.org/pdf/1911.03721.pdf'>1911.03721</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp 1.6454баллов, №32</br>
<b>Block-Coordinate Descent on the Riemannian Staircase for Certifiably\n  Correct Distributed Rotation and Pose Synchronization</b></br>
Authors: , Tian, Yulun, Khosoussi, Kasra, How, Jonathan P.</br>
  This paper presents the first certifiably correct solver for distributed rotation and pose synchronization, the backbone of modern collaborative simultaneous localization and mapping (CSLAM) and camera network localization (CNL) systems. By pursuing a sparse semidefinite relaxation, our approach provides formal performance guarantees that match the <font color="red">state of the art</font> in the centralized setting. In particular, we prove that under &quot;low&quot; noise, the solution to the semidefinite relaxation is guaranteed to provide a globally optimal solution to the original non-convex problem. To solve the resulting large-scale semidefinite programs, we adopt the <font color="red">state-of-the-art</font> Riemannian Staircase framework and develop Riemannian block-coordinate descent (RBCD) as the core distributed local search algorithm. RBCD is well-suited to distributed synchronization problems as it only requires local communication, provides <font color="#be00be">privacy</font> protection, and is easily parallelizable. Furthermore, we prove that RBCD converges to first-order critical points for general Riemannian optimization problems over product of matrix submanifolds, with a global sublinear convergence rate. Extensive evaluations on real and synthetic datasets demonstrate that the proposed solver correctly recovers globally optimal solutions under low-to-moderate noise, and <font color="#00be00">outperform</font>s alternative distributed techniques in terms of solution precision and convergence speed. </br></br>

<a href='http://arxiv.org/pdf/1911.09464.pdf'>1911.09464</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.6447баллов, №33</br>
<b>Quantization Networks</b></br>
Authors: , Yang, Jiwei, Shen, Xu, Xing, Jun, Tian, Xinmei, Li, Houqiang, Deng, Bing, Huang, Jianqiang, Hua, Xiansheng</br>
  Although deep neural networks are highly effective, their high computational and memory costs severely challenge their applications on portable devices. As a consequence, low-bit quantization, which converts a full-precision neural network into a low-bitwidth integer version, has been an active and promising research topic. Existing methods formulate the low-bit quantization of networks as an approximation or optimization problem. Approximation-based methods confront the gradient mismatch problem, while optimization-based methods are only suitable for quantizing weights and could introduce high computational cost in the training stage. In this paper, we propose a novel perspective of <font color="#be00be">interpret</font>ing and implementing neural network quantization by formulating low-bit quantization as a differentiable non-linear function (termed quantization function). The proposed quantization function can be learned in a lossless and end-to-end manner and works for any weights and activations of neural networks in a simple and uniform way. Extensive experiments on image classification and <font color="#be00be">object detection</font> tasks show that our quantization networks <font color="#00be00">outperform</font> the <font color="red">state-of-the-art</font> methods. We believe that the proposed method will shed new insights on the interpretation of neural network quantization. Our code is available at <font color="#006400">http</font>s://<font color="red">github</font>.com/aliyun/alibabacloud-quantization-networks. </br></br>

<a href='http://arxiv.org/pdf/1911.07141.pdf'>1911.07141</a> &nbsp&nbsp (cs:ML, cs:AI, cs:CL) &nbsp&nbsp 1.6020баллов, №34</br>
<b>Working Memory Graphs</b></br>
Authors: , Loynd, Ricky, Fernandez, Roland, Celikyilmaz, Asli, Swaminathan, Adith, Hausknecht, Matthew</br>
  Transformers have increasingly <font color="#00be00">outperform</font>ed gated RNNs in obtaining new <font color="red">state-of-the-art</font> results on supervised tasks involving text sequences. Inspired by this trend, we study the question of how Transformer-based models can improve the performance of sequential decision-making agents. We present the Working Memory Graph (WMG), an agent that employs multi-head self-attention to reason over a dynamic set of vectors representing observed and recurrent state. We evaluate WMG in two partially observable environments, one that requires complex reasoning over past observations, and another that features factored observations. We find that WMG significantly outperforms gated RNNs on these tasks, supporting the hypothesis that WMG\'s inductive bias in favor of learning and leveraging factored representations can dramatically boost <font color="#00be00">sample efficien</font>cy in environments featuring such structure. </br></br>

<a href='http://arxiv.org/pdf/1911.06849.pdf'>1911.06849</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 1.5834баллов, №35</br>
<b>Curriculum Self-Paced Learning for Cross-Domain Object Detection</b></br>
Authors: , Soviany, Petru, Ionescu, Radu Tudor, Rota, Paolo, Sebe, Nicu</br>
  Training (source) domain bias affects <font color="red">state-of-the-art</font> object detectors, such as Faster R-CNN, when applied to new (target) domains. To alleviate this problem, researchers proposed various domain adaptation methods to improve <font color="#be00be">object detection</font> results in the cross-domain setting, e.g. by translating images with ground-truth labels from the source domain to the target domain using Cycle-GAN or by applying self-paced learning. On top of combining Cycle-GAN transformations and self-paced learning, in this paper, we propose a novel self-paced algorithm that learns from easy to hard. To estimate the difficulty of each image, we use the number of detected objects divided by their average size. Our method is simple and effective, without any overhead during inference. It uses only pseudo-labels for samples taken from the target domain, i.e. the domain adaptation is unsupervised. We conduct experiments on two cross-domain benchmarks, showing better results than the <font color="red">state of the art</font>. We also perform an ablation study demonstrating the utility of each component in our framework. </br></br>

<a href='http://arxiv.org/pdf/1911.08666.pdf'>1911.08666</a> &nbsp&nbsp (cs:ML, cs:RO, stat:ML) &nbsp&nbsp 1.4914баллов, №36</br>
<b>Evaluating task-agnostic exploration for fixed-batch learning of\n  arbitrary future tasks</b></br>
Authors: , Dasagi, Vibhavari, Lee, Robert, Bruce, Jake, Leitner, J&#xfc;rgen</br>
  Deep <font color="#00be00">reinforcement learning</font> has been shown to solve challenging tasks where large amounts of training experience is available, usually obtained online while learning the task. Robotics is a significant potential application domain for many of these algorithms, but generating robot experience in the <font color="#009600">real world</font> is expensive, especially when each task requires a lengthy online training procedure. Off-policy algorithms can in principle learn arbitrary tasks from a diverse enough fixed dataset. In this work, we evaluate popular exploration methods by generating robotics datasets for the purpose of learning to solve tasks completely offline without any further interaction in the real world. We present results on three popular continuous control tasks in simulation, as well as continuous control of a high-dimensional real robot arm. Code documenting all algorithms, experiments, and hyper-parameters is available at <font color="#006400">http</font>s://<font color="red">github</font>.com/qutrobotlearning/batchlearning. </br></br>

<a href='http://arxiv.org/pdf/1911.09272.pdf'>1911.09272</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 1.4309баллов, №37</br>
<b>Robustness Certificates for Sparse Adversarial Attacks by Randomized\n  Ablation</b></br>
Authors: , Levine, Alexander, Feizi, Soheil</br>
  Recently, techniques have been developed to provably guarantee the robustness of a classifier to adversarial perturbations of bounded L_1 and L_2 magnitudes by using randomized smoothing: the robust classification is a consensus of base classifications on randomly noised samples where the noise is additive. In this paper, we extend this technique to the L_0 threat model. We propose an efficient and certifiably robust defense against sparse <font color="blue">adversarial att</font>acks by randomly ablating input features, rather than using additive noise. Experimentally, on MNIST, we can certify the classifications of over 50% of images to be robust to any distortion of at most 8 pixels. This is comparable to the observed empirical robustness of unprotected classifiers on MNIST to modern L_0 attacks, demonstrating the tightness of the proposed robustness certificate. We also evaluate our certificate on ImageNet and CIFAR-10. Our certificates represent an improvement on those provided in a concurrent work (Lee et al. 2019) which uses random noise rather than ablation (median certificates of 8 pixels versus 4 pixels on MNIST; 16 pixels versus 1 pixel on ImageNet.) Additionally, we empirically demonstrate that our classifier is highly robust to modern sparse adversarial attacks on MNIST. Our classifications are robust, in median, to adversarial perturbations of up to 31 pixels, compared to 22 pixels reported as the <font color="red">state-of-the-art</font> defense, at the cost of a slight decrease (around 2.3%) in the classification accuracy. Code is available at <font color="#006400">http</font>s://<font color="red">github</font>.com/alevine0/randomizedAblation/. </br></br>

<a href='http://arxiv.org/pdf/1911.06994.pdf'>1911.06994</a> &nbsp&nbsp (cs:RO, cs:CV) &nbsp&nbsp 1.3463баллов, №38</br>
<b>Ground and Non-Ground Separation Filter for UAV Lidar Point Cloud</b></br>
Authors: , Prathap, Geesara, Fedorenko, Roman, Klimchik, Alexandr</br>
  This paper proposes a novel approach for separating ground plane and non-ground objects on <font color="blue">Lidar</font> 3D <font color="#be00be">point cloud</font> as a filter. It is specially designed for real-time applications on unmanned aerial vehicles and works on sparse Lidar point clouds without preliminary mapping. We use this filter as a crucial component of fast obstacle avoidance system for agriculture drone operating at low altitude. As the first step, a point cloud is transformed into a depth image and then places with high density nearest to the vehicle (local maxima) are identified. Then we merge original depth image with identified locations after maximizing intensities of pixels in which local maxima were found. Next step is to calculate range angle image which represents angles between two consecutive laser beams based on improved depth image. Once a range angle image is constructed, smoothing is applied to reduce the noise. Finally, we find out connected components in the improved depth image while incorporating smoothed range angle image. This allows separating the non-ground objects. The rest of the locations of depth image belong to the ground plane. The filter has been tested on a simulated environment as well as an actual drone and provides real-time performance. We make our <font color="red">source code</font> and dataset available online\\footnote[2]{Source code and dataset are available at <font color="#006400">http</font>s://<font color="red">github</font>.com/GPrathap/hagen.git </br></br>

<a href='http://arxiv.org/pdf/1911.09099.pdf'>1911.09099</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.3424баллов, №39</br>
<b>SINet: Extreme Lightweight Portrait Segmentation Networks with Spatial\n  Squeeze Modules and Information Blocking Decoder</b></br>
Authors: , Park, Hyojin, Sj&#xf6;sund, Lars Lowe, Monet, Nicolas, Yoo, YoungJoon, Kwak, Nojun</br>
  Designing a <font color="#be00be">lightweight</font> and robust portrait <font color="#be00be">segmentation</font> algorithm is an important task for a wide range of<font color="#be00be"> face </font>applications. However, the problem has been considered as a subset of the object segmentation problem and less handled in the semantic segmentation field. Obviously, portrait segmentation has its unique requirements. First, because the portrait segmentation is performed in the middle of a whole process of many <font color="#009600">real-world</font> applications, it requires extremely lightweight models. Second, there has not been any public datasets in this domain that contain a sufficient number of images with unbiased statistics. To solve the first problem, we introduce the new extremely lightweight portrait segmentation model SINet, containing an information blocking decoder and spatial squeeze modules. The information blocking decoder uses confidence estimates to recover local spatial information without spoiling global consistency. The spatial squeeze module uses multiple receptive fields to cope with various sizes of consistency in the image. To tackle the second problem, we propose a simple method to create additional portrait segmentation data which can improve accuracy on the EG1800 dataset. In our qualitative and quantitative analysis on the EG1800 dataset, we show that our method <font color="#00be00">outperform</font>s various existing lightweight segmentation models. Our method reduces the number of parameters from2.1Mto86.9K(around 95.9% reduction), while maintaining the accuracy under an 1% margin from the <font color="red">state-of-the-art</font> portrait segmentation method. We also show our model is successfully executed on a real<font color="#960096"> mobile </font>device with 100.6 FPS. In addition, we demonstrate that our method can be used for general semantic segmentation on the Cityscape dataset. The code is available in<font color="#006400">http</font>s://<font color="red">github</font>.com/HYOJINPARK/ExtPortraitSeg </br></br>

<a href='http://arxiv.org/pdf/1911.08648.pdf'>1911.08648</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 1.3400баллов, №40</br>
<b>Co-Attention Hierarchical Network: Generating Coherent Long Distractors\n  for Reading Comprehension</b></br>
Authors: , Zhou, Xiaorui, Luo, Senlin, Wu, Yunfang</br>
  In reading comprehension, generating sentence-level distractors is a significant task, which requires a deep understanding of the article and question. The traditional entity-centered methods can only generate word-level or phrase-level distractors. Although recently proposed neural-based methods like sequence-to-sequence (Seq2Seq) model show great potential in generating creative text, the previous neural methods for distractor generation ignore two important aspects. First, they didn\'t model the interactions between the article and question, making the generated distractors tend to be too general or not relevant to question context. Second, they didn\'t emphasize the relationship between the distractor and article, making the generated distractors not semantically relevant to the article and thus fail to form a set of meaningful options. To solve the first problem, we propose a co-attention enhanced <font color="#00be00">hierarchical</font> architecture to better capture the interactions between the article and question, thus guide the decoder to generate more coherent distractors. To alleviate the second problem, we add an additional semantic similarity loss to push the generated distractors more relevant to the article. Experimental results show that our model <font color="#00be00">outperform</font>s several strong baselines on automatic metrics, achieving <font color="red">state-of-the-art</font> performance. Further human evaluation indicates that our generated distractors are more coherent and more educative compared with those distractors generated by baselines. </br></br>

<a href='http://arxiv.org/pdf/1911.09338.pdf'>1911.09338</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.3321баллов, №41</br>
<b>Voice-Face Cross-modal Matching and Retrieval: A Benchmark</b></br>
Authors: , Xiong, Chuyuan, Zhang, Deyuan, Liu, Tao, Du, Xiaoyong</br>
  Cross-modal associations between voice and<font color="#be00be"> face </font>from a person can be learnt algorithmically, which can benefit a lot of applications. The problem can be defined as voice-face matching and retrieval tasks. Much research attention has been paid on these tasks recently. However, this research is still in the early stage. Test schemes based on random tuple mining tend to have low test confidence. Generalization ability of models can not be evaluated by small scale datasets. Performance metrics on various tasks are scarce. A benchmark for this problem needs to be established. In this paper, first, a framework based on comprehensive studies is proposed for voice-face matching and retrieval. It achieves <font color="red">state-of-the-art</font> performance with various performance metrics on different tasks and with high test confidence on large scale datasets, which can be taken as a baseline for the follow-up research. In this framework, a voice anchored L2-Norm constrained metric space is proposed, and cross-modal embeddings are learned with CNN-based networks and triplet loss in the metric space. The embedding learning process can be more effective and efficient with this strategy. Different network structures of the framework and the cross language transfer abilities of the model are also analyzed. Second, a voice-face dataset (with 1.15M face data and 0.29M audio data) from <font color="#be00be">Chinese</font> speakers is constructed, and a convenient and quality controllable dataset collection tool is developed. The dataset and <font color="red">source code</font> of the paper will be published together with this paper. </br></br>

<a href='http://arxiv.org/pdf/1911.02140.pdf'>1911.02140</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp 1.3139баллов, №42</br>
<b>Fully Parameterized Quantile Function for Distributional Reinforcement\n  Learning</b></br>
Authors: , Yang, Derek, Zhao, Li, Lin, Zichuan, Qin, Tao, Bian, Jiang, Liu, Tieyan</br>
  Distributional <font color="#00be00">Reinforcement Learning</font> (RL) differs from traditional RL in that, rather than the expectation of total returns, it estimates distributions and has achieved <font color="red">state-of-the-art</font> performance on Atari Games. The key challenge in practical distributional RL algorithms lies in how to parameterize estimated distributions so as to better approximate the true continuous distribution. Existing distributional RL algorithms parameterize either the probability side or the return value side of the distribution function, leaving the other side uniformly fixed as in C51, QR-DQN or randomly sampled as in IQN. In this paper, we propose fully parameterized quantile function that parameterizes both the quantile fraction axis (i.e., the x-axis) and the value axis (i.e., y-axis) for distributional RL. Our algorithm contains a fraction proposal network that generates a discrete set of quantile fractions and a quantile value network that gives corresponding quantile values. The two networks are jointly trained to find the best approximation of the true distribution. Experiments on 55 Atari Games show that our algorithm significantly <font color="#00be00">outperform</font>s existing distributional RL algorithms and creates a new record for the Atari Learning Environment for non-distributed agents. </br></br>

<a href='http://arxiv.org/pdf/1911.08517.pdf'>1911.08517</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 1.3117баллов, №43</br>
<b>Generalizable Resource Allocation in Stream Processing via Deep\n  Reinforcement Learning</b></br>
Authors: , Ni, Xiang, Li, Jing, Yu, Mo, Zhou, Wang, Wu, Kun-Lung</br>
  This paper considers the problem of resource allocation in stream processing, where continuous data flows must be processed in real time in a large distributed system. To maximize system throughput, the resource allocation strategy that partitions the computation tasks of a stream processing graph onto computing devices must simultaneously balance workload distribution and minimize communication. Since this problem of graph partitioning is known to be NP-complete yet crucial to practical streaming systems, many heuristic-based algorithms have been developed to find reasonably good solutions. In this paper, we present a graph-aware encoder-decoder framework to learn a generalizable resource allocation strategy that can properly distribute computation tasks of stream processing graphs unobserved from training data. We, for the first time, propose to leverage graph embedding to learn the structural information of the stream processing graphs. Jointly trained with the graph-aware decoder using deep <font color="#00be00">reinforcement learning</font>, our approach can effectively find optimized solutions for unseen graphs. Our experiments show that the proposed model <font color="#00be00">outperform</font>s both METIS, a <font color="red">state-of-the-art</font> graph partitioning algorithm, and an LSTM-based encoder-decoder model, in about 70% of the test cases. </br></br>

<a href='http://arxiv.org/pdf/1911.06641.pdf'>1911.06641</a> &nbsp&nbsp (cs:CL, cs:ML, cs:NE) &nbsp&nbsp 1.3081баллов, №44</br>
<b>CatGAN: Category-aware Generative Adversarial Networks with Hierarchical\n  Evolutionary Learning for Category Text Generation</b></br>
Authors: , Liu, Zhiyue, Wang, Jiahai, Liang, Zhiwei</br>
  Generating multiple categories of texts is a challenging task and draws more and more attention. Since generative adversarial nets (GANs) have shown <font color="#960096">competitive</font> results on general text generation, they are extended for category text generation in some previous works. However, the complicated model structures and learning strategies limit their performance and exacerbate the training instability. This paper proposes a category-aware GAN (CatGAN) which consists of an efficient category-aware model for category text generation and a <font color="#00be00">hierarchical</font> evolutionary learning algorithm for training our model. The category-aware model directly measures the gap between real samples and generated samples on each category, then reducing this gap will guide the model to generate high-quality category samples. The Gumbel-Softmax relaxation further frees our model from complicated learning strategies for updating CatGAN on discrete data. Moreover, only focusing on the sample quality normally leads the mode collapse problem, thus a hierarchical evolutionary learning algorithm is introduced to stabilize the training procedure and obtain the trade-off between quality and diversity while training CatGAN. Experimental results demonstrate that CatGAN <font color="#00be00">outperform</font>s most of the existing <font color="red">state-of-the-art</font> methods. </br></br>

<a href='http://arxiv.org/pdf/1911.04094.pdf'>1911.04094</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 1.3021баллов, №45</br>
<b>SMIX($\\lambda$): Enhancing Centralized Value Functions for Cooperative\n  Multi-Agent Reinforcement Learning</b></br>
Authors: , Yao, Xinghu, Wen, Chao, Wang, Yuhui, Tan, Xiaoyang</br>
  Learning a stable and generalizable centralized value function (CVF) is a crucial but challenging task in multi-agent <font color="#00be00">reinforcement learning</font> (MARL), as it has to deal with the issue that the joint action space increases exponentially with the number of agents in such scenarios. This paper proposes an approach, named SMIX(${\\lambda}$), that uses an off-policy training to achieve this by avoiding the greedy assumption commonly made in CVF learning. As importance sampling for such off-policy training is both computationally costly and numerically unstable, we proposed to use the ${\\lambda}$-return as a proxy to compute the TD error. With this new loss function objective, we adopt a modified QMIX network structure as the base to train our model. By further connecting it with the ${Q(\\lambda)}$ approach from an unified expectation correction viewpoint, we show that the proposed SMIX(${\\lambda}$) is equivalent to ${Q(\\lambda)}$ and hence shares its convergence properties, while without being suffered from the aforementioned curse of dimensionality problem inherent in MARL. Experiments on the StarCraft Multi-Agent Challenge (SMAC) benchmark demonstrate that our approach not only <font color="#00be00">outperform</font>s several <font color="red">state-of-the-art</font> MARL methods by a large margin, but also can be used as a general tool to improve the overall performance of other CTDE-type algorithms by enhancing their CVFs. </br></br>

<a href='http://arxiv.org/pdf/1911.09676.pdf'>1911.09676</a> &nbsp&nbsp (cs:ML, cs:CV, cs:RO, stat:ML) &nbsp&nbsp 1.2789баллов, №46</br>
<b>Third-Person Visual Imitation Learning via Decoupled Hierarchical\n  Controller</b></br>
Authors: , Sharma, Pratyusha, Pathak, Deepak, Gupta, Abhinav</br>
  We study a generalized setup for learning from demonstration to build an agent that can manipulate novel objects in unseen scenarios by looking at only a single video of human demonstration from a third-person perspective. To accomplish this goal, our agent should not only learn to understand the intent of the demonstrated third-person video in its context but also perform the intended task in its environment configuration. Our central insight is to enforce this structure explicitly during learning by decoupling what to achieve (intended task) from how to perform it (controller). We propose a <font color="#00be00">hierarchical</font> setup where a high-level module learns to generate a series of first-person sub-goals conditioned on the third-person video demonstration, and a low-level controller predicts the actions to achieve those sub-goals. Our agent acts from raw image observations without any access to the full state information. We show results on a real robotic platform using Baxter for the manipulation tasks of pouring and placing objects in a box. Project video and code are at <font color="#006400">http</font>s://pathak22.<font color="red">github</font>.io/hierarchical-imitation/ </br></br>

<a href='http://arxiv.org/pdf/1911.09487.pdf'>1911.09487</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 1.2631баллов, №47</br>
<b>Chemical-protein Interaction Extraction via Gaussian Probability\n  Distribution and External Biomedical Knowledge</b></br>
Authors: , Sun, Cong, Yang, Zhihao, Su, Leilei, Wang, Lei, Zhang, Yin, Lin, Hongfei, Wang, Jian</br>
  The bio<font color="blue">medic</font>al literature contains a wealth of chemical-protein interactions (CPIs). Automatically extracting CPIs described in biomedical literature is essential for drug discovery, precision medicine, as well as basic biomedical research. However, the existing methods do not consider the impact of overlapping relations on CPI extraction. This leads to the extraction of sentences with overlapping relations becoming the bottleneck of CPI extraction. In this paper, we propose a novel neural network-based approach to improve the CPI extraction performance of sentences with overlapping relations. Specifically, the approach first employs<font color="#00be00"> BERT </font>to generate high-quality contextual representations of the title sequence, instance sequence, and knowledge sequence. Then, the <font color="blue">Gaussi</font>an probability distribution is introduced to capture the local structure of the instance. Meanwhile, the attention mechanism is applied to fuse the title information and biomedical knowledge, respectively. Finally, the related representations are concatenated and fed into the softmax function to extract CPIs. We evaluate our proposed model on the CHEMPROT corpus. Our proposed model is superior in performance as compared with other <font color="red">state-of-the-art</font> models. The experimental results show that the Gaussian probability distribution and external knowledge are complementary to each other. Integrating them can effectively improve the CPI extraction performance. Furthermore, the Gaussian probability distribution can significantly improve the extraction performance of sentences with overlapping relations in biomedical relation extraction tasks. Data and code are available at <font color="#006400">http</font>s://<font color="red">github</font>.com/CongSun-dlut/CPI_extraction. </br></br>

<a href='http://arxiv.org/pdf/1911.08059.pdf'>1911.08059</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 1.2518баллов, №48</br>
<b>Prestopping: How Does Early Stopping Help Generalization against Label\n  Noise?</b></br>
Authors: , Song, Hwanjun, Kim, Minseok, Park, Dongmin, Lee, Jae-Gil</br>
  Noisy labels are very common in <font color="#009600">real-world</font> training data, which lead to poor generalization on test data because of overfitting to the noisy labels. In this paper, we claim that such overfitting can be avoided by &quot;early stopping&quot; training a deep neural network before the noisy labels are severely memorized. Then, we resume training the early stopped network using a &quot;maximal safe set,&quot; which maintains a collection of almost certainly true-labeled samples at each epoch since the early stop point. Putting them all together, our novel two-phase training method, called Prestopping, realizes noise-free training under any type of label noise for practical use. Extensive experiments using four image benchmark data sets verify that our method significantly <font color="#00be00">outperform</font>s four <font color="red">state-of-the-art</font> methods in test error by 0.4-8.2 percent points under existence of real-world noise. </br></br>

<a href='http://arxiv.org/pdf/1911.09334.pdf'>1911.09334</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp 1.2220баллов, №49</br>
<b>Incorporating Textual Evidence in Visual Storytelling</b></br>
Authors: , Li, Tianyi, Li, Sujian</br>
  Previous work on visual storytelling mainly focused on exploring image sequence as evidence for storytelling and neglected textual evidence for guiding story generation. Motivated by human storytelling process which recalls stories for familiar images, we exploit textual evidence from similar images to help generate coherent and meaningful stories. To pick the images which may provide textual experience, we propose a two-step ranking method based on image object recognition techniques. To utilize textual information, we design an extended Seq2Seq model with two-channel encoder and attention. Experiments on the VIST dataset show that our method <font color="#00be00">outperform</font>s <font color="red">state-of-the-art</font> baseline models without heavy engineering. </br></br>

<a href='http://arxiv.org/pdf/1911.09483.pdf'>1911.09483</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp 1.2214баллов, №50</br>
<b>MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning</b></br>
Authors: , Zhao, Guangxiang, Sun, Xu, Xu, Jingjing, Zhang, Zhiyuan, Luo, Liangchen</br>
  In sequence to sequence learning, the self-attention mechanism proves to be highly effective, and achieves significant improvements in many tasks. However, the self-attention mechanism is not without its own flaws. Although self-attention can model extremely long dependencies, the attention in deep layers tends to overconcentrate on a single token, leading to insufficient use of local information and difficultly in representing long sequences. In this work, we explore parallel multi-scale representation learning on sequence data, striving to capture both long-range and short-range language structures. To this end, we propose the Parallel MUlti-Scale attEntion (MUSE) and MUSE-simple. MUSE-simple contains the basic idea of parallel multi-scale sequence representation learning, and it encodes the sequence in parallel, in terms of different scales with the help from self-attention, and pointwise transformation. MUSE builds on MUSE-simple and explores combining convolution and self-attention for learning sequence representations from more different scales. We focus on machine translation and the proposed approach achieves substantial performance improvements over Transformer, especially on long sequences. More importantly, we find that although conceptually simple, its success in practice requires intricate considerations, and the multi-scale attention must build on unified semantic space. Under common setting, the proposed model achieves substantial performance and <font color="#00be00">outperform</font>s all previous models on three main machine translation tasks. In addition, MUSE has potential for accelerating inference due to its parallelism. Code will be available at <font color="#006400">http</font>s://<font color="red">github</font>.com/lancopku/MUSE </br></br>

<a href='http://arxiv.org/pdf/1911.08609.pdf'>1911.08609</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.2090баллов, №51</br>
<b>Hybrid Composition with IdleBlock: More Efficient Networks for Image\n  Recognition</b></br>
Authors: , Xu, Bing, Tulloch, Andrew, Chen, Yunpeng, Yang, Xiaomeng, Qiao, Lin</br>
  We propose a new building block, IdleBlock, which naturally prunes connections within the block. To fully utilize the IdleBlock we break the tradition of monotonic design in <font color="red">state-of-the-art</font> networks, and introducing hybrid composition with IdleBlock. We study hybrid composition on MobileNet v3 and EfficientNet-B0, two of the most efficient networks. Without any neural <font color="#00be00">architecture search</font>, the deeper &quot;MobileNet v3&quot; with hybrid composition design surpasses possibly all state-of-the-art image recognition network designed by human experts or neural architecture search algorithms. Similarly, the hybridized EfficientNet-B0 networks are more efficient than previous state-of-the-art networks with similar computation budgets. These results suggest a new simpler and more efficient direction for network design and neural architecture search. </br></br>

<a href='http://arxiv.org/pdf/1911.06948.pdf'>1911.06948</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 1.2087баллов, №52</br>
<b>Robust Reading Comprehension with Linguistic Constraints via Posterior\n  Regularization</b></br>
Authors: , Zhou, Mantong, Huang, Minlie, Zhu, Xiaoyan</br>
  In spite of great advancements of machine reading comprehension (RC), existing RC models are still vulnerable and not robust to different types of adversarial examples. Neural models over-confidently predict wrong answers to semantic different adversarial examples, while over-sensitively predict wrong answers to semantic equivalent adversarial examples. Existing methods which improve the robustness of such neural models merely mitigate one of the two issues but ignore the other. In this paper, we address the over-confidence issue and the over-sensitivity issue existing in current RC models simultaneously with the help of external linguistic knowledge. We first incorporate external knowledge to impose different linguistic constraints (entity constraint, lexical constraint, and predicate constraint), and then regularize RC models through posterior regularization. Linguistic constraints induce more reasonable predictions for both semantic different and semantic equivalent adversarial examples, and posterior regularization provides an effective mechanism to incorporate these constraints. Our method can be applied to any existing neural RC models including <font color="red">state-of-the-art</font><font color="#00be00"> BERT </font>models. Extensive experiments show that our method remarkably improves the robustness of base RC models, and is better to cope with these two issues simultaneously. </br></br>

<a href='http://arxiv.org/pdf/1911.08415.pdf'>1911.08415</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 1.2061баллов, №53</br>
<b>GMAN: A Graph Multi-Attention Network for Traffic Prediction</b></br>
Authors: , Zheng, Chuanpan, Fan, Xiaoliang, Wang, Cheng, Qi, Jianzhong</br>
  Long-term traffic prediction is highly challenging due to the complexity of traffic systems and the constantly changing nature of many impacting factors. In this paper, we focus on the spatio-temporal factors, and propose a graph multi-attention network (GMAN) to predict traffic conditions for time steps ahead at different locations on a road network graph. GMAN adapts an encoder-decoder architecture, where both the encoder and the decoder consist of multiple spatio-temporal attention blocks to model the impact of the spatio-temporal factors on traffic conditions. The encoder encodes the input traffic features and the decoder predicts the output sequence. Between the encoder and the decoder, a transform attention layer is applied to convert the encoded traffic features to generate the sequence representations of future time steps as the input of the decoder. The transform attention mechanism models the direct relationships between historical and future time steps that helps to alleviate the error propagation problem among prediction time steps. Experimental results on two <font color="#009600">real-world</font> traffic prediction tasks (i.e., traffic volume prediction and traffic speed prediction) demonstrate the superiority of GMAN. In particular, in the 1 hour ahead prediction, GMAN <font color="#00be00">outperform</font>s <font color="red">state-of-the-art</font> methods by up to 4% improvement in MAE measure. </br></br>

<a href='http://arxiv.org/pdf/1911.08581.pdf'>1911.08581</a> &nbsp&nbsp (cs:RO, cs:ML, stat:ML) &nbsp&nbsp 1.1988баллов, №54</br>
<b>A Configuration-Space Decomposition Scheme for Learning-based Collision\n  Checking</b></br>
Authors: , Han, Yiheng, Zhao, Wang, Pan, Jia, Ye, Zipeng, Yi, Ran, Liu, Yong-Jin</br>
  Motion planning for robots of high degrees-of-freedom (DOFs) is an important problem in robotics with sampling-based methods in configuration space C as one popular solution. Recently, machine learning methods have been introduced into sampling-based motion planning methods, which train a classifier to distinguish collision free subspace from in-collision subspace in C. In this paper, we propose a novel configuration space decomposition method and show two nice properties resulted from this decomposition. Using these two properties, we build a composite classifier that works compatibly with previous machine learning methods by using them as the elementary classifiers. Experimental results are presented, showing that our composite classifier <font color="#00be00">outperform</font>s <font color="red">state-of-the-art</font> single classifier methods by a large margin. A real application of motion planning in a multi-robot system in plant phenotyping using three UR5 robotic arms is also presented. </br></br>

<a href='http://arxiv.org/pdf/1911.07917.pdf'>1911.07917</a> &nbsp&nbsp (cs:CV, cs:ML, cs:SD) &nbsp&nbsp 1.1944баллов, №55</br>
<b>Cross-modal supervised learning for better acoustic representations</b></br>
Authors: , Jia, Shaoyong, Shu, Xin, Yang, Yang, Liang, Dawei, Liu, Qiyue, Liu, Junhui</br>
  Obtaining large-scale human-labeled datasets to train acoustic representation models is a very challenging task. On the contrary, we can easily collect data with machine-generated labels. In this work, we propose to exploit machine-generated labels to learn better acoustic representations, based on the synchronization between vision and audio. Firstly, we collect a large-scale video dataset with 15 million samples, which totally last 16,320 hours. Each video is 3 to 5 seconds in length and annotated automatically by <font color="#00be00">publicly available</font> visual and audio classification models. Secondly, we train various classical convolutional neural networks (CNNs) including VGGish, ResNet_50 and Mobilenet_v2. We also make several improvements to VGGish and achieve better results. Finally, we transfer our models on three external standard benchmarks for audio classification task, and achieve significant performance boost over the <font color="red">state-of-the-art</font> results. Models and codes will be released in the future. </br></br>

<a href='http://arxiv.org/pdf/1911.07470.pdf'>1911.07470</a> &nbsp&nbsp (cs:CL, cs:AI) &nbsp&nbsp 1.1826баллов, №56</br>
<b>Graph Transformer for Graph-to-Sequence Learning</b></br>
Authors: , Cai, Deng, Lam, Wai</br>
  The dominant graph-to-sequence transduction models employ graph neural networks for graph representation learning, where the structural information is reflected by the receptive field of neurons. Unlike graph neural networks that restrict the information exchange between immediate neighborhood, we propose a new model, known as Graph Transformer, that uses explicit relation encoding and allows direct communication between two distant nodes. It provides a more efficient way for global graph structure modeling. Experiments on the applications of text generation from Abstract Meaning Representation (AMR) and syntax-based neural machine translation show the superiority of our proposed model. Specifically, our model achieves 27.4 BLEU on LDC2015E86 and 29.7 BLEU on LDC2017T10 for AMR-to-text generation, <font color="#00be00">outperform</font>ing the <font color="red">state-of-the-art</font> results by up to 2.2 points. On the syntax-based translation tasks, our model establishes new single-model state-of-the-art BLEU scores, 21.3 for English-to-German and 14.1 for English-to-Czech, improving over the existing best results, including ensembles, by over 1 BLEU. </br></br>

<a href='http://arxiv.org/pdf/1911.07053.pdf'>1911.07053</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.1806баллов, №57</br>
<b>Maintaining Discrimination and Fairness in Class Incremental Learning</b></br>
Authors: , Zhao, Bowen, Xiao, Xi, Gan, Guojun, Zhang, Bin, Xia, Shutao</br>
  Deep neural networks (DNNs) have been applied in class incremental learning, which aims to solve common <font color="#009600">real-world</font> problems of learning new classes continually. One drawback of standard DNNs is that they are prone to catastrophic forgetting. Knowledge distillation (KD) is a commonly used technique to alleviate this problem. In this paper, we demonstrate it can indeed help the model to output more discriminative results within old classes. However, it cannot alleviate the problem that the model tends to classify objects into new classes, causing the positive effect of KD to be hidden and limited. We observed that an important factor causing catastrophic forgetting is that the weights in the last fully connected (FC) layer are highly biased in class incremental learning. In this paper, we propose a simple and effective solution motivated by the aforementioned observations to address catastrophic forgetting. Firstly, we utilize KD to maintain the discrimination within old classes. Then, to further maintain the fairness between old classes and new classes, we propose Weight Aligning (WA) that corrects the biased weights in the FC layer after normal training process. Unlike previous work, WA does not require any extra parameters or a validation set in advance, as it utilizes the information provided by the biased weights themselves. The proposed method is evaluated on ImageNet-1000, ImageNet-100, and CIFAR-100 under various settings. Experimental results show that the proposed method can effectively alleviate catastrophic forgetting and significantly <font color="#00be00">outperform</font> <font color="red">state-of-the-art</font> methods. </br></br>

<a href='http://arxiv.org/pdf/1911.08332.pdf'>1911.08332</a> &nbsp&nbsp (cs:CL, cs:ML, cs:SD, stat:ML) &nbsp&nbsp 1.1735баллов, №58</br>
<b>Neural Network based End-to-End Query by Example Spoken Term Detection</b></br>
Authors: , Ram, Dhananjay, Miculicich, Lesly, Bourlard, Herv&#xe9;</br>
  This paper focuses on the problem of query by example spoken term detection (QbE-STD) in zero-resource scenario. <font color="red">State-of-the-art</font> approaches primarily rely on dynamic time warping (DTW) based template matching techniques using phone posterior or bottleneck features extracted from a deep neural network (DNN). We use both monolingual and multilingual bottleneck features, and show that multilingual features perform increasingly better with more training languages. Previously, it has been shown that the DTW based matching can be replaced with a CNN based matching while using posterior features. Here, we show that the CNN based matching <font color="#00be00">outperform</font>s DTW based matching using bottleneck features as well. In this case, the feature extraction and pattern matching stages of our QbE-STD system are optimized independently of each other. We propose to integrate these two stages in a fully neural network based end-to-end learning framework to enable joint optimization of those two stages simultaneously. The proposed approaches are evaluated on two challenging multilingual datasets: Spoken Web Search 2013 and Query by Example Search on Speech Task 2014, demonstrating in each case significant improvements. </br></br>

<a href='http://arxiv.org/pdf/1911.06940.pdf'>1911.06940</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 1.1723баллов, №59</br>
<b>Utterance-to-Utterance Interactive Matching Network for Multi-Turn\n  Response Selection in Retrieval-Based Chatbots</b></br>
Authors: , Gu, Jia-Chen, Ling, Zhen-Hua, Liu, Quan</br>
  This paper proposes an utterance-to-utterance interactive matching network (U2U-IMN) for multi-turn response selection in retrieval-based chatbots. Different from previous methods following context-to-response matching or utterance-to-response matching frameworks, this model treats both contexts and responses as sequences of utterances when calculating the matching degrees between them. For a context-response pair, the U2U-IMN model first encodes each utterance separately using recurrent and self-attention layers. Then, a global and bidirectional interaction between the context and the response is conducted using the attention mechanism to collect the matching information between them. The distances between context and response utterances are employed as a prior component when calculating the attention weights. Finally, sentence-level aggregation and context-response-level aggregation are executed in turn to obtain the feature vector for matching degree prediction. Experiments on four public datasets showed that our proposed method <font color="#00be00">outperform</font>ed baseline methods on all metrics, achieving a new <font color="red">state-of-the-art</font> performance and demonstrating compatibility across domains for multi-turn response selection. </br></br>

<a href='http://arxiv.org/pdf/1911.04623.pdf'>1911.04623</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.1701баллов, №60</br>
<b>SimpleShot: Revisiting Nearest-Neighbor Classification for Few-Shot\n  Learning</b></br>
Authors: , Wang, Yan, Chao, Wei-Lun, Weinberger, Kilian Q., van der Maaten, Laurens</br>
  <font color="#00be00">Few-shot</font> learners aim to recognize new object classes based on a small number of labeled training examples. To prevent overfitting, <font color="red">state-of-the-art</font> few-shot learners use meta-learning on convolutional-network features and perform classification using a nearest-neighbor classifier. This paper studies the accuracy of nearest-neighbor baselines without meta-learning. Surprisingly, we find simple feature transformations suffice to obtain <font color="#960096">competitive</font> few-shot learning accuracies. For example, we find that a nearest-neighbor classifier used in combination with mean-subtraction and L2-normalization <font color="#00be00">outperform</font>s prior results in three out of five settings on the miniImageNet dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.07013.pdf'>1911.07013</a> &nbsp&nbsp (cs:ML, cs:CL, stat:ML) &nbsp&nbsp 1.1699баллов, №61</br>
<b>Understanding and Improving Layer Normalization</b></br>
Authors: , Xu, Jingjing, Sun, Xu, Zhang, Zhiyuan, Zhao, Guangxiang, Lin, Junyang</br>
  Layer normalization (LayerNorm) is a technique to normalize the distributions of intermediate layers. It enables smoother gradients, faster training, and better generalization accuracy. However, it is still unclear where the effectiveness stems from. In this paper, our main contribution is to take a step further in understanding LayerNorm. Many of previous studies believe that the success of LayerNorm comes from forward normalization. Unlike them, we find that the derivatives of the mean and variance are more important than forward normalization by re-centering and re-scaling backward gradients. Furthermore, we find that the parameters of LayerNorm, including the bias and gain, increase the risk of over-fitting and do not work in most cases. Experiments show that a simple version of LayerNorm (LayerNorm-simple) without the bias and gain <font color="#00be00">outperform</font>s LayerNorm on four datasets. It obtains the <font color="red">state-of-the-art</font> performance on En-Vi machine translation. To address the over-fitting problem, we propose a new normalization method, Adaptive Normalization (AdaNorm), by replacing the bias and gain with a new transformation function. Experiments show that AdaNorm demonstrates better results than LayerNorm on seven out of eight datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.04118.pdf'>1911.04118</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 1.1644баллов, №62</br>
<b>TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer\n  Sentence Selection</b></br>
Authors: , Garg, Siddhant, Vu, Thuy, Moschitti, Alessandro</br>
  We propose TANDA, an effective technique for fine-tuning pre-trained Transformer models for natural language tasks. Specifically, we first transfer a pre-trained model into a model for a general task by fine-tuning it with a large and high-quality dataset. We then perform a second fine-tuning step to adapt the transferred model to the target domain. We demonstrate the benefits of our approach for answer sentence selection, which is a well-known inference task in Question Answering. We built a large scale dataset to enable the transfer step, exploiting the Natural Questions dataset. Our approach establishes the <font color="red">state of the art</font> on two well-known benchmarks, WikiQA and TREC-QA, achieving MAP scores of 92% and 94.3%, respectively, which largely <font color="#00be00">outperform</font> the previous highest scores of 83.4% and 87.5%, obtained in very recent work. We empirically show that TANDA generates more stable and robust models reducing the effort required for selecting optimal hyper-parameters. Additionally, we show that the transfer step of TANDA makes the adaptation step more robust to noise. This enables a more effective use of noisy datasets for fine-tuning. Finally, we also confirm the positive impact of TANDA in an industrial setting, using domain specific datasets subject to different types of noise. </br></br>

<a href='http://arxiv.org/pdf/1911.07004.pdf'>1911.07004</a> &nbsp&nbsp (cs:CV, cs:ML, cs:NE) &nbsp&nbsp 1.1631баллов, №63</br>
<b>AETv2: AutoEncoding Transformations for Self-Supervised Representation\n  Learning by Minimizing Geodesic Distances in Lie Groups</b></br>
Authors: , Lin, Feng, Xu, Haohang, Li, Houqiang, Xiong, Hongkai, Qi, Guo-Jun</br>
  Self-supervised learning by predicting transformations has demonstrated outstanding performances in both unsupervised and (semi-)supervised tasks. Among the <font color="red">state-of-the-art</font> methods is the AutoEncoding Transformations (AET) by decoding transformations from the learned representations of original and transformed images. Both deterministic and probabilistic AETs rely on the Euclidean distance to measure the deviation of estimated transformations from their groundtruth counterparts. However, this assumption is questionable as a group of transformations often reside on a curved manifold rather staying in a flat Euclidean space. For this reason, we should use the geodesic to characterize how an image transform along the manifold of a transformation group, and adopt its length to measure the deviation between transformations. Particularly, we present to autoencode a Lie group of homography transformations PG(2) to learn image representations. For this, we make an estimate of the intractable Riemannian logarithm by projecting PG(2) to a subgroup of rotation transformations SO(3) that allows the closed-form expression of geodesic distances. Experiments demonstrate the proposed AETv2 model <font color="#00be00">outperform</font>s the previous version as well as the other state-of-the-art self-supervised models in multiple tasks. </br></br>

<a href='http://arxiv.org/pdf/1911.07251.pdf'>1911.07251</a> &nbsp&nbsp (cs:CV, cs:CL) &nbsp&nbsp 1.1619баллов, №64</br>
<b>DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in\n  Visual Dialogue</b></br>
Authors: , Jiang, Xiaoze, Yu, Jing, Qin, Zengchang, Zhuang, Yingying, Zhang, Xingxing, Hu, Yue, Wu, Qi</br>
  Different from Visual Question Answering task that requires to answer only one question about an image, Visual Dialogue involves multiple questions which cover a broad range of visual content that could be related to any objects, relationships or semantics. The key challenge in Visual Dialogue task is thus to learn a more comprehensive and semantic-rich image representation which may have adaptive attentions on the image for variant questions. In this research, we propose a novel model to depict an image from both visual and semantic perspectives. Specifically, the visual view helps capture the appearance-level information, including objects and their relationships, while the semantic view enables the agent to understand high-level visual semantics from the whole image to the local regions. Futhermore, on top of such multi-view image features, we propose a feature selection framework which is able to adaptively capture question-relevant information <font color="#00be00">hierarchical</font>ly in fine-grained level. The proposed method achieved <font color="red">state-of-the-art</font> results on benchmark Visual Dialogue datasets. More importantly, we can tell which modality (visual or semantic) has more contribution in answering the current question by visualizing the gate values. It gives us insights in understanding of human cognition in Visual Dialogue. </br></br>

<a href='http://arxiv.org/pdf/1910.08144.pdf'>1910.08144</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 1.1619баллов, №65</br>
<b>Explainable Authorship Verification in Social Media via Attention-based\n  Similarity Learning</b></br>
Authors: , Boenninghoff, Benedikt, Hessler, Steffen, Kolossa, Dorothea, Nickel, Robert M.</br>
  <font color="#be00be">Authorship</font> verification is the task of analyzing the linguistic patterns of two or more texts to determine whether they were written by the same author or not. The analysis is traditionally performed by experts who consider linguistic features, which include spelling mistakes, grammatical inconsistencies, and stylistics for example. Machine learning algorithms, on the other hand, can be trained to accomplish the same, but have traditionally relied on so-called stylometric features. The disadvantage of such features is that their reliability is greatly diminished for short and topically varied social media texts. In this interdisciplinary work, we propose a substantial extension of a recently published <font color="#00be00">hierarchical</font> Siamese neural network approach, with which it is feasible to learn neural features and to visualize the decision-making process. For this purpose, a new large-scale corpus of short Amazon reviews for text comparison research is compiled and we show that the Siamese network topologies <font color="#00be00">outperform</font> <font color="red">state-of-the-art</font> approaches that were built up on stylometric features. Our linguistic analysis of the internal attention weights of the network shows that the proposed method is indeed able to latch on to some traditional linguistic categories. </br></br>

<a href='http://arxiv.org/pdf/1911.09419.pdf'>1911.09419</a> &nbsp&nbsp (cs:ML, cs:CL, stat:ML) &nbsp&nbsp 1.1533баллов, №66</br>
<b>Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction</b></br>
Authors: , Zhang, Zhanqiu, Cai, Jianyu, Zhang, Yongdong, Wang, Jie</br>
  <font color="#960096">Knowledge graph</font> embedding, which aims to represent entities and relations as low dimensional vectors (or matrices, tensors, etc.), has been shown to be a powerful technique for predicting missing links in knowledge graphs. Existing knowledge graph embedding models mainly focus on modeling relation patterns such as symmetry/antisymmetry, inversion, and composition. However, many existing approaches fail to model semantic hierarchies, which are common in <font color="#009600">real-world</font> applications. To address this challenge, we propose a novel knowledge graph embedding model---namely, Hierarchy-Aware Knowledge Graph Embedding (HAKE)---which maps entities into the polar coordinate system. HAKE is inspired by the fact that concentric circles in the polar coordinate system can naturally reflect the hierarchy. Specifically, the radial coordinate aims to model entities at different levels of the hierarchy, and entities with smaller radii are expected to be at higher levels; the angular coordinate aims to distinguish entities at the same level of the hierarchy, and these entities are expected to have roughly the same radii but different angles. Experiments demonstrate that HAKE can effectively model the semantic hierarchies in knowledge graphs, and significantly <font color="#00be00">outperform</font>s existing <font color="red">state-of-the-art</font> methods on benchmark datasets for the link prediction task. </br></br>

<a href='http://arxiv.org/pdf/1911.08836.pdf'>1911.08836</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 1.1448баллов, №67</br>
<b>Table-Of-Contents generation on contemporary documents</b></br>
Authors: , Bentabet, Najah-Imane, Juge, R&#xe9;mi, Ferradans, Sira</br>
  The generation of precise and detailed Table-Of-Contents (TOC) from a document is a problem of major importance for document understanding and information extraction. Despite its importance, it is still a challenging task, especially for non-standardized documents with rich layout information such as commercial documents. In this paper, we present a new neural-based pipeline for TOC generation applicable to any searchable document. Unlike previous methods, we do not use semantic labeling nor assume the presence of parsable TOC pages in the document. Moreover, we analyze the influence of using external knowledge encoded as a template. We empirically show that this approach is only useful in a very low resource environment. Finally, we propose a new domain-specific data set that sheds some light on the difficulties of TOC generation in <font color="#009600">real-world</font> documents. The proposed method shows better performance than the <font color="red">state-of-the-art</font> on a public data set and on the newly released data set. </br></br>

<a href='http://arxiv.org/pdf/1910.09706.pdf'>1910.09706</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 1.1290баллов, №68</br>
<b>Collaborative Graph Walk for Semi-supervised Multi-Label Node\n  Classification</b></br>
Authors: , Akujuobi, Uchenna, Yufei, Han, Zhang, Qiannan, Zhang, Xiangliang</br>
  In this work, we study semi-supervised multi-label node classification problem in attributed graphs. Classic solutions to multi-label node classification follow two steps, first learn node embedding and then build a node classifier on the learned embedding. To improve the discriminating power of the node embedding, we propose a novel collaborative graph walk, named Multi-Label-Graph-Walk, to finely tune node representations with the available label assignments in attributed graphs via <font color="#00be00">reinforcement learning</font>. The proposed method formulates the multi-label node classification task as simultaneous graph walks conducted by multiple label-specific agents. Furthermore, policies of the label-wise graph walks are learned in a cooperative way to capture first the predictive relation between node labels and structural attributes of graphs; and second, the correlation among the multiple label-specific classification tasks. A comprehensive experimental study demonstrates that the proposed method can achieve significantly better multi-label classification performance than the <font color="red">state-of-the-art</font> approaches and conduct more efficient graph exploration. </br></br>

<a href='http://arxiv.org/pdf/1911.08151.pdf'>1911.08151</a> &nbsp&nbsp (cs:CL, cs:AI) &nbsp&nbsp 1.1157баллов, №69</br>
<b>Retrospective and Prospective Mixture-of-Generators for Task-oriented\n  Dialogue Response Generation</b></br>
Authors: , Pei, Jiahuan, Ren, Pengjie, Monz, Christof, de Rijke, Maarten</br>
  Dialogue response generation (DRG) is a critical component of task-oriented dialogue systems (TDSs). Its purpose is to generate proper natural language responses given some context, e.g., historical utterances, system states, etc. <font color="red">State-of-the-art</font> work focuses on how to better tackle DRG in an end-to-end way. Typically, such studies assume that each token is drawn from a single distribution over the output vocabulary, which may not always be optimal. Responses vary greatly with different intents, e.g., domains, system actions.   We propose a novel mixture-of-generators network (MoGNet) for DRG, where we assume that each token of a response is drawn from a mixture of distributions. MoGNet consists of a chair generator and several expert generators. Each expert is specialized for DRG w.r.t. a particular intent. The chair coordinates multiple experts and combines the output they have generated to produce more appropriate responses. We propose two strategies to help the chair make better decisions, namely, a retrospective mixture-of-generators (RMoG) and prospective mixture-of-generators (PMoG). The former only considers the historical expert-generated responses until the current time step while the latter also considers possible expert-generated responses in the future by encouraging exploration. In order to differentiate experts, we also devise a global-and-local (GL) learning scheme that forces each expert to be specialized towards a particular intent using a local loss and trains the chair and all experts to coordinate using a global loss.   We carry out extensive experiments on the MultiWOZ benchmark dataset. MoGNet significantly <font color="#00be00">outperform</font>s state-of-the-art methods in terms of both automatic and human evaluations, demonstrating its effectiveness for DRG. </br></br>

<a href='http://arxiv.org/pdf/1911.07144.pdf'>1911.07144</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.1129баллов, №70</br>
<b>Extra Proximal-Gradient Inspired Non-local Network</b></br>
Authors: , Zhang, Qingchao, Chen, Yunmei</br>
  Variational method and deep learning method are two mainstream powerful approaches to solve inverse problems in computer vision. To take advantages of advanced optimization algorithms and powerful representation ability of deep neural networks, we propose a novel deep network for image reconstruction. The architecture of this network is inspired by our proposed accelerated extra proximal gradient algorithm. It is able to incorporate non-local operation to exploit the non-local self-similarity of the images and to learn the nonlinear transform, under which the solution is sparse. All the parameters in our network are learned from minimizing a loss function. Our experimental results show that our network <font color="#00be00">outperform</font>s several <font color="red">state-of-the-art</font> deep networks with almost the same number of learnable parameter. </br></br>

<a href='http://arxiv.org/pdf/1911.07177.pdf'>1911.07177</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.1060баллов, №71</br>
<b>Fast Color Constancy with Patch-wise Bright Pixels</b></br>
Authors: , Shi, Yiyao, Wang, Jian, Xue, Xiangyang</br>
  In this paper, a learning-free color constancy algorithm called the Patch-wise Bright Pixels (PBP) is proposed. In this algorithm, an input image is first downsampled and then cut equally into a few patches. After that, according to the modified brightness of each patch, a proper fraction of brightest pixels in the patch is selected. Finally, Gray World (GW)-based methods are applied to the selected bright pixels to estimate the illuminant of the scene. Experiments on NUS $8$-Camera Dataset show that the PBP algorithm <font color="#00be00">outperform</font>s the <font color="red">state-of-the-art</font> learning-free methods as well as a broad range of learning-based ones. In particular, PBP processes a $1080$p image within two milliseconds, which is hundreds of times faster than the existing learning-free ones. Our algorithm offers a potential solution to the full-screen smart phones whose screen-to-body ratio is $100$\\%. </br></br>

<a href='http://arxiv.org/pdf/1911.07410.pdf'>1911.07410</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.1043баллов, №72</br>
<b>Multi-Temporal Recurrent Neural Networks For Progressive Non-Uniform\n  Single Image Deblurring With Incremental Temporal Training</b></br>
Authors: , Park, Dongwon, Kang, Dong Un, Kim, Jisoo, Chun, Se Young</br>
  Multi-scale (MS) approaches have been widely investigated for blind single image / video deblurring that sequentially recovers deblurred images in low spatial scale first and then in high spatial scale later with the output of lower scales. MS approaches have been effective especially for severe blurs induced by large motions in high spatial scale since those can be seen as small blurs in low spatial scale. In this work, we investigate alternative approach to MS, called multi-temporal (MT) approach, for non-uniform single image deblurring. We propose incremental temporal training with constructed MT level dataset from time-resolved dataset, develop novel MT-RNNs with recurrent feature maps, and investigate progressive single image deblurring over iterations. Our proposed MT methods <font color="#00be00">outperform</font> <font color="red">state-of-the-art</font> MS methods on the GoPro dataset in PSNR with the smallest number of parameters. </br></br>

<a href='http://arxiv.org/pdf/1911.07440.pdf'>1911.07440</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.0976баллов, №73</br>
<b>Large Scale Open-Set Deep Logo Detection</b></br>
Authors: , Bastan, Muhammet, Wu, Hao-Yu, Cao, Tian, Kota, Bhargava, Tek, Mehmet</br>
  We present an open-set logo detection (OSLD) system, which can detect (localize and recognize) any number of unseen logo classes without re-training; it only requires a small set of canonical logo images for each logo class. We achieve this using a two-stage approach: (1) Generic logo detection to detect candidate logo regions in an image. (2) Logo matching for matching the detected logo regions to a set of canonical logo images to recognize them. We also introduce a \'simple deep metric learning\' (SDML) framework that <font color="#00be00">outperform</font>ed more complicated ensemble and attention models and boosted the logo matching accuracy. Furthermore, we constructed a new open-set logo detection dataset with thousands of logo classes, and will release it for research purposes. We demonstrate the effectiveness of OSLD on our dataset and on the standard Flickr-32 logo dataset, outperforming the <font color="red">state-of-the-art</font> open-set and closed-set logo detection methods by a large margin. </br></br>

<a href='http://arxiv.org/pdf/1911.08650.pdf'>1911.08650</a> &nbsp&nbsp (cs:NE, cs:ML) &nbsp&nbsp 1.0932баллов, №74</br>
<b>Genetic Programming Hyper-Heuristics with Vehicle Collaboration for\n  Uncertain Capacitated Arc Routing Problems</b></br>
Authors: , MacLachlan, Jordan, Mei, Yi, Branke, Juergen, Zhang, Mengjie</br>
  Due to its direct relevance to post-disaster operations, meter reading and civil refuse collection, the Uncertain Capacitated Arc Routing Problem (UCARP) is an important optimisation problem. Stochastic models are critical to study as they more accurately represent the <font color="#009600">real-world</font> than their deterministic counterparts. Although there have been extensive studies in solving routing problems under uncertainty, very few have considered UCARP, and none consider collaboration between vehicles to handle the negative effects of uncertainty. This paper proposes a novel Solution Construction Procedure (SCP) that generates solutions to UCARP within a collaborative, multi-vehicle framework. It consists of two types of collaborative activities: one when a vehicle unexpectedly expends capacity (\\emph{route failure}), and the other during the refill process. Then, we propose a <font color="#be00be">Genetic</font> Programming Hyper-Heuristic (GPHH) algorithm to evolve the routing policy used within the collaborative framework. The experimental studies show that the new heuristic with vehicle collaboration and GP-evolved routing policy significantly <font color="#00be00">outperform</font>s the compared <font color="red">state-of-the-art</font> algorithms on commonly studied test problems. This is shown to be especially true on instances with larger numbers of tasks and vehicles. This clearly shows the advantage of vehicle collaboration in handling the uncertain environment, and the effectiveness of the newly proposed algorithm. </br></br>

<a href='http://arxiv.org/pdf/1911.04610.pdf'>1911.04610</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 1.0874баллов, №75</br>
<b>XPipe: Efficient Pipeline Model Parallelism for Multi-GPU DNN Training</b></br>
Authors: , Guan, Lei, Yin, Wotao, Li, Dongsheng, Lu, Xicheng</br>
  We propose XPipe, an efficient asynchronous pipeline model parallelism approach for multi-GPU DNN training. XPipe is designed to make use of multiple GPUs to concurrently and continuously train different parts of a DNN model. To improve GPU utilization and achieve high throughput, it splits a mini-batch into a set of micro-batches and allows the overlapping of the pipelines of multiple micro-batches, including those belonging to different mini-batches. Most importantly, the novel weight prediction strategy adopted by XPipe enables it to effectively address the weight inconsistency and staleness issues incurred by the asynchronous pipeline parallelism. As a result, XPipe incorporates the advantages of both synchronous and asynchronous pipeline model parallelism approaches. Concretely, it can achieve very comparable (even slightly better) model accuracy as its synchronous counterpart, while obtaining higher throughput than it. Experimental results show that XPipe <font color="#00be00">outperform</font>s other <font color="red">state-of-the-art</font> synchronous and asynchronous model parallelism approaches. </br></br>

<a href='http://arxiv.org/pdf/1911.07848.pdf'>1911.07848</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 1.0874баллов, №76</br>
<b>Modality To Modality Translation: An Adversarial Representation Learning\n  and Graph Fusion Network for Multimodal Fusion</b></br>
Authors: , Mai, Sijie, Hu, Haifeng, Xing, Songlong</br>
  Learning joint embedding space for various modalities is of vital importance for multimodal fusion. Mainstream modality fusion approaches fail to achieve this goal, leaving a modality gap which heavily affects cross-modal fusion. In this paper, we propose a novel adversarial encoder-decoder-classifier framework to learn a modality-invariant embedding space. Since the distributions of various modalities vary in nature, to reduce the modality gap, we translate the distributions of source modalities into that of target modality via their respective encoders using adversarial training. Furthermore, we exert additional constraints on embedding space by introducing reconstruction and classification losses. Then we fuse the encoded representations using <font color="#00be00">hierarchical</font> graph neural network which explicitly explores unimodal, bimodal and trimodal interactions in multi-stage. Our method achieves <font color="red">state-of-the-art</font> performances on multiple datasets. Visualization of the learned embeddings suggests that the joint embedding space learned by our method is discriminative. </br></br>

<a href='http://arxiv.org/pdf/1911.05369.pdf'>1911.05369</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp 1.0873баллов, №77</br>
<b>Fair Adversarial Gradient Tree Boosting</b></br>
Authors: , Grari, Vincent, Ruf, Boris, Lamprier, Sylvain, Detyniecki, Marcin</br>
  Fair classification has become an important topic in machine learning research. While most bias mitigation strategies focus on neural networks, we noticed a lack of work on fair classifiers based on decision trees even though they have proven very efficient. In an up-to-date comparison of <font color="red">state-of-the-art</font> classification algorithms in tabular data, tree boosting <font color="#00be00">outperform</font>s deep learning. For this reason, we have developed a novel approach of adversarial gradient tree boosting. The objective of the algorithm is to predict the output $Y$ with gradient tree boosting while minimizing the ability of an adversarial neural network to predict the sensitive attribute $S$. The approach incorporates at each iteration the gradient of the neural network directly in the gradient tree boosting. We empirically assess our approach on 4 popular data sets and compare against state-of-the-art algorithms. The results show that our algorithm achieves a higher accuracy while obtaining the same level of fairness, as measured using a set of different common fairness definitions. </br></br>

<a href='http://arxiv.org/pdf/1911.07346.pdf'>1911.07346</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp 1.0858баллов, №78</br>
<b>Any-Precision Deep Neural Networks</b></br>
Authors: , Yu, Haichao, Li, Haoxiang, Shi, Honghui, Huang, Thomas S., Hua, Gang</br>
  We present Any-Precision Deep Neural Networks (Any-Precision DNNs), which are trained with a new method that empowers learned DNNs to be flexible in any numerical precision during inference. The same model in runtime can be flexibly and directly set to different bit-width, by truncating the least significant bits, to support dynamic speed and accuracy trade-off. When all layers are set to low-bits, we show that the model achieved accuracy comparable to dedicated models trained at the same precision. This nice property facilitates flexible deployment of deep learning models in <font color="#009600">real-world</font> applications, where in practice trade-offs between model accuracy and runtime efficiency are often sought. Previous literature presents solutions to train models at each individual fixed efficiency/accuracy trade-off point. But how to produce a model flexible in runtime precision is largely unexplored. When the demand of efficiency/accuracy trade-off varies from time to time or even dynamically changes in runtime, it is infeasible to re-train models accordingly, and the storage budget may forbid keeping multiple models. Our proposed framework achieves this flexibility without performance degradation. More importantly, we demonstrate that this achievement is agnostic to model architectures. We experimentally validated our method with different deep network backbones (AlexNet-small, Resnet-20, Resnet-50) on different datasets (SVHN, Cifar-10, ImageNet) and observed consistent results. Code and models will be available at <font color="#006400">http</font>s://<font color="red">github</font>.com/haichaoyu. </br></br>

<a href='http://arxiv.org/pdf/1911.07183.pdf'>1911.07183</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 1.0836баллов, №79</br>
<b>Scale- and Context-Aware Convolutional Non-intrusive Load Monitoring</b></br>
Authors: , Chen, Kunjin, Zhang, Yu, Wang, Qin, Hu, Jun, Fan, Hang, He, Jinliang</br>
  Non-intrusive load monitoring addresses the challenging task of decomposing the aggregate signal of a household\'s electricity consumption into appliance-level data without installing dedicated meters. By detecting load malfunction and recommending energy reduction programs, cost-effective non-intrusive load monitoring provides intelligent demand-side management for utilities and end users. In this paper, we boost the accuracy of energy disaggregation with a novel neural network structure named scale- and context-aware network, which exploits multi-scale features and contextual information. Specifically, we develop a multi-branch architecture with multiple receptive field sizes and branch-wise gates that connect the branches in the sub-networks. We build a self-attention module to facilitate the integration of global context, and we incorporate an adversarial loss and on-state augmentation to further improve the model\'s performance. Extensive simulation results tested on open datasets corroborate the merits of the proposed approach, which significantly <font color="#00be00">outperform</font>s <font color="red">state-of-the-art</font> methods. </br></br>

<a href='http://arxiv.org/pdf/1911.06905.pdf'>1911.06905</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 1.0835баллов, №80</br>
<b>Coupling Matrix Manifolds and Their Applications in Optimal Transport</b></br>
Authors: , Shi, Dai, Gao, Junbin, Hong, Xia, Choy, S. T. Boris, Wang, Zhiyong</br>
  Optimal transport (OT) is a powerful tool for measuring the distance between two defined probability distributions. In this paper, we develop a new manifold named the coupling matrix manifold (CMM), where each point on CMM can be regarded as the transportation plan of the OT problem. We firstly explore the Riemannian geometry of CMM with the metric expressed by the Fisher information. These geometrical features of CMM have paved the way for developing numerical Riemannian optimization algorithms such as Riemannian gradient descent and Riemannian trust-region algorithms, forming a uniform optimization method for all types of OT problems. The proposed method is then applied to solve several OT problems studied by previous literature. The results of the numerical experiments illustrate that the optimization algorithms that are based on the method proposed in this paper are comparable to the classic ones, for example, the Sinkhorn algorithm, while <font color="#00be00">outperform</font>ing other <font color="red">state-of-the-art</font> algorithms without considering the geometry information, especially in the case of non-entropy optimal transport. </br></br>

<a href='http://arxiv.org/pdf/1911.09046.pdf'>1911.09046</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp 1.0791баллов, №81</br>
<b>Heterogeneous Graph-based Knowledge Transfer for Generalized Zero-shot\n  Learning</b></br>
Authors: , Wang, Junjie, Wang, Xiangfeng, Jin, Bo, Yan, Junchi, Zhang, Wenjie, Zha, Hongyuan</br>
  Generalized <font color="#00be00">zero-shot</font> learning (GZSL) tackles the problem of learning to classify instances involving both seen classes and unseen ones. The key issue is how to effectively transfer the model learned from seen classes to unseen classes. Existing works in GZSL usually assume that some prior information about unseen classes are available. However, such an assumption is unrealistic when new unseen classes appear dynamically. To this end, we propose a novel heterogeneous graph-based knowledge transfer method (HGKT) for GZSL, agnostic to unseen classes and instances, by leveraging graph neural network. Specifically, a structured heterogeneous graph is constructed with high-level representative nodes for seen classes, which are chosen through Wasserstein barycenter in order to simultaneously capture inter-class and intra-class relationship. The aggregation and embedding functions can be learned through graph neural network, which can be used to compute the embeddings of unseen classes by transferring the knowledge from their neighbors. Extensive experiments on public benchmark datasets show that our method achieves <font color="red">state-of-the-art</font> results. </br></br>

<a href='http://arxiv.org/pdf/1911.08114.pdf'>1911.08114</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.0763баллов, №82</br>
<b>Neural Network Pruning with Residual-Connections and Limited-Data</b></br>
Authors: , Luo, Jian-Hao, Wu, Jianxin</br>
  Filter level pruning is an effective method to accelerate the inference speed of deep CNN models. Although numerous pruning algorithms have been proposed, there are still two open issues. The first problem is how to prune residual connections. Most previous filter level pruning algorithms only prune channels inside residual blocks, leaving the number of output channels unchanged. We show that pruning both channels inside and outside the residual connections is crucial to achieve better performance. The second issue is pruning with limited data. We observe an interesting phenomenon: directly pruning on a small dataset is usually worse than fine-tuning a small model which is pruned or trained from scratch on the large dataset. In this paper, we propose a novel method, namely Compression Using Residual-connections and Limited-data (CURL), to tackle these two challenges. Experiments on the large scale dataset demonstrate the effectiveness of CURL. CURL significantly <font color="#00be00">outperform</font>s previous <font color="red">state-of-the-art</font> methods on ImageNet. More importantly, when pruning on small datasets, CURL achieves comparable or much better performance than fine-tuning a pretrained small model. </br></br>

<a href='http://arxiv.org/pdf/1911.07046.pdf'>1911.07046</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.0762баллов, №83</br>
<b>SA-Text: Simple but Accurate Detector for Text of Arbitrary Shapes</b></br>
Authors: , Wang, Qitong, Zheng, Yi, Betke, Margrit</br>
  We introduce a new framework for text detection named SA-Text meaning &quot;Simple but Accurate,&quot; which utilizes heatmaps to detect text regions in natural scene images effectively. SA-Text detects text that occurs in various fonts, shapes, and orientations in natural scene images with complicated backgrounds. Experiments on three challenging and public scene-text-detection datasets, Total-Text, SCUT-CTW1500, and MSRA-TD500 show the effectiveness and generalization ability of SA-Text in detecting not only multi-lingual oriented straight but also curved text in scripts of multiple languages. To further show the practicality of SA-Text, we combine it with a powerful <font color="red">state-of-the-art</font> text recognition model and thus propose a pipeline-based text spotting system called SAA (&quot;text spotting&quot; is used as the technical term for &quot;detection and recognition of text&quot;). Our experimental results of SAA on the Total-Text dataset show that SAA <font color="#00be00">outperform</font>s four state-of-the-art text spotting frameworks by at least 9 percent points in the F-measure, which means that SA-Text can be used as a complete text detection and recognition system in real applications. </br></br>

<a href='http://arxiv.org/pdf/1911.07203.pdf'>1911.07203</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 1.0727баллов, №84</br>
<b>Prototypical Networks for Multi-Label Learning</b></br>
Authors: , Yang, Zhuo, Han, Yufei, Yu, Guoxian, Zhang, Xiangliang</br>
  We propose to address multi-label learning by jointly estimating the distribution of positive and negative instances for all labels. By a shared mapping function, each label\'s positive and negative instances are mapped into a new space forming a mixture distribution of two components (positive and negative). Due to the dependency among labels, positive instances are mapped close if they share common labels, while positive and negative embeddings of the same label are pushed away. The distribution is learned in the new space, and thus well presents both the distance between instances in their original feature space and their common membership w.r.t. different categories. By measuring the density function values, new instances mapped to the new space can easily identify their membership to possible multiple categories. We use neural networks for learning the mapping function and use the expectations of the positive and negative embedding as prototypes of the positive and negative components for each label, respectively. Therefore, we name our proposed method PNML (prototypical networks for multi-label learning). Extensive experiments verify that PNML significantly <font color="#00be00">outperform</font>s the <font color="red">state-of-the-art</font>s. </br></br>

<a href='http://arxiv.org/pdf/1911.08995.pdf'>1911.08995</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.0698баллов, №85</br>
<b>Unsupervised Monocular Depth Prediction for Indoor Continuous Video\n  Streams</b></br>
Authors: , Feng, Yinglong, Wu, Shuncheng, K&#xf6;p&#xfc;kl&#xfc;, Okan, Kang, Xueyang, Tombari, Federico</br>
  This paper studies unsupervised monocular depth prediction problem. Most of existing unsupervised depth prediction algorithms are developed for outdoor scenarios, while the depth prediction work in the indoor environment is still very scarce to our knowledge. Therefore, this work focuses on narrowing the gap by firstly evaluating existing approaches in the indoor environments and then improving the <font color="red">state-of-the-art</font> design of architecture. Unlike typical outdoor training dataset, such as KITTI with motion constraints, data for indoor environment contains more arbitrary camera movement and short baseline between two consecutive images, which deteriorates the network training for the pose estimation. To address this issue, we propose two methods: Firstly, we propose a novel reconstruction loss function to constraint pose estimation, resulting in accuracy improvement of the predicted disparity map; secondly, we use an ensemble learning with a flipping strategy along with a median filter, directly taking operation on the output disparity map. We evaluate our approaches on the TUM RGB-D and self-collected datasets. The results have shown that both approaches <font color="#00be00">outperform</font> the previous state-of-the-art unsupervised learning approaches. </br></br>

<a href='http://arxiv.org/pdf/1911.09427.pdf'>1911.09427</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 1.0628баллов, №86</br>
<b>Accurate Hydrologic Modeling Using Less Information</b></br>
Authors: , Shalev, Guy, El-Yaniv, Ran, Klotz, Daniel, Kratzert, Frederik, Metzger, Asher, Nevo, Sella</br>
  Joint models are a common and important tool in the intersection of machine learning and the physical sciences, particularly in contexts where <font color="#009600">real-world</font> measurements are scarce. Recent developments in rainfall-runoff modeling, one of the prime challenges in hydrology, show the value of a joint model with shared representation in this important context. However, current <font color="red">state-of-the-art</font> models depend on detailed and reliable attributes characterizing each site to help the model differentiate correctly between the behavior of different sites. This dependency can present a challenge in data-poor regions. In this paper, we show that we can replace the need for such location-specific attributes with a completely data-driven learned embedding, and match previous state-of-the-art results with less information. </br></br>

<a href='http://arxiv.org/pdf/1911.09074.pdf'>1911.09074</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 1.0628баллов, №87</br>
<b>Search to Distill: Pearls are Everywhere but not the Eyes</b></br>
Authors: , Liu, Yu, Jia, Xuhui, Tan, Mingxing, Vemulapalli, Raviteja, Zhu, Yukun, Green, Bradley, Wang, Xiaogang</br>
  Standard Knowledge Distillation (KD) approaches distill the knowledge of a cumbersome teacher model into the parameters of a student model with a pre-defined architecture. However, the knowledge of a neural network, which is represented by the network\'s output distribution conditioned on its input, depends not only on its parameters but also on its architecture. Hence, a more generalized approach for KD is to distill the teacher\'s knowledge into both the parameters and architecture of the student. To achieve this, we present a new Architecture-aware Knowledge Distillation (AKD) approach that finds student models (pearls for the teacher) that are best for distilling the given teacher model. In particular, we leverage Neural <font color="#00be00">Architecture Search</font> (NAS), equipped with our KD-guided reward, to search for the best student architectures for a given teacher. Experimental results show our proposed AKD consistently <font color="#00be00">outperform</font>s the conventional NAS plus KD approach, and achieves <font color="red">state-of-the-art</font> results on the ImageNet classification task under various latency settings. Furthermore, the best AKD student architecture for the ImageNet classification task also transfers well to other tasks such as million level<font color="#be00be"> face </font>recognition and ensemble learning. </br></br>

<a href='http://arxiv.org/pdf/1910.02060.pdf'>1910.02060</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 1.0594баллов, №88</br>
<b>Neural Puppet: Generative Layered Cartoon Characters</b></br>
Authors: , Poursaeed, Omid, Kim, Vladimir G., Shechtman, Eli, Saito, Jun, Belongie, Serge</br>
  We propose a learning based method for generating new animations of a cartoon character given a few example images. Our method is designed to learn from a traditionally animated sequence, where each frame is drawn by an artist, and thus the input images lack any common structure, correspondences, or labels. We express pose changes as a deformation of a layered 2.5D template mesh, and devise a novel architecture that learns to predict mesh deformations matching the template to a target image. This enables us to extract a common low-dimensional structure from a diverse set of character poses. We combine recent advances in differentiable rendering as well as mesh-aware models to successfully align common template even if only a few character images are available during training. In addition to coarse poses, character appearance also varies due to shading, out-of-plane motions, and artistic effects. We capture these subtle changes by applying an image translation network to refine the mesh rendering, providing an end-to-end model to generate new animations of a character with high visual quality. We demonstrate that our generative model can be used to synthesize in-between frames and to create data-driven deformation. Our template fitting procedure <font color="#00be00">outperform</font>s <font color="red">state-of-the-art</font> generic techniques for detecting image correspondences. </br></br>

<a href='http://arxiv.org/pdf/1911.08206.pdf'>1911.08206</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.0590баллов, №89</br>
<b>Mimic The Raw Domain: Accelerating Action Recognition in the Compressed\n  Domain</b></br>
Authors: , Battash, Barak, Barad, Haim, Tang, Hanlin, Bleiweiss, Amit</br>
  Video understanding usually requires expensive computation that prohibits its deployment, yet videos contain significant spatiotemporal redundancy that can be exploited. In particular, operating directly on the motion vectors and residuals in the compressed video domain can significantly accelerate the compute, by not using the raw videos which demand colossal storage capacity. Existing methods approach this task as a multiple modalities problem. In this paper we are approaching the task in a completely different way; we are looking at the data from the compressed stream as a one unit clip and propose that the residual frames can replace the original RGB frames from the raw domain. Furthermore, we are using teacher-student method to aid the network in the compressed domain to mimic the teacher network in the raw domain. We show experiments on three leading datasets (HMDB51, UCF1, and Kinetics) that approach <font color="red">state-of-the-art</font> accuracy on raw video data by using compressed data. Our model MFCD-Net <font color="#00be00">outperform</font>s prior methods in the compressed domain and more importantly, our model has 11X fewer parameters and 3X fewer Flops, dramatically improving the efficiency of video recognition inference. This approach enables applying neural networks exclusively in the compressed domain without compromising accuracy while accelerating performance. </br></br>

<a href='http://arxiv.org/pdf/1911.08400.pdf'>1911.08400</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.0585баллов, №90</br>
<b>KISS: Keeping It Simple for Scene Text Recognition</b></br>
Authors: , Bartz, Christian, Bethge, Joseph, Yang, Haojin, Meinel, Christoph</br>
  Over the past few years, several new methods for scene text recognition have been proposed. Most of these methods propose novel building blocks for neural networks. These novel building blocks are specially tailored for the task of scene text recognition and can thus hardly be used in any other tasks. In this paper, we introduce a new model for scene text recognition that only consists of off-the-shelf building blocks for neural networks. Our model (KISS) consists of two ResNet based feature extractors, a spatial transformer, and a transformer. We train our model only on <font color="#00be00">publicly available</font>, synthetic training data and evaluate it on a range of scene text recognition benchmarks, where we reach <font color="red">state-of-the-art</font> or <font color="#960096">competitive</font> performance, although our model does not use methods like 2D-attention, or image rectification. </br></br>

<a href='http://arxiv.org/pdf/1911.09389.pdf'>1911.09389</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.0558баллов, №91</br>
<b>Classification-driven Single Image Dehazing</b></br>
Authors: , Pei, Yanting, Huang, Yaping, Zhang, Xingyuan</br>
  Most existing dehazing algorithms often use hand-crafted features or Convolutional Neural Networks (CNN)-based methods to generate clear images using pixel-level Mean Square Error (MSE) loss. The generated images generally have better visual appeal, but not always have better performance for high-level vision tasks, e.g. image classification. In this paper, we investigate a new point of view in addressing this problem. Instead of focusing only on achieving good quantitative performance on pixel-based metrics such as Peak Signal to Noise Ratio (PSNR), we also ensure that the dehazed image itself does not degrade the performance of the high-level vision tasks such as image classification. To this end, we present an unified CNN architecture that includes three parts: a dehazing sub-network (DNet), a classification-driven Conditional Generative Adversarial Networks sub-network (CCGAN) and a classification sub-network (CNet) related to image classification, which has better performance both on visual appeal and image classification. We conduct comprehensive experiments on two challenging benchmark datasets for fine-grained and object classification: CUB-200-2011 and Caltech-256. Experimental results demonstrate that the proposed method <font color="#00be00">outperform</font>s many recent <font color="red">state-of-the-art</font> single image dehazing methods in terms of image dehazing metrics and classification accuracy. </br></br>

<a href='http://arxiv.org/pdf/1911.07771.pdf'>1911.07771</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.0502баллов, №92</br>
<b>MaskedFusion: Mask-based 6D Object Pose Detection</b></br>
Authors: , Pereira, Nuno, Alexandre, Lu&#xed;s A.</br>
  MaskedFusion is a framework to estimate 6D pose of objects using RGB-D data, with an architecture that leverages multiple stages in a pipeline to achieve accurate 6D poses. 6D pose estimation is an open challenge due to complex world objects and many possible problems when capturing data from the <font color="#009600">real world</font>, e.g., occlusions, truncations, and noise in the data. Achieving accurate 6D poses will improve results in other open problems like robot grasping or positioning objects in augmented reality. MaskedFusion improves upon DenseFusion where the key differences are in pre-processing data before it enters the Neural Network (NN), eliminating non-relevant data, and adding additional features extracted from the mask of the objects to the NN to improve its estimation. It achieved $5.9mm$ average error on the widely used LineMOD dataset, which is an improvement, of more than 20\\%, compared to the <font color="red">state-of-the-art</font> method, DenseFusion. </br></br>

<a href='http://arxiv.org/pdf/1911.06878.pdf'>1911.06878</a> &nbsp&nbsp (cs:SD) &nbsp&nbsp 1.0467баллов, №93</br>
<b>Adaptive Multi-scale Detection of Acoustic Events</b></br>
Authors: , Ding, Wenhao, He, Liang</br>
  The goal of acoustic (or sound) events detection (AED or SED) is to predict the temporal position of target events in given audio segments. This task plays a significant role in safety monitoring, acoustic early warning and other scenarios. However, the deficiency of data and diversity of acoustic event sources make the AED task a tough issue, especially for prevalent data-driven methods. In this paper, we start by analyzing acoustic events according to their time-frequency domain properties, showing that different acoustic events have different time-frequency scale characteristics. Inspired by the analysis, we propose an adaptive multi-scale detection (AdaMD) method. By taking advantage of the hourglass neural network and gated recurrent unit (GRU) module, our AdaMD produces multiple predictions at different temporal and frequency resolutions. An adaptive training algorithm is subsequently adopted to combine multi-scale predictions to enhance its overall capability. Experimental results on Detection and Classification of Acoustic Scenes and Events 2017 (DCASE 2017) Task 2, DCASE 2016 Task 3 and DCASE 2017 Task 3 demonstrate that the AdaMD <font color="#00be00">outperform</font>s published <font color="red">state-of-the-art</font> competitors in terms of the metrics of event error rate (ER) and F1-score. The verification experiment on our collected factory mechanical dataset also proves the noise-resistant capability of the AdaMD, providing the possibility for it to be deployed in the complex environment. </br></br>

<a href='http://arxiv.org/pdf/1911.07883.pdf'>1911.07883</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 1.0446баллов, №94</br>
<b>Vision-Language Navigation with Self-Supervised Auxiliary Reasoning\n  Tasks</b></br>
Authors: , Zhu, Fengda, Zhu, Yi, Chang, Xiaojun, Liang, Xiaodan</br>
  Vision-Language Navigation (VLN) is a task where agents learn to navigate following natural language instructions. The key to this task is to perceive both the visual scene and natural language sequentially. Conventional approaches exploit the vision and language features in cross-modal grounding. However, the VLN task remains challenging, since previous works have neglected the rich semantic information contained in the environment (such as implicit navigation graphs or sub-trajectory semantics). In this paper, we introduce Auxiliary Reasoning Navigation (AuxRN), a framework with four self-supervised auxiliary reasoning tasks to take advantage of the additional training signals derived from the semantic information. The auxiliary tasks have four reasoning objectives: explaining the previous actions, estimating the navigation progress, predicting the next orientation, and evaluating the trajectory consistency. As a result, these additional training signals help the agent to acquire knowledge of semantic representations in order to reason about its activity and build a thorough perception of the environment. Our experiments indicate that auxiliary reasoning tasks improve both the performance of the main task and the model generalizability by a large margin. Empirically, we demonstrate that an agent trained with self-supervised auxiliary reasoning tasks substantially <font color="#00be00">outperform</font>s the previous <font color="red">state-of-the-art</font> method, being the best existing approach on the standard benchmark. </br></br>

<a href='http://arxiv.org/pdf/1911.07543.pdf'>1911.07543</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 1.0410баллов, №95</br>
<b>Multi-Task Learning of Height and Semantics from Aerial Images</b></br>
Authors: , Carvalho, Marcela, Saux, Bertrand Le, Trouv&#xe9;-Peloux, Pauline, Champagnat, Fr&#xe9;d&#xe9;ric, Almansa, Andr&#xe9;s</br>
  Aerial or satellite imagery is a great source for land surface analysis, which might yield land use maps or elevation models. In this investigation, we present a neural network framework for learning semantics and local height together. We show how this joint multi-task learning benefits to each task on the large dataset of the 2018 Data Fusion Contest. Moreover, our framework also yields an uncertainty map which allows assessing the prediction of the model. Code is available at <font color="#006400">http</font>s://<font color="red">github</font>.com/marcelampc/mtl_aerial_images . </br></br>

<a href='http://arxiv.org/pdf/1911.08082.pdf'>1911.08082</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 1.0224баллов, №96</br>
<b>Low rank tensor completion with sparse regularization in a transformed\n  domain</b></br>
Authors: , Wang, Ping-Ping, Li, Liang, Cheng, Guang-Hui</br>
  Tensor completion is a challenging problem with various applications. Many related models based on the low-rank prior of the tensor have been proposed. However, the low-rank prior may not be enough to recover the original tensor from the observed incomplete tensor. In this paper, we prose a tensor completion method by exploiting both the low-rank and sparse prior of tensor. Specifically, the tensor completion task can be formulated as a low-rank minimization problem with a sparse regularizer. The low-rank property is depicted by the tensor truncated nuclear norm based on tensor singular value decomposition (T-SVD) which is a better approximation of tensor tubal rank than tensor nuclear norm. While the sparse regularizer is imposed by a $\\ell_{1}$-norm in a discrete cosine transformation (DCT) domain, which can better employ the local sparse property of completed data. To solve the optimization problem, we employ an alternating direction method of multipliers (ADMM) in which we only need to solve several subproblems which have closed-form solutions. Substantial experiments on <font color="#009600">real world</font> images and videos show that the proposed method has better performances than the existing <font color="red">state-of-the-art</font> methods. </br></br>

<a href='http://arxiv.org/pdf/1911.08870.pdf'>1911.08870</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp 1.0216баллов, №97</br>
<b>A Comparative Study on End-to-end Speech to Text Translation</b></br>
Authors: , Bahar, Parnia, Bieschke, Tobias, Ney, Hermann</br>
  Recent advances in deep learning show that end-to-end speech to text translation model is a promising approach to direct the speech translation field. In this work, we provide an overview of different end-to-end architectures, as well as the usage of an auxiliary connectionist temporal classification (CTC) loss for better convergence. We also investigate on pre-training variants such as initializing different components of a model using pre-trained models, and their impact on the final performance, which gives boosts up to 4% in BLEU and 5% in TER. Our experiments are performed on 270h IWSLT TED-talks En-&gt;De, and 100h LibriSpeech Audiobooks En-&gt;Fr. We also show improvements over the current end-to-end <font color="red">state-of-the-art</font> systems on both tasks. </br></br>

<a href='http://arxiv.org/pdf/1911.07257.pdf'>1911.07257</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 1.0176баллов, №98</br>
<b>Learning with Hierarchical Complement Objective</b></br>
Authors: , Chen, Hao-Yun, Tsai, Li-Huang, Chang, Shih-Chieh, Pan, Jia-Yu, Chen, Yu-Ting, Wei, Wei, Juan, Da-Cheng</br>
  Label hierarchies widely exist in many vision-related problems, ranging from explicit label hierarchies existed in image classification to latent label hierarchies existed in semantic <font color="#be00be">segmentation</font>. Nevertheless, <font color="red">state-of-the-art</font> methods often deploy cross-entropy loss that implicitly assumes class labels to be exclusive and thus independence from each other. Motivated by the fact that classes from the same parental category usually share certain similarity, we design a new training diagram called <font color="#00be00">Hierarchical</font> Complement Objective Training (HCOT) that leverages the information from label hierarchy. HCOT maximizes the probability of the ground truth class, and at the same time, neutralizes the probabilities of rest of the classes in a hierarchical fashion, making the model take advantage of the label hierarchy explicitly. The proposed HCOT is evaluated on both image classification and semantic segmentation tasks. Experimental results confirm that HCOT <font color="#00be00">outperform</font>s state-of-the-art models in CIFAR-100, ImageNet-2012, and PASCAL-Context. The study further demonstrates that HCOT can be applied on tasks with latent label hierarchies, which is a common characteristic in many machine learning tasks. </br></br>

<a href='http://arxiv.org/pdf/1911.08928.pdf'>1911.08928</a> &nbsp&nbsp (cs:RO, cs:CV, cs:ML) &nbsp&nbsp 1.0170баллов, №99</br>
<b>A Human Action Descriptor Based on Motion Coordination</b></br>
Authors: , Falco, Pietro, Saveriano, Matteo, Hasany, Eka Gibran, Kirk, Nicholas H., Lee, Dongheui</br>
  In this paper, we present a descriptor for human whole-body actions based on motion coordination. We exploit the principle, well known in neuromechanics, that humans move their joints in a coordinated fashion. Our coordination-based descriptor (CODE) is computed by two main steps. The first step is to identify the most informative joints which characterize the motion. The second step enriches the descriptor considering minimum and maximum joint velocities and the correlations between the most informative joints. In order to compute the distances between action descriptors, we propose a novel correlation-based similarity measure. The performance of CODE is tested on two public datasets, namely HDM05 and Berkeley MHAD, and compared with <font color="red">state-of-the-art</font> approaches, showing recognition results. </br></br>

<a href='http://arxiv.org/pdf/1911.08755.pdf'>1911.08755</a> &nbsp&nbsp (cs:CL, cs:AI) &nbsp&nbsp 1.0151баллов, №100</br>
<b>Global Thread-Level Inference for Comment Classification in Community\n  Question Answering</b></br>
Authors: , Joty, Shafiq, Barr&#xf3;n-Cede&#xf1;o, Alberto, Martino, Giovanni Da San, Filice, Simone, M&#xe0;rquez, Llu&#xed;s, Moschitti, Alessandro, Nakov, Preslav</br>
  Community question answering, a recent evolution of question answering in the Web context, allows a user to quickly consult the opinion of a number of people on a particular topic, thus taking advantage of the wisdom of the crowd. Here we try to help the user by deciding automatically which answers are good and which are bad for a given question. In particular, we focus on exploiting the output structure at the thread level in order to make more consistent global decisions. More specifically, we exploit the relations between pairs of comments at any distance in the thread, which we incorporate in a graph-cut and in an ILP frameworks. We evaluated our approach on the benchmark dataset of SemEval-2015 Task 3. Results improved over the <font color="red">state of the art</font>, confirming the importance of using thread level information. </br></br>

<a href='http://arxiv.org/pdf/1911.07062.pdf'>1911.07062</a> &nbsp&nbsp (cs:SD) &nbsp&nbsp 1.0092баллов, №101</br>
<b>N-HANS: Introducing the Augsburg Neuro-Holistic Audio-eNhancement System</b></br>
Authors: , Liu, Shuo, Keren, Gil, Schuller, Bj&#xf6;rn</br>
  N-HANS is a Python toolkit for in-the-wild audio enhancement, including speech, music, and general audio denoising, separation, and selective noise or source suppression. The functionalities are realised based on two neural network models sharing the same architecture, but trained separately. The models are comprised of stacks of residual blocks, each conditioned on additional speech or environmental noise recordings for adapting to different unseen speakers or environments in real life. In addition to a Python API, a command line interface is provided to researchers and developers, both of which are documented at <font color="#006400">http</font>s://<font color="red">github</font>.com/N-HANS/N-HANS. Experimental results indicate that N-HANS achieves outstanding performance, and ensure its reliable usage in real-life audio and speech-related tasks, reaching very high audio and speech quality. </br></br>

<a href='http://arxiv.org/pdf/1911.09333.pdf'>1911.09333</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 1.0065баллов, №102</br>
<b>Generating Diverse Translation by Manipulating Multi-Head Attention</b></br>
Authors: , Sun, Zewei, Huang, Shujian, Wei, Hao-Ran, Dai, Xin-yu, Chen, Jiajun</br>
  Transformer model has been widely used on machine translation tasks and obtained <font color="red">state-of-the-art</font> results. In this paper, we report an interesting phenomenon in its encoder-decoder multi-head attention: different attention heads of the final decoder layer align to different word translation candidates. We empirically verify this discovery and propose a method to generate diverse translations by manipulating heads. Furthermore, we make use of these diverse translations with the back-translation technique for better data augmentation. Experiment results show that our method generates diverse translations without severe drop in translation quality. Experiments also show that back-translation with these diverse translations could bring significant improvement on performance on translation tasks. An auxiliary experiment of conversation response generation task proves the effect of diversity as well. </br></br>

<a href='http://arxiv.org/pdf/1911.07190.pdf'>1911.07190</a> &nbsp&nbsp (cs:ML, cs:CV) &nbsp&nbsp 0.9937баллов, №103</br>
<b>Loss Aware Post-training Quantization</b></br>
Authors: , Nahshan, Yury, Chmiel, Brian, Baskin, Chaim, Zheltonozhskii, Evgenii, Banner, Ron, Bronstein, Alex M., Mendelson, Avi</br>
  Neural network quantization enables the deployment of large models on resource-constrained devices. Current post-training quantization methods fall short in terms of accuracy for INT4 (or lower) but provide reasonable accuracy for INT8 (or above). In this work, we study the effect of quantization on the structure of the loss landscape. We show that the structure is flat and separable for mild quantization, enabling straightforward post-training quantization methods to achieve good results. On the other hand, we show that with more aggressive quantization, the loss landscape becomes highly non-separable with sharp minima points, making the selection of quantization parameters more challenging. Armed with this understanding, we design a method that quantizes the layer parameters jointly, enabling significant accuracy improvement over current post-training quantization methods. Reference implementation accompanies the paper at <font color="#006400">http</font>s://<font color="red">github</font>.com/ynahshan/nn-quantization-pytorch/tree/master/lapq </br></br>

<a href='http://arxiv.org/pdf/1911.07523.pdf'>1911.07523</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp 0.9912баллов, №104</br>
<b>Fine-Grained Static Detection of Obfuscation Transforms Using\n  Ensemble-Learning and Semantic Reasoning</b></br>
Authors: , Tofighi-Shirazi, Ramtine, Asavoae, Irina Mariuca, Elbaz-Vincent, Philippe</br>
  The ability to efficiently detect the software protections used is at a prime to facilitate the selection and application of adequate deob-fuscation techniques. We present a novel approach that combines semantic reasoning techniques with ensemble learning classification for the purpose of providing a static detection framework for obfuscation transformations. By contrast to existing work, we provide a methodology that can detect multiple layers of obfuscation, without depending on knowledge of the underlying functionality of the training-set used. We also extend our work to detect constructions of obfuscation transformations, thus providing a fine-grained methodology. To that end, we provide several studies for the best practices of the use of machine learning techniques for a scalable and efficient model. According to our experimental results and evaluations on obfuscators such as Tigress and OLLVM, our models have up to 91% accuracy on <font color="red">state-of-the-art</font> obfuscation transformations. Our overall accuracies for their constructions are up to 100%. </br></br>

<a href='http://arxiv.org/pdf/1911.09251.pdf'>1911.09251</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.9911баллов, №105</br>
<b>AutoShrink: A Topology-aware NAS for Discovering Efficient Neural\n  Architecture</b></br>
Authors: , Zhang, Tunhou, Cheng, Hsin-Pai, Li, Zhenwen, Yan, Feng, Huang, Chengyu, Li, Hai, Chen, Yiran</br>
  Resource is an important constraint when deploying Deep Neural Networks (DNNs) on<font color="#960096"> mobile </font>and edge devices. Existing works commonly adopt the cell-based search approach, which limits the flexibility of network patterns in learned cell structures. Moreover, due to the topology-agnostic nature of existing works, including both cell-based and node-based approaches, the search process is time consuming and the performance of found architecture may be sub-optimal. To address these problems, we propose AutoShrink, a topology-aware Neural <font color="#00be00">Architecture Search</font>(NAS) for searching efficient building blocks of neural architectures. Our method is node-based and thus can learn flexible network patterns in cell structures within a topological search space. Directed Acyclic Graphs (DAGs) are used to abstract DNN architectures and progressively optimize the cell structure through edge shrinking. As the search space intrinsically reduces as the edges are progressively shrunk, AutoShrink explores more flexible search space with even less search time. We evaluate AutoShrink on image classification and language tasks by crafting ShrinkCNN and ShrinkRNN models. ShrinkCNN is able to achieve up to 48% parameter reduction and save 34% Multiply-Accumulates (MACs) on ImageNet-1K with comparable accuracy of <font color="red">state-of-the-art</font> (SOTA) models. Specifically, both ShrinkCNN and ShrinkRNN are crafted within 1.5 GPU hours, which is 7.2x and 6.7x faster than the crafting time of SOTA CNN and RNN models, respectively. </br></br>

<a href='http://arxiv.org/pdf/1911.08935.pdf'>1911.08935</a> &nbsp&nbsp (cs:CL, cs:AI, cs:ML) &nbsp&nbsp 0.9862баллов, №106</br>
<b>Rule-Guided Compositional Representation Learning on Knowledge Graphs</b></br>
Authors: , Niu, Guanglin, Zhang, Yongfei, Li, Bo, Cui, Peng, Liu, Si, Li, Jingyang, Zhang, Xiaowei</br>
  Representation learning on a <font color="#960096">knowledge graph</font> (KG) is to embed entities and relations of a KG into low-dimensional continuous vector spaces. Early KG embedding methods only pay attention to structured information encoded in triples, which would cause limited performance due to the structure sparseness of KGs. Some recent attempts consider paths information to expand the structure of KGs but lack explainability in the process of obtaining the path representations. In this paper, we propose a novel Rule and Path-based Joint Embedding (RPJE) scheme, which takes full advantage of the explainability and accuracy of logic rules, the generalization of KG embedding as well as the supplementary semantic structure of paths. Specifically, logic rules of different lengths (the number of relations in rule body) in the form of Horn clauses are first mined from the KG and elaborately encoded for representation learning. Then, the rules of length 2 are applied to compose paths accurately while the rules of length 1 are explicitly employed to create semantic associations among relations and constrain relation embeddings. Besides, the confidence level of each rule is also considered in optimization to guarantee the availability of applying the rule to representation learning. Extensive experimental results illustrate that RPJE <font color="#00be00">outperform</font>s other <font color="red">state-of-the-art</font> baselines on KG completion task, which also demonstrate the superiority of utilizing logic rules as well as paths for improving the accuracy and explainability of representation learning. </br></br>

<a href='http://arxiv.org/pdf/1911.06965.pdf'>1911.06965</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.9851баллов, №107</br>
<b>An &quot;outside the box&quot; solution for imbalanced data classification</b></br>
Authors: , Jegierski, Hubert, Saganowski, Stanis&#x142;aw</br>
  A common problem of the <font color="#009600">real-world</font> data sets is the class imbalance, which can significantly affect the classification abilities of classifiers. Numerous methods have been proposed to cope with this problem; however, even <font color="red">state-of-the-art</font> methods offer a limited improvement (if any) for data sets with critically under-represented minority classes. For such problematic cases, an &quot;outside the box&quot; solution is required. Therefore, we propose a novel technique, called enrichment, which uses the information (observations) from the external data set(s). We present three approaches to implement enrichment technique: (1) selecting observations randomly, (2) iteratively choosing observations that improve the classification result, (3) adding observations that help the classifier to determine the border between classes better. We then thoroughly analyze developed solutions on ten real-world data sets to experimentally validate their usefulness. On average, our best approach improves the classification quality by 27\\%, and in the best case, by outstanding 66\\%. We also compare our technique with the universally applicable state-of-the-art methods. We find that our technique surpasses the existing methods performing, on average, 21\\% better. The advantage is especially noticeable for the smallest data sets, for which existing methods failed, while our solutions achieved the best results. Additionally, our technique applies to both the multi-class and binary classification tasks. It can also be combined with other techniques dealing with the class imbalance problem. </br></br>

<a href='http://arxiv.org/pdf/1911.09218.pdf'>1911.09218</a> &nbsp&nbsp (cs:ML, cs:AI) &nbsp&nbsp 0.9836баллов, №108</br>
<b>Discovering Subdimensional Motifs of Different Lengths in Large-Scale\n  Multivariate Time Series</b></br>
Authors: , Gao, Yifeng, Lin, Jessica</br>
  Detecting repeating patterns of different lengths in time series, also called variable-length motifs, has received a great amount of attention by researchers and practitioners. Despite the significant progress that has been made in recent single dimensional variable-length motif discovery work, detecting variable-length \\textit{subdimensional motifs}---patterns that are simultaneously occurring only in a subset of dimensions in multivariate time series---remains a difficult task. The main challenge is scalability. On the one hand, the brute-force enumeration solution, which searches for motifs of all possible lengths, is very time consuming even in single dimensional time series. On the other hand, previous work show that index-based fixed-length approximate motif discovery algorithms such as random projection are not suitable for detecting variable-length motifs due to memory requirement. In this paper, we introduce an approximate variable-length subdimensional motif discovery algorithm called \\textbf{C}ollaborative \\textbf{HI}erarchy based \\textbf{M}otif \\textbf{E}numeration (CHIME) to efficiently detect variable-length subdimensional motifs given a minimum motif length in large-scale multivariate time series. We show that the memory cost of the approach is significantly smaller than that of random projection. Moreover, the speed of the proposed algorithm is significantly faster than that of the <font color="red">state-of-the-art</font> algorithms. We demonstrate that CHIME can efficiently detect meaningful variable-length subdimensional motifs in large <font color="#009600">real world</font> multivariate time series datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.07176.pdf'>1911.07176</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 0.9805баллов, №109</br>
<b>Quick and (not so) Dirty: Unsupervised Selection of Justification\n  Sentences for Multi-hop Question Answering</b></br>
Authors: , Yadav, Vikas, Bethard, Steven, Surdeanu, Mihai</br>
  We propose an unsupervised strategy for the selection of justification sentences for multi-hop question answering (QA) that (a) maximizes the relevance of the selected sentences, (b) minimizes the overlap between the selected facts, and (c) maximizes the coverage of both question and answer. This unsupervised sentence selection method can be coupled with any supervised QA approach. We show that the sentences selected by our method improve the performance of a <font color="red">state-of-the-art</font> supervised QA model on two multi-hop QA datasets: AI2\'s Reasoning Challenge (ARC) and Multi-Sentence Reading Comprehension (MultiRC). We obtain new state-of-the-art performance on both datasets among approaches that do not use external resources for training the QA system: 56.82\\% F1 on ARC (41.24\\% on Challenge and 64.49\\% on Easy) and 26.1\\% EM0 on MultiRC. Our justification sentences have higher quality than the justifications selected by a strong information retrieval baseline, e.g., by 5.4\\% F1 in MultiRC. We also show that our unsupervised selection of justification sentences is more stable across domains than a state-of-the-art supervised sentence selection method. </br></br>

<a href='http://arxiv.org/pdf/1911.07757.pdf'>1911.07757</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.9741баллов, №110</br>
<b>Satellite Image Time Series Classification with Pixel-Set Encoders and\n  Temporal Self-Attention</b></br>
Authors: , Garnot, Vivien Sainte Fare, Landrieu, Loic, Giordano, Sebastien, Chehata, Nesrine</br>
  Satellite image time series, bolstered by their growing availability, are at the forefront of an extensive effort towards automated Earth monitoring by international institutions. In particular, large-scale control of agricultural parcels is an issue of major political and <font color="#be00be">economic</font> importance. In this regard, hybrid convolutional-recurrent neural architectures have shown promising results for the automated classification of satellite image time series.We propose an alternative approach in which the convolutional layers are advantageously replaced with encoders operating on unordered sets of pixels to exploit the typically coarse resolution of <font color="#00be00">publicly available</font> satellite images. We also propose to extract temporal features using a bespoke neural architecture based on self-attention instead of recurrent networks. We demonstrate experimentally that our method not only <font color="#00be00">outperform</font>s previous <font color="red">state-of-the-art</font> approaches in terms of precision, but also significantly decreases processing time and memory requirements. Lastly, we release a large open-access annotated dataset as a benchmark for future work on satellite image time series. </br></br>

<a href='http://arxiv.org/pdf/1911.08522.pdf'>1911.08522</a> &nbsp&nbsp (cs:CL, cs:AI, cs:ML) &nbsp&nbsp 0.9707баллов, №111</br>
<b>Aging Memories Generate More Fluent Dialogue Responses with Memory\n  Networks</b></br>
Authors: , Florez, Omar U., Mueller, Erik</br>
  The integration of a Knowledge Base (KB) into a neural dialogue agent is one of the key challenges in Conversational AI. Memory networks has proven to be effective to encode KB information into an external memory to thus generate more fluent and informed responses. Unfortunately, such memory becomes full of latent representations during training, so the most common strategy is to overwrite old memory entries randomly.   In this paper, we question this approach and provide experimental evidence showing that conventional memory networks generate many redundant latent vectors resulting in overfitting and the need for larger memories.   We introduce memory dropout as an automatic technique that encourages diversity in the latent space by 1) Aging redundant memories to increase their probability of being overwritten during training 2) Sampling new memories that summarize the knowledge acquired by redundant memories. This technique allows us to incorporate Knowledge Bases to achieve <font color="red">state-of-the-art</font> dialogue generation in the Stanford Multi-Turn Dialogue dataset. Considering the same architecture, its use provides an improvement of +2.2 BLEU points for the automatic generation of responses and an increase of +8.1% in the recognition of named entities. </br></br>

<a href='http://arxiv.org/pdf/1911.09375.pdf'>1911.09375</a> &nbsp&nbsp (cs:CV, cs:CL, cs:ML) &nbsp&nbsp 0.9684баллов, №112</br>
<b>ChartNet: Visual Reasoning over Statistical Charts using MAC-Networks</b></br>
Authors: , Sharma, Monika, Gupta, Shikha, Chowdhury, Arindam, Vig, Lovekesh</br>
  Despite the improvements in perception accuracies brought about via deep learning, developing systems combining accurate visual perception with the ability to reason over the visual percepts remains extremely challenging. A particular application area of interest from an accessibility perspective is that of reasoning over statistical charts such as bar and pie charts. To this end, we formulate the problem of reasoning over statistical charts as a classification task using MAC-Networks to give answers from a predefined vocabulary of generic answers. Additionally, we enhance the capabilities of MAC-Networks to give chart-specific answers to open-ended questions by replacing the classification layer by a <font color="#be00be">regression</font> layer to localize the textual answers present over the images. We call our network ChartNet, and demonstrate its efficacy on predicting both in vocabulary and out of vocabulary answers. To test our methods, we generated our own dataset of statistical chart images and corresponding question answer pairs. Results show that ChartNet consistently <font color="#00be00">outperform</font> other <font color="red">state-of-the-art</font> methods on reasoning over these questions and may be a viable candidate for applications containing images of statistical charts. </br></br>

<a href='http://arxiv.org/pdf/1911.09532.pdf'>1911.09532</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 0.9545баллов, №113</br>
<b>A Cluster Ranking Model for Full Anaphora Resolution</b></br>
Authors: , Yu, Juntao, Uma, Alexandra, Poesio, Massimo</br>
  Anaphora resolution (coreference) systems designed for the CONLL 2012 dataset typically cannot handle key aspects of the full anaphora resolution task such as the identification of singletons and of certain types of non-referring expressions (e.g., expletives), as these aspects are not annotated in that corpus. However, the recently released dataset for the CRAC 2018 Shared Task can now be used for that purpose. In this paper, we introduce an architecture to simultaneously identify non-referring expressions (including expletives, predicative {\\NP}s, and other types) and build coreference chains, including singletons. Our cluster-ranking system uses an attention mechanism to determine the relative importance of the mentions in the same cluster. Additional classifiers are used to identify singletons and non-referring markables. Our contributions are as follows. First all, we report the first result on the CRAC data using system mentions; our result is 5.8% better than the shared task baseline system, which used gold mentions. Second, we demonstrate that the availability of singleton clusters and non-referring expressions can lead to substantially improved performance on non-singleton clusters as well. Third, we show that despite our model not being designed specifically for the CONLL data, it achieves a score equivalent to that of the <font color="red">state-of-the-art</font> system by Kantor and Globerson (2019) on that dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.08616.pdf'>1911.08616</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.9543баллов, №114</br>
<b>Attention Guided Anomaly Detection and Localization in Images</b></br>
Authors: , Venkataramanan, Shashanka, Peng, Kuan-Chuan, Singh, Rajat Vikram, Mahalanobis, Abhijit</br>
  <font color="#be00be">Anomal</font>y detection and localization is a popular computer vision problem involving detecting anomalous images and localizing anomalies within them. However, this task is challenging due to the small sample size and pixel coverage of the anomaly in <font color="#009600">real-world</font> scenarios. Prior works need to use anomalous training images to compute a threshold to detect and localize anomalies. To remove this need, we propose Convolutional Adversarial Variational autoencoder with Guided Attention (CAVGA), which localizes the anomaly with a convolutional latent variable to preserve the spatial information. In the unsupervised setting, we propose an attention expansion loss, where we encourage CAVGA to focus on all normal regions in the image without using any anomalous training image. Furthermore, using only 2% anomalous images in the weakly supervised setting we propose a complementary guided attention loss, where we encourage the normal attention to focus on all normal regions while minimizing the regions covered by the anomalous attention in the normal image. CAVGA <font color="#00be00">outperform</font>s the <font color="red">state-of-the-art</font> (SOTA) anomaly detection methods on the MNIST, CIFAR-10, Fashion-MNIST, MVTec Anomaly Detection (MVTAD), and modified ShanghaiTech Campus (mSTC) datasets. CAVGA also outperforms the SOTA anomaly localization methods on the MVTAD and mSTC datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.07056.pdf'>1911.07056</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp 0.9528баллов, №115</br>
<b>AttaCut: A Fast and Accurate Neural Thai Word Segmenter</b></br>
Authors: , Chormai, Pattarawat, Prasertsom, Ponrawee, Rutherford, Attapol</br>
  Word <font color="#be00be">segmentation</font> is a fundamental pre-processing step for Thai Natural Language Processing. The current off-the-shelf solutions are not benchmarked consistently, so it is difficult to compare their trade-offs. We conducted a speed and accuracy comparison of the popular systems on three different domains and found that the <font color="red">state-of-the-art</font> deep learning system is slow and moreover does not use sub-word structures to guide the model. Here, we propose a fast and accurate neural Thai Word Segmenter that uses dilated CNN filters to capture the environment of each character and uses syllable embeddings as features. Our system runs at least 5.6x faster and <font color="#00be00">outperform</font>s the previous state-of-the-art system on some domains. In addition, we develop the first ML-based Thai orthographical syllable segmenter, which yields syllable embeddings to be used as features by the word segmenter. </br></br>

<a href='http://arxiv.org/pdf/1911.02875.pdf'>1911.02875</a> &nbsp&nbsp (cs:ML, cs:RO) &nbsp&nbsp 0.9507баллов, №116</br>
<b>$H_\\infty$ Model-free Reinforcement Learning with Robust Stability\n  Guarantee</b></br>
Authors: , Han, Minghao, Tian, Yuan, Zhang, Lixian, Wang, Jun, Pan, Wei</br>
  <font color="#00be00">Reinforcement learning</font> is showing great potentials in robotics applications, including autonomous driving, robot manipulation and locomotion. However, with complex uncertainties in the <font color="#009600">real-world</font> environment, it is difficult to guarantee the successful generalization and sim-to-real transfer of learned policies <font color="blue">theor</font>etically. In this paper, we introduce and extend the idea of robust stability and $H_\\infty$ control to design policies with both stability and robustness guarantee. Specifically, a sample-based approach for analyzing the Lyapunov stability and performance robustness of a learning-based control system is proposed. Based on the theoretical results, a maximum entropy algorithm is developed for searching Lyapunov function and designing a policy with provable robust stability guarantee. Without any specific domain knowledge, our method can find a policy that is robust to various uncertainties and generalizes well to different test environments. In our experiments, we show that our method achieves better robustness to both large impulsive disturbances and parametric variations in the environment than the state-of-art results in both robust and generic RL, as well as classic control. Anonymous code is available to reproduce the experimental results at <font color="#006400">http</font>s://<font color="red">github</font>.com/RobustStabilityGuaranteeRL/RobustStabilityGuaranteeRL. </br></br>

<a href='http://arxiv.org/pdf/1911.08020.pdf'>1911.08020</a> &nbsp&nbsp (cs:ML, cs:NE) &nbsp&nbsp 0.9506баллов, №117</br>
<b>DARB: A Density-Aware Regular-Block Pruning for Deep Neural Networks</b></br>
Authors: , Ren, Ao, Zhang, Tao, Wang, Yuhao, Lin, Sheng, Dong, Peiyan, Chen, Yen-kuang, Xie, Yuan, Wang, Yanzhi</br>
  The rapidly growing parameter volume of deep neural networks (DNNs) hinders the artificial intelligence applications on resource constrained devices, such as<font color="#960096"> mobile </font>and wearable devices. Neural network pruning, as one of the mainstream model compression techniques, is under extensive study to reduce the number of parameters and computations. In contrast to irregular pruning that incurs high index storage and decoding overhead, structured pruning techniques have been proposed as the promising solutions. However, prior studies on structured pruning tackle the problem mainly from the perspective of facilitating hardware implementation, without analyzing the characteristics of sparse neural networks. The neglect on the study of sparse neural networks causes inefficient trade-off between regularity and pruning ratio. Consequently, the potential of structurally pruning neural networks is not sufficiently mined.   In this work, we examine the structural characteristics of the irregularly pruned weight matrices, such as the diverse redundancy of different rows, the sensitivity of different rows to pruning, and the positional characteristics of retained weights. By leveraging the gained insights as a guidance, we first propose the novel block-max weight masking (BMWM) method, which can effectively retain the salient weights while imposing high regularity to the weight matrix. As a further optimization, we propose a density-adaptive regular-block (DARB) pruning that <font color="#00be00">outperform</font>s prior structured pruning work with high pruning ratio and decoding efficiency. Our experimental results show that DARB can achieve 13$\\times$ to 25$\\times$ pruning ratio, which are 2.8$\\times$ to 4.3$\\times$ improvements than the <font color="red">state-of-the-art</font> counterparts on multiple neural network models and tasks. Moreover, DARB can achieve 14.3$\\times$ decoding efficiency than block pruning with higher pruning ratio. </br></br>

<a href='http://arxiv.org/pdf/1911.07067.pdf'>1911.07067</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.9502баллов, №118</br>
<b>ResUNet++: An Advanced Architecture for Medical Image Segmentation</b></br>
Authors: , Jha, Debesh, Smedsrud, Pia H., Riegler, Michael A., Johansen, Dag, de Lange, Thomas, Halvorsen, Pal, Johansen, Havard D.</br>
  Accurate computer-aided polyp detection and <font color="#be00be">segmentation</font> during colonoscopy examinations can help endoscopists resect abnormal tissue and thereby decrease chances of polyps growing into <font color="#be00be">cancer</font>. Towards developing a fully automated model for pixel-wise polyp segmentation, we propose ResUNet++, which is an improved ResUNet architecture for colonoscopic image segmentation. Our experimental evaluations show that the suggested architecture produces good segmentation results on <font color="#00be00">publicly available</font> datasets. Furthermore, ResUNet++ significantly <font color="#00be00">outperform</font>s U-Net and ResUNet, two key <font color="red">state-of-the-art</font> deep learning architectures, by achieving high evaluation scores with a dice coefficient of 81.33%, and a mean Intersection over Union (mIoU) of 79.27% for the Kvasir-SEG dataset and a dice coefficient of 79.55%, and a mIoU of 79.62% with CVC-612 dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.04060.pdf'>1911.04060</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.9429баллов, №119</br>
<b>Invariant Representations through Adversarial Forgetting</b></br>
Authors: , Jaiswal, Ayush, Moyer, Daniel, Steeg, Greg Ver, AbdAlmageed, Wael, Natarajan, Premkumar</br>
  We propose a novel approach to achieving invariance for deep neural networks in the form of inducing amnesia to unwanted factors of data through a new adversarial forgetting mechanism. We show that the forgetting mechanism serves as an information-bottleneck, which is manipulated by the adversarial training to learn invariance to unwanted factors. Empirical results show that the proposed framework achieves <font color="red">state-of-the-art</font> performance at learning invariance in both nuisance and bias settings on a diverse collection of datasets and tasks. </br></br>

<a href='http://arxiv.org/pdf/1911.09326.pdf'>1911.09326</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.9312баллов, №120</br>
<b>LCD: Learned Cross-Domain Descriptors for 2D-3D Matching</b></br>
Authors: , Pham, Quang-Hieu, Uy, Mikaela Angelina, Hua, Binh-Son, Nguyen, Duc Thanh, Roig, Gemma, Yeung, Sai-Kit</br>
  In this work, we present a novel method to learn a local cross-domain descriptor for 2D image and 3D <font color="#be00be">point cloud</font> matching. Our proposed method is a dual auto-encoder neural network that maps 2D and 3D input into a shared latent space representation. We show that such local cross-domain descriptors in the shared embedding are more discriminative than those obtained from individual training in 2D and 3D domains. To facilitate the training process, we built a new dataset by collecting $\\approx 1.4$ millions of 2D-3D correspondences with various lighting conditions and settings from <font color="#00be00">publicly available</font> RGB-D scenes. Our descriptor is evaluated in three main experiments: 2D-3D matching, cross-domain retrieval, and sparse-to-dense depth estimation. Experimental results confirm the robustness of our approach as well as its <font color="#960096">competitive</font> performance not only in solving cross-domain tasks but also in being able to generalize to solve sole 2D and 3D tasks. Our dataset and code are released publicly at \\url{<font color="#006400">http</font>s://hkust-vgd.<font color="red">github</font>.io/lcd}. </br></br>

<a href='http://arxiv.org/pdf/1911.08962.pdf'>1911.08962</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 0.9258баллов, №121</br>
<b>CAIL2019-SCM: A Dataset of Similar Case Matching in Legal Domain</b></br>
Authors: , Xiao, Chaojun, Zhong, Haoxi, Guo, Zhipeng, Tu, Cunchao, Liu, Zhiyuan, Sun, Maosong, Zhang, Tianyang, Han, Xianpei, hu, Zhen, Wang, Heng, Xu, Jianfeng</br>
  In this paper, we introduce CAIL2019-SCM, <font color="#be00be">Chinese</font> AI and Law 2019 Similar Case Matching dataset. CAIL2019-SCM contains 8,964 triplets of cases published by the Supreme People\'s Court of China. CAIL2019-SCM focuses on detecting similar cases, and the participants are required to check which two cases are more similar in the triplets. There are 711 teams who participated in this year\'s competition, and the best team has reached a score of 71.88. We have also implemented several baselines to help researchers better understand this task. The dataset and more details can be found from <font color="#006400">http</font>s://<font color="red">github</font>.com/china-ai-law-challenge/CAIL2019/tree/master/scm. </br></br>

<a href='http://arxiv.org/pdf/1911.07163.pdf'>1911.07163</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.9227баллов, №122</br>
<b>Dense Color Constancy with Effective Edge Augmentation</b></br>
Authors: , Zhang, Yilang, Wei, Zheng, Wang, Jian, Yuan, Xin</br>
  Recently, computational color constancy via convolutional neural networks (CNNs) has received much attention. In this paper, we propose a color constancy algorithm called the Dense Color Constancy (DCC), which employs a self-attention DenseNet to estimate the illuminant based on the $2$D $\\log$-chrominance histograms of input images and their augmented edges. The augmented edges help to tell apart the edge and non-edge pixels in the $\\log$-histogram, which largely contribute to the feature extraction and color ambiguity elimination, thereby improving the accuracy of illuminant estimation. Experiments on benchmark datasets show that the DCC algorithm is very effective for illuminant estimation compared to the <font color="red">state-of-the-art</font> methods. </br></br>

<a href='http://arxiv.org/pdf/1911.08252.pdf'>1911.08252</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.9225баллов, №123</br>
<b>Inter-layer Collision Networks</b></br>
Authors: , An, Junyi, Liu, Fengshan, Shen, Furao, Zhao, Jian</br>
  Deeper neural networks are hard to train. Inspired by the elastic collision model in physics, we present a universal structure that could be integrated into the existing network structures to speed up the training process and eventually increase its generalization ability. We apply our structure to the Convolutional Neural Networks(CNNs) to form a new structure, which we term the &quot;Inter-layer Collision&quot; (IC) structure. The IC structure provides the deeper layer a better representation of the input features. We evaluate the IC structure on CIFAR10 and Imagenet by integrating it into the existing <font color="red">state-of-the-art</font> CNNs. Our experiment shows that the proposed IC structure can effectively increase the accuracy and convergence speed. </br></br>

<a href='http://arxiv.org/pdf/1911.07245.pdf'>1911.07245</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.9211баллов, №124</br>
<b>Encouraging an Appropriate Representation Simplifies Training of Neural\n  Networks</b></br>
Authors: , Buza, Krisztian</br>
  A common assumption about neural networks is that they can learn an appropriate internal representations on their own, see e.g. end-to-end learning. In this work we challenge this assumption. We consider two simple tasks and show that the <font color="red">state-of-the-art</font> training algorithm fails, although the model itself is able to represent an appropriate solution. We will demonstrate that encouraging an appropriate internal representation allows the same model to solve these tasks. While we do not claim that it is impossible to solve these tasks by other means (such as neural networks with more layers), our results illustrate that integration of domain knowledge in form of a desired internal representation may improve the generalization ability of neural networks. </br></br>

<a href='http://arxiv.org/pdf/1911.07471.pdf'>1911.07471</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.9062баллов, №125</br>
<b>Preparing Lessons: Improve Knowledge Distillation with Better\n  Supervision</b></br>
Authors: , Wen, Tiancheng, Lai, Shenqi, Qian, Xueming</br>
  Knowledge distillation (KD) is widely used for training a compact model with the supervision of another large model, which could effectively improve the performance. Previous methods mainly focus on two aspects: 1) training the student to mimic representation space of the teacher; 2) training the model progressively or adding extra module like discriminator. Knowledge from teacher is useful, but it is still not exactly right compared with ground truth. Besides, overly uncertain supervision also influences the result. We introduce two novel approaches, Knowledge Adjustment (KA) and Dynamic Temperature Distillation (DTD), to penalize bad supervision and improve student model. Experiments on CIFAR-100, CINIC-10 and Tiny ImageNet show that our methods get encouraging performance compared with <font color="red">state-of-the-art</font> methods. When combined with other KD-based methods, the performance will be further improved. </br></br>

<a href='http://arxiv.org/pdf/1911.09058.pdf'>1911.09058</a> &nbsp&nbsp (cs:CV, cs:ML, stat:ML) &nbsp&nbsp 0.9053баллов, №126</br>
<b>Fine-grained Synthesis of Unrestricted Adversarial Examples</b></br>
Authors: , Poursaeed, Omid, Jiang, Tianxing, Yang, Harry, Belongie, Serge, Lim, Ser-Nam</br>
  We propose a novel approach for generating unrestricted adversarial examples by manipulating fine-grained aspects of image generation. Unlike existing unrestricted attacks that typically hand-craft geometric transformations, we learn stylistic and stochastic modifications leveraging <font color="red">state-of-the-art</font> generative models. This allows us to manipulate an image in a controlled, fine-grained manner without being bounded by a norm threshold. Our model can be used for both targeted and non-targeted unrestricted attacks. We demonstrate that our attacks can bypass certified defenses, yet our adversarial images look indistinguishable from natural images as verified by human evaluation. Adversarial training can be used as an effective defense without degrading performance of the model on clean images. We perform experiments on LSUN and CelebA-HQ as high resolution datasets to validate efficacy of our proposed approach. </br></br>

<a href='http://arxiv.org/pdf/1910.01764.pdf'>1910.01764</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.9051баллов, №127</br>
<b>Two Stream Networks for Self-Supervised Ego-Motion Estimation</b></br>
Authors: , Ambrus, Rares, Guizilini, Vitor, Li, Jie, Pillai, Sudeep, Gaidon, Adrien</br>
  Learning depth and camera ego-motion from raw unlabeled RGB video streams is seeing exciting progress through self-supervision from strong geometric cues. To leverage not only appearance but also scene geometry, we propose a novel self-supervised two-stream network using RGB and inferred depth information for accurate visual odometry. In addition, we introduce a sparsity-inducing data augmentation policy for ego-motion learning that effectively regularizes the pose network to enable stronger generalization performance. As a result, we show that our proposed two-stream pose network achieves <font color="red">state-of-the-art</font> results among learning-based methods on the KITTI odometry benchmark, and is especially suited for self-supervision at scale. Our experiments on a large-scale urban driving dataset of 1 million frames indicate that the performance of our proposed architecture does indeed scale progressively with more data. </br></br>

<a href='http://arxiv.org/pdf/1911.06080.pdf'>1911.06080</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.9039баллов, №128</br>
<b>CMSN: Continuous Multi-stage Network and Variable Margin Cosine Loss for\n  Temporal Action Proposal Generation</b></br>
Authors: , Hu, Yushuai, Jin, Yaochu, Li, Runhua, Zhang, Xiangxiang</br>
  Accurately locating the start and end time of an action in untrimmed videos is a challenging task. One of the important reasons is the boundary of action is not highly distinguishable, and the features around the boundary are difficult to discriminate. To address this problem, we propose a novel framework for temporal action proposal generation, namely Continuous Multi-stage Network (CMSN), which divides a video that contains a complete action instance into six stages, namely Backgroud, Ready, Start, Confirm, End, Follow. To distinguish between Ready and Start, End and Follow more accurately, we propose a novel loss function, Variable Margin Cosine Loss (VMCL), which allows for different margins between different categories. Our experiments on THUMOS14 show that the proposed method for temporal proposal generation performs better than the <font color="red">state-of-the-art</font> methods using the same network architecture and training dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.07036.pdf'>1911.07036</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.9038баллов, №129</br>
<b>Quality Assessment of DIBR-synthesized views: An Overview</b></br>
Authors: , Tian, Shishun, Zhang, Lu, Zou, Wenbin, Li, Xia, Su, Ting, Morin, Luce, Deforges, Olivier</br>
  The Depth-Image-Based-Rendering (DIBR) is one of the main fundamental technique to generate new views in 3D video applications, such as Multi-View Videos (MVV), Free-Viewpoint Videos (FVV) and Virtual Reality (VR). However, the quality assessment of DIBR-synthesized views is quite different from the traditional 2D images/videos. In recent years, several efforts have been made towards this topic, but there lacks a detailed survey in literature. In this paper, we provide a comprehensive survey on various current approaches for DIBR-synthesized views. The current accessible datasets of DIBR-synthesized views are firstly reviewed. Followed by a summary and analysis of the representative <font color="red">state-of-the-art</font> objective metrics. Then, the performances of different objective metrics are evaluated and discussed on all available datasets. Finally, we discuss the potential challenges and suggest possible directions for future research. </br></br>

<a href='http://arxiv.org/pdf/1911.07255.pdf'>1911.07255</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp 0.9028баллов, №130</br>
<b>Deep geometric matrix completion: Are we doing it right?</b></br>
Authors: , Boyarski, Amit, Vedula, Sanketh, Bronstein, Alex</br>
  We address the problem of reconstructing a matrix from a subset of its entries. Current methods, branded as geometric matrix completion, augment classical rank regularization techniques by incorporating geometric information into the solution. This information is usually provided as graphs encoding relations between rows/columns. In this work we propose a simple spectral approach for solving the matrix completion problem, via the framework of functional maps. We introduce the zoomout loss, a multiresolution spectral geometric loss inspired by recent advances in shape correspondence, whose minimization leads to <font color="red">state-of-the-art</font> results on various recommender systems datasets. Surprisingly, for some datasets we were able to achieve comparable results even without incorporating geometric information. This puts into question both the quality of such information and current methods\' ability to use it in a meaningful and efficient way. </br></br>

<a href='http://arxiv.org/pdf/1911.07147.pdf'>1911.07147</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp 0.9010баллов, №131</br>
<b>Causality-based Feature Selection: Methods and Evaluations</b></br>
Authors: , Yu, Kui, Guo, Xianjie, Liu, Lin, Li, Jiuyong, Wang, Hao, Ling, Zhaolong, Wu, Xindong</br>
  Feature selection is a crucial preprocessing step in data analytics and machine learning. Classical feature selection algorithms select features based on the correlations between predictive features and the class variable and do not attempt to capture causal relationships between them. It has been shown that the knowledge about the causal relationships between features and the class variable has potential benefits for building <font color="#be00be">interpret</font>able and robust prediction models, since causal relationships imply the underlying mechanism of a system. Consequently, causality-based feature selection has gradually attracted greater attentions and many algorithms have been proposed. In this paper, we present a comprehensive review of recent advances in causality-based feature selection. To facilitate the development of new algorithms in the research area and make it easy for the comparisons between new methods and existing ones, we develop the first open-source package, called CausalFS, which consists of most of the representative causality-based feature selection algorithms (available at <font color="#006400">http</font>s://<font color="red">github</font>.com/kuiy/CausalFS). Using CausalFS, we conduct extensive experiments to compare the representative algorithms with both synthetic and <font color="#009600">real-world</font> data sets. Finally, we discuss some challenging problems to be tackled in future causality-based feature selection research. </br></br>

<a href='http://arxiv.org/pdf/1911.08941.pdf'>1911.08941</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.9002баллов, №132</br>
<b>Fast and Deep Graph Neural Networks</b></br>
Authors: , Gallicchio, Claudio, Micheli, Alessio</br>
  We address the efficiency issue for the construction of a deep graph neural network (GNN). The approach exploits the idea of representing each input graph as a fixed point of a dynamical system (implemented through a recurrent neural network), and leverages a deep architectural organization of the recurrent units. Efficiency is gained by many aspects, including the use of small and very sparse networks, where the weights of the recurrent units are left untrained under the stability condition introduced in this work. This can be viewed as a way to study the intrinsic power of the architecture of a deep GNN, and also to provide insights for the set-up of more complex fully-trained models. Through experimental results, we show that even without training of the recurrent connections, the architecture of small deep GNN is surprisingly able to achieve or improve the <font color="red">state-of-the-art</font> performance on a significant set of tasks in the field of graphs classification. </br></br>

<a href='http://arxiv.org/pdf/1911.07722.pdf'>1911.07722</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.8998баллов, №133</br>
<b>SySCD: A System-Aware Parallel Coordinate Descent Algorithm</b></br>
Authors: , Ioannou, Nikolas, Mendler-D&#xfc;nner, Celestine, Parnell, Thomas</br>
  In this paper we propose a novel parallel stochastic coordinate descent (SCD) algorithm with convergence guarantees that exhibits strong scalability. We start by studying a <font color="red">state-of-the-art</font> parallel implementation of SCD and identify scalability as well as system-level performance bottlenecks of the respective implementation. We then take a principled approach to develop a new SCD variant which is designed to avoid the identified system bottlenecks, such as limited scaling due to coherence traffic of model sharing across threads, and inefficient CPU cache accesses. Our proposed system-aware parallel coordinate descent algorithm (SySCD) scales to many cores and across numa nodes, and offers a consistent bottom line speedup in training time of up to x12 compared to an optimized asynchronous parallel SCD algorithm and up to x42, compared to state-of-the-art GLM solvers (scikit-learn, Vowpal Wabbit, and H2O) on a range of datasets and multi-core CPU architectures. </br></br>

<a href='http://arxiv.org/pdf/1911.09245.pdf'>1911.09245</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.8989баллов, №134</br>
<b>Consensus-based Optimization for 3D Human Pose Estimation in Camera\n  Coordinates</b></br>
Authors: , Luvizon, Diogo C, Tabia, Hedi, Picard, David</br>
  3D human pose estimation is frequently seen as the task of estimating 3D poses relative to the root body joint. Alternatively, in this paper, we propose a 3D human pose estimation method in camera coordinates, which allows effective combination of 2D annotated data and 3D poses, as well as a straightforward multi-view generalization. To that end, we cast the problem into a different perspective, where 3D poses are predicted in the image plane, in pixels, and the absolute depth is estimated in millimeters. Based on this, we propose a consensus-based optimization algorithm for multi-view predictions from uncalibrated images, which requires a single monocular training procedure. Our method improves the <font color="red">state-of-the-art</font> on well known 3D human pose datasets, reducing the prediction error by 32% in the most common benchmark. In addition, we also reported our results in absolute pose position error, achieving 80mm for monocular estimations and 51mm for multi-view, on average. </br></br>

<a href='http://arxiv.org/pdf/1911.07710.pdf'>1911.07710</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.8970баллов, №135</br>
<b>Feedback Control for Online Training of Neural Networks</b></br>
Authors: , Zhao, Zilong, Cerf, Sophie, Robu, Bogdan, Marchand, Nicolas</br>
  Convolutional neural networks (CNNs) are commonly used for image classification tasks, raising the challenge of their application on data flows. During their training, adaptation is often performed by tuning the learning rate. Usual learning rate strategies are time-based i.e. monotonously decreasing. In this paper, we advocate switching to a performance-based adaptation, in order to improve the learning efficiency. We present E (Exponential)/PD (Proportional Derivative)-Control, a conditional learning rate strategy that combines a feedback PD controller based on the CNN loss function, with an exponential control signal to smartly boost the learning and adapt the PD parameters. Stability proof is provided as well as an experimental evaluation using two <font color="red">state of the art</font> image datasets (CIFAR-10 and Fashion-MNIST). Results show better performances than the related works (faster network accuracy growth reaching higher levels) and robustness of the E/PD-Control regarding its parametrization. </br></br>

<a href='http://arxiv.org/pdf/1911.07424.pdf'>1911.07424</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.8901баллов, №136</br>
<b>Capturing Hand Articulations using Recurrent Neural Network for 3D Hand\n  Pose Estimation</b></br>
Authors: , Yoo, Cheol-hwan, Kim, Seung-wook, Ji, Seo-won, Shin, Yong-goo, Ko, Sung-jea</br>
  3D hand pose estimation from a single depth image plays an important role in computer vision and human-computer interaction. Although recent hand pose estimation methods using convolution neural network~(CNN) have shown notable improvements in accuracy, most of them have a limitation that they rely on a complex network structure without fully exploiting the articulated structure of the hand. A hand, which is an articulated object, is composed of six local parts: the palm and five independent fingers. Each finger consists of sequential-joints that provide constrained motion, referred to as a kinematic chain. In this paper, we propose a <font color="#00be00">hierarchical</font>ly-structured convolutional recurrent neural network~(HCRNN) with six branches that estimate the 3D position of the palm and five fingers independently. The palm position is predicted via fully-connected layers. Each sequential-joint, i.e. finger position, is obtained using a recurrent neural network~(RNN) to capture the spatial dependencies between adjacent joints. HCRNN directly takes the depth map as an input without a time-consuming data conversion, such as 3D voxels and <font color="#be00be">point cloud</font>s. Experimental results on public datasets demonstrate that the proposed HCRNN not only <font color="#00be00">outperform</font>s the 2D CNN-based methods using the depth image as their inputs but also achieves <font color="#960096">competitive</font> results with <font color="red">state-of-the-art</font> 3D CNN-based methods with a highly efficient running speed of 240 fps on a single GPU. </br></br>

<a href='http://arxiv.org/pdf/1911.06892.pdf'>1911.06892</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.8873баллов, №137</br>
<b>Topological based classification using graph convolutional networks</b></br>
Authors: , Abel, Roy, Benami, Idan, Louzoun, Yoram</br>
  In colored graphs, node classes are often associated with either their neighbors class or with information not incorporated in the graph associated with each node. We here propose that node classes are also associated with topological features of the nodes. We use this association to improve Graph machine learning in general and specifically, Graph Convolutional Networks (GCN).   First, we show that even in the absence of any external information on nodes, a good accuracy can be obtained on the prediction of the node class using either topological features, or using the neighbors class as an input to a GCN. This accuracy is slightly less than the one that can be obtained using content based GCN.   Secondly, we show that explicitly adding the topology as an input to the GCN does not improve the accuracy when combined with external information on nodes. However, adding an additional adjacency matrix with edges between distant nodes with similar topology to the GCN does significantly improve its accuracy, leading to results better than all <font color="red">state of the art</font> methods in multiple datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.06957.pdf'>1911.06957</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 0.8843баллов, №138</br>
<b>An Induced Multi-Relational Framework for Answer Selection in Community\n  Question Answer Platforms</b></br>
Authors: , Narang, Kanika, Yang, Chaoqi, Krishnan, Adit, Wang, Junting, Sundaram, Hari, Sutter, Carolyn</br>
  This paper addresses the question of identifying the best candidate answer to a question on Community Question Answer (CQA) forums. The problem is important because Individuals often visit CQA forums to seek answers to nuanced questions. We develop a novel induced relational graph convolutional network (IR-GCN) framework to address the question. We make three contributions. First, we introduce a modular framework that separates the construction of the graph with the label selection mechanism. We use equivalence relations to induce a graph comprising cliques and identify two label assignment mechanisms---label contrast, label sharing. Then, we show how to encode these assignment mechanisms in GCNs. Second, we show that encoding contrast creates discriminative magnification---enhancing the separation between nodes in the embedding space. Third, we show a surprising result---boosting techniques improve learning over familiar stacking, fusion, or aggregation approaches for neural architectures. We show strong results over the <font color="red">state-of-the-art</font> neural baselines in extensive experiments on 50 StackExchange communities. </br></br>

<a href='http://arxiv.org/pdf/1911.08217.pdf'>1911.08217</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.8821баллов, №139</br>
<b>Constrained R-CNN: A general image manipulation detection model</b></br>
Authors: , Li, Huizhou, Yang, Chao, Lin, Fangting, Jiang, Bin</br>
  Recently, deep learning-based models have exhibited remarkable performance for image manipulation detection. However, most of them suffer from poor universality of handcrafted or predetermined features. Meanwhile, they only focus on manipulation localization and overlook manipulation classification. To address these issues, we propose a coarse-to-fine architecture named Constrained R-CNN for complete and accurate image forensics. First, the learnable manipulation feature extractor learns a unified feature representation directly from data. Second, the attention region proposal network effectively discriminates manipulated regions for the next manipulation classification and coarse localization. Then, the skip structure fuses low-level and high-level information to refine the global manipulation features. Finally, the coarse localization information guides the model to further learn the finer local features and segment out the tampered region. Experimental results demonstrate that our model achieves <font color="red">state-of-the-art</font> performance. Especially, the F1 score is increased by 28.4%, 73.2%, 13.3% on the NIST16, COVERAGE, and Columbia dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.09178.pdf'>1911.09178</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.8819баллов, №140</br>
<b>RIS-GAN: Explore Residual and Illumination with Generative Adversarial\n  Networks for Shadow Removal</b></br>
Authors: , Zhang, Ling, Long, Chengjiang, Zhang, Xiaolong, Xiao, Chunxia</br>
  Residual images and illumination estimation have been proved very helpful in image enhancement. In this paper, we propose a general and novel framework RIS-GAN which explores residual and illumination with Generative Adversarial Networks for shadow removal. Combined with the coarse shadow-removal image, the estimated negative residual images and inverse illumination maps can be used to generate indirect shadow-removal images to refine the coarse shadow-removal result to the fine shadow-free image in a coarse-to-fine fashion. Three discriminators are designed to distinguish whether the predicted negative residual images, shadow-removal images, and the inverse illumination maps are real or fake jointly compared with the corresponding ground-truth information. To our best knowledge, we are the first one to explore residual and illumination for shadow removal. We evaluate our proposed method on two benchmark datasets, i.e., SRD and ISTD, and the extensive experiments demonstrate that our proposed method achieves the superior performance to <font color="red">state-of-the-art</font>s, although we have no particular shadow-aware components designed in our generators. </br></br>

<a href='http://arxiv.org/pdf/1911.09349.pdf'>1911.09349</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.8811баллов, №141</br>
<b>An End-to-End Audio Classification System based on Raw Waveforms and\n  Mix-Training Strategy</b></br>
Authors: , Chen, Jiaxu, Hao, Jing, Chen, Kai, Xie, Di, Yang, Shicai, Pu, Shiliang</br>
  Audio classification can distinguish different kinds of sounds, which is helpful for intelligent applications in daily life. However, it remains a challenging task since the sound events in an audio clip is probably multiple, even overlapping. This paper introduces an end-to-end audio classification system based on raw waveforms and mix-training strategy. Compared to human-designed features which have been widely used in existing research, raw waveforms contain more complete information and are more appropriate for multi-label classification. Taking raw waveforms as input, our network consists of two variants of ResNet structure which can learn a discriminative representation. To explore the information in intermediate layers, a multi-level prediction with attention structure is applied in our model. Furthermore, we design a mix-training strategy to break the performance limitation caused by the amount of training data. Experiments show that the mean average precision of the proposed audio classification system on Audio Set dataset is 37.2%. Without using extra training data, our system exceeds the <font color="red">state-of-the-art</font> multi-level attention model. </br></br>

<a href='http://arxiv.org/pdf/1911.00108.pdf'>1911.00108</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.8801баллов, №142</br>
<b>RankML: a Meta Learning-Based Approach for Pre-Ranking Machine Learning\n  Pipelines</b></br>
Authors: , Laadan, Doron, Vainshtein, Roman, Curiel, Yarden, Katz, Gilad, Rokach, Lior</br>
  The explosion of digital data has created multiple opportunities for organizations and individuals to leverage machine learning (ML) to transform the way they operate. However, the shortage of experts in the field of machine learning -- data scientists -- is often a setback to the use of ML. In an attempt to alleviate this shortage, multiple approaches for the automation of machine learning have been proposed in recent years. While these approaches are effective, they often require a great deal of time and computing resources. In this study, we propose RankML, a meta-learning based approach for predicting the performance of whole machine learning pipelines. Given a previously-unseen dataset, a performance metric, and a set of candidate pipelines, RankML immediately produces a ranked list of all pipelines based on their predicted performance. Extensive evaluation on 244 datasets, both in <font color="#be00be">regression</font> and classification tasks, shows that our approach either <font color="#00be00">outperform</font>s or is comparable to <font color="red">state-of-the-art</font>, computationally heavy approaches while requiring a fraction of the time and computational cost. </br></br>

<a href='http://arxiv.org/pdf/1911.09514.pdf'>1911.09514</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp 0.8773баллов, №143</br>
<b>Continual Learning with Adaptive Weights (CLAW)</b></br>
Authors: , Adel, Tameem, Zhao, Han, Turner, Richard E.</br>
  Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves <font color="red">state-of-the-art</font> performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting. </br></br>

<a href='http://arxiv.org/pdf/1911.08907.pdf'>1911.08907</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 0.8723баллов, №144</br>
<b>Auto-Precision Scaling for Distributed Deep Learning</b></br>
Authors: , Han, Ruobing, You, Yang, Demmel, James</br>
  In recent years, large-batch optimization is becoming the key of distributed deep learning. However, large-batch optimization is hard. Straightforwardly porting the code often leads to a significant loss in testing accuracy. As some researchers suggested that large batch optimization leads to a low generalization performance, and they further conjectured that large-batch training needs a higher floating-point precision to achieve a higher generalization performance. To solve this problem, we conduct an open study in this paper. Our target is to find the number of bits that large-batch training needs. To do so, we need a system for customized precision study. However, <font color="red">state-of-the-art</font> systems have some limitations that lower the efficiency of developers and researchers. To solve this problem, we design and implement our own system CPD: A High Performance System for Customized-Precision Distributed DL. In our experiments, our application often loses accuracy if we use a very-low precision (e.g. 8 bits or 4 bits). To solve this problem, we proposed the APS (Auto-Precision-Scaling) algorithm, which is a layer-wise adaptive scheme for gradients shifting. With APS, we are able to make the large-batch training converge with only 4 bits. </br></br>

<a href='http://arxiv.org/pdf/1911.07951.pdf'>1911.07951</a> &nbsp&nbsp (cs:SD, cs:ML, stat:ML) &nbsp&nbsp 0.8616баллов, №145</br>
<b>Improving Universal Sound Separation Using Sound Classification</b></br>
Authors: , Tzinis, Efthymios, Wisdom, Scott, Hershey, John R., Jansen, Aren, Ellis, Daniel P. W.</br>
  Deep learning approaches have recently achieved impressive performance on both audio source separation and sound classification. Most audio source separation approaches focus only on separating sources belonging to a restricted domain of source classes, such as speech and music. However, recent work has demonstrated the possibility of &quot;universal sound separation&quot;, which aims to separate acoustic sources from an open domain, regardless of their class. In this paper, we utilize the semantic information learned by sound classifier networks trained on a vast amount of diverse sounds to improve universal sound separation. In particular, we show that semantic embeddings extracted from a sound classifier can be used to condition a separation network, providing it with useful additional information. This approach is especially useful in an iterative setup, where source estimates from an initial separation stage and their corresponding classifier-derived embeddings are fed to a second separation network. By performing a thorough hyperparameter search consisting of over a thousand experiments, we find that classifier embeddings from clean sources provide nearly one dB of SNR gain, and our best iterative models achieve a significant fraction of this oracle performance, establishing a new <font color="red">state-of-the-art</font> for universal sound separation. </br></br>

<a href='http://arxiv.org/pdf/1911.07808.pdf'>1911.07808</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.8614баллов, №146</br>
<b>Unsupervised Representation Learning by Discovering Reliable Image\n  Relations</b></br>
Authors: , Milbich, Timo, Ghori, Omair, Diego, Ferran, Ommer, Bj&#xf6;rn</br>
  Learning robust representations that allow to reliably establish relations between images is of paramount importance for virtually all of computer vision. Annotating the quadratic number of pairwise relations between training images is simply not feasible, while unsupervised inference is prone to noise, thus leaving the vast majority of these relations to be unreliable. To nevertheless find those relations which can be reliably utilized for learning, we follow a divide-and-conquer strategy: We find reliable similarities by extracting compact groups of images and reliable dissimilarities by partitioning these groups into subsets, converting the complicated overall problem into few reliable local subproblems. For each of the subsets we obtain a representation by learning a mapping to a target feature space so that their reliable relations are kept. Transitivity relations between the subsets are then exploited to consolidate the local solutions into a concerted global representation. While iterating between grouping, partitioning, and learning, we can successively use more and more reliable relations which, in turn, improves our image representation. In experiments, our approach shows <font color="red">state-of-the-art</font> performance on unsupervised classification on ImageNet with 46.0% and competes favorably on different transfer learning tasks on PASCAL VOC. </br></br>

<a href='http://arxiv.org/pdf/1911.09267.pdf'>1911.09267</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.8608баллов, №147</br>
<b>Semantic Hierarchy Emerges in Deep Generative Representations for Scene\n  Synthesis</b></br>
Authors: , Yang, Ceyuan, Shen, Yujun, Zhou, Bolei</br>
  Despite the success of Generative Adversarial Networks (GANs) in image synthesis, there lacks enough understanding on what networks have learned inside the deep generative representations and how photo-realistic images are able to be composed from random noises. In this work, we show that highly-structured semantic hierarchy emerges as variation factors for synthesizing scenes from the generative representations in <font color="red">state-of-the-art</font> GAN models, like <font color="#be00be">Style</font>GAN and BigGAN. By probing the layer-wise representations with a broad set of semantics at different abstraction levels, we are able to quantify the causality between the activations and semantics occurring in the output image. Such a quantification identifies the human-understandable variation factors learned by GANs to compose scenes. The qualitative and quantitative results suggest that the generative representations learned by the GANs with layer-wise latent codes are specialized to synthesize different <font color="#00be00">hierarchical</font> semantics: the early layers tend to determine the spatial layout and configuration, the middle layers control the categorical objects, and the later layers finally render the scene attributes as well as color scheme. Identifying such a set of manipulatable latent variation factors facilitates semantic scene manipulation. </br></br>

<a href='http://arxiv.org/pdf/1911.09307.pdf'>1911.09307</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.8601баллов, №148</br>
<b>Patch-level Neighborhood Interpolation: A General and Effective\n  Graph-based Regularization Strategy</b></br>
Authors: , Sun, Ke, Yu, Bing, Lin, Zhouchen, Zhu, Zhanxing</br>
  Regularization plays a crucial role in machine learning models, especially for deep neural networks. The existing regularization techniques mainly reply on the i.i.d. assumption and only employ the information of the current sample, without the leverage of neighboring information between samples. In this work, we propose a general regularizer called Patch-level Neighborhood Interpolation~(\\textbf{Pani}) that fully exploits the relationship between samples. Furthermore, by explicitly constructing a patch-level graph in the different network layers and interpolating the neighborhood features to refine the representation of the current sample, our Patch-level Neighborhood Interpolation can then be applied to enhance two popular regularization strategies, namely Virtual Adversarial Training (VAT) and MixUp, yielding their neighborhood versions. The first derived \\textbf{Pani VAT} presents a novel way to construct non-local adversarial smoothness by incorporating patch-level interpolated perturbations. In addition, the \\textbf{Pani MixUp} method extends the original MixUp regularization to the patch level and then can be developed to MixMatch, achieving the <font color="red">state-of-the-art</font> performance. Finally, extensive experiments are conducted to verify the effectiveness of the Patch-level Neighborhood Interpolation in both supervised and semi-supervised settings. </br></br>

<a href='http://arxiv.org/pdf/1911.07747.pdf'>1911.07747</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.8599баллов, №149</br>
<b>DeepSat V2: Feature Augmented Convolutional Neural Nets for Satellite\n  Image Classification</b></br>
Authors: , Liu, Qun, Basu, Saikat, Ganguly, Sangram, Mukhopadhyay, Supratik, DiBiano, Robert, Karki, Manohar, Nemani, Ramakrishna</br>
  Satellite image classification is a challenging problem that lies at the crossroads of remote sensing, computer vision, and machine learning. Due to the high variability inherent in satellite data, most of the current object classification approaches are not suitable for handling satellite datasets. The progress of satellite image analytics has also been inhibited by the lack of a single labeled high-resolution dataset with multiple class labels. In a preliminary version of this work, we introduced two new high resolution satellite imagery datasets (SAT-4 and SAT-6) and proposed DeepSat framework for classification based on &quot;handcrafted&quot; features and a deep belief network (DBN). The present paper is an extended version, we present an end-to-end framework leveraging an improved architecture that augments a convolutional neural network (CNN) with handcrafted features (instead of using DBN-based architecture) for classification. Our framework, having access to fused spatial information obtained from handcrafted features as well as CNN feature maps, have achieved accuracies of 99.90% and 99.84% respectively, on SAT-4 and SAT-6, surpassing all the other <font color="red">state-of-the-art</font> results. A statistical analysis based on Distribution Separability Criterion substantiates the robustness of our approach in learning better representations for satellite imagery. </br></br>

<a href='http://arxiv.org/pdf/1911.07528.pdf'>1911.07528</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.8576баллов, №150</br>
<b>Ladder Loss for Coherent Visual-Semantic Embedding</b></br>
Authors: , Zhou, Mo, Niu, Zhenxing, Wang, Le, Gao, Zhanning, Zhang, Qilin, Hua, Gang</br>
  For visual-semantic embedding, the existing methods normally treat the relevance between queries and candidates in a bipolar way -- relevant or irrelevant, and all &quot;irrelevant&quot; candidates are uniformly pushed away from the query by an equal margin in the embedding space, regardless of their various proximity to the query. This practice disregards relatively discriminative information and could lead to suboptimal ranking in the retrieval results and poorer user experience, especially in the long-tail query scenario where a matching candidate may not necessarily exist. In this paper, we introduce a continuous variable to model the relevance degree between queries and multiple candidates, and propose to learn a coherent embedding space, where candidates with higher relevance degrees are mapped closer to the query than those with lower relevance degrees. In particular, the new ladder loss is proposed by extending the triplet loss inequality to a more general inequality chain, which implements variable push-away margins according to respective relevance degrees. In addition, a proper Coherent Score metric is proposed to better measure the ranking results including those &quot;irrelevant&quot; candidates. Extensive experiments on multiple datasets validate the efficacy of our proposed method, which achieves significant improvement over existing <font color="red">state-of-the-art</font> methods. </br></br>

<a href='http://arxiv.org/pdf/1911.08432.pdf'>1911.08432</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.8574баллов, №151</br>
<b>Defective Convolutional Layers Learn Robust CNNs</b></br>
Authors: , Luo, Tiange, Cai, Tianle, Zhang, Mengxiao, Chen, Siyu, He, Di, Wang, Liwei</br>
  Robustness of convolutional neural networks has recently been highlighted by the adversarial examples, i.e., inputs added with well-designed perturbations which are imperceptible to humans but can cause the network to give incorrect outputs. Recent research suggests that the noises in adversarial examples break the textural structure, which eventually leads to wrong predictions by convolutional neural networks. To help a convolutional neural network make predictions relying less on textural information, we propose defective convolutional layers which contain defective neurons whose activations are set to be a constant function. As the defective neurons contain no information and are far different from the standard neurons in its spatial neighborhood, the textural features cannot be accurately extracted and the model has to seek for other features for classification, such as the shape. We first show that predictions made by the defective CNN are less dependent on textural information, but more on shape information, and further find that adversarial examples generated by the defective CNN appear to have semantic shapes. Experimental results demonstrate the defective CNN has higher defense ability than the standard CNN against various types of attack. In particular, it achieves <font color="red">state-of-the-art</font> performance against transfer-based attacks without applying any adversarial training. </br></br>

<a href='http://arxiv.org/pdf/1911.07924.pdf'>1911.07924</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.8566баллов, №152</br>
<b>Logo-2K+: A Large-Scale Logo Dataset for Scalable Logo Classification</b></br>
Authors: , Wang, Jing, Min, Weiqing, Hou, Sujuan, Ma, Shengnan, Zheng, Yuanjie, Wang, Haishuai, Jiang, Shuqiang</br>
  Logo classification has gained increasing attention for its various applications, such as copyright infringement detection, product <font color="blue">recommendat</font>ion and contextual advertising. Compared with other types of object images, the <font color="#009600">real-world</font> logo images have larger variety in logo appearance and more complexity in their background. Therefore, recognizing the logo from images is challenging. To support efforts towards scalable logo classification task, we have curated a dataset, Logo-2K+, a new large-scale <font color="#00be00">publicly available</font> real-world logo dataset with 2,341 categories and 167,140 images. Compared with existing popular logo datasets, such as FlickrLogos-32 and LOGO-Net, Logo-2K+ has more comprehensive coverage of logo categories and larger quantity of logo images. Moreover, we propose a Discriminative Region Navigation and Augmentation Network (DRNA-Net), which is capable of discovering more informative logo regions and augmenting these image regions for logo classification. DRNA-Net consists of four sub-networks: the navigator sub-network first selected informative logo-relevant regions guided by the teacher sub-network, which can evaluate its confidence belonging to the ground-truth logo class. The data augmentation sub-network then augments the selected regions via both region cropping and region dropping. Finally, the scrutinizer sub-network fuses features from augmented regions and the whole image for logo classification. Comprehensive experiments on Logo-2K+ and other three existing benchmark datasets demonstrate the effectiveness of proposed method. Logo-2K+ and the proposed strong baseline DRNA-Net are expected to further the development of scalable logo image recognition, and the Logo-2K+ dataset can be found at <font color="#006400">http</font>s://<font color="red">github</font>.com/msn199959/Logo-2k-plus-Dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.06982.pdf'>1911.06982</a> &nbsp&nbsp (cs:ML, cs:CV) &nbsp&nbsp 0.8553баллов, №153</br>
<b>VLUC: An Empirical Benchmark for Video-Like Urban Computing on Citywide\n  Crowd and Traffic Prediction</b></br>
Authors: , Jiang, Renhe, Cai, Zekun, Wang, Zhaonan, Yang, Chuang, Fan, Zipei, Song, Xuan, Tsubouchi, Kota, Shibasaki, Ryosuke</br>
  Nowadays, massive urban human mobility data are being generated from<font color="#960096"> mobile </font>phones, car navigation systems, and traffic sensors. Predicting the density and flow of the crowd or traffic at a citywide level becomes possible by using the big data and cutting-edge AI technologies. It has been a very significant research topic with high social impact, which can be widely applied to emergency management, traffic regulation, and urban planning. In particular, by meshing a large urban area to a number of fine-grained mesh-grids, citywide crowd and traffic information in a continuous time period can be represented like a video, where each timestamp can be seen as one video frame. Based on this idea, a series of methods have been proposed to address video-like prediction for citywide crowd and traffic. In this study, we publish a new aggregated human mobility dataset generated from a <font color="#009600">real-world</font> smartphone application and build a standard benchmark for such kind of video-like urban computing with this new dataset and the existing open datasets. We first comprehensively review the <font color="red">state-of-the-art</font> works of literature and formulate the density and in-out flow prediction problem, then conduct a thorough performance assessment for those methods. With this benchmark, we hope researchers can easily follow up and quickly launch a new solution on this topic. </br></br>

<a href='http://arxiv.org/pdf/1911.08688.pdf'>1911.08688</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.8518баллов, №154</br>
<b>SSAH: Semi-supervised Adversarial Deep Hashing with Self-paced Hard\n  Sample Generation</b></br>
Authors: , Jin, Sheng, Zhou, Shangchen, Liu, Yao, Chen, Chao, Sun, Xiaoshuai, Yao, Hongxun, Hua, Xiansheng</br>
  Deep hashing methods have been proved to be effective and efficient for large-scale Web media search. The success of these data-driven methods largely depends on collecting sufficient labeled data, which is usually a crucial limitation in practical cases. The current solutions to this issue utilize Generative Adversarial Network (GAN) to augment data in semi-supervised learning. However, existing GAN-based methods treat image generations and hashing learning as two isolated processes, leading to generation ineffectiveness. Besides, most works fail to exploit the semantic information in unlabeled data. In this paper, we propose a novel Semi-supervised Self-pace Adversarial Hashing method, named SSAH to solve the above problems in a unified framework. The SSAH method consists of an adversarial network (A-Net) and a hashing network (H-Net). To improve the quality of generative images, first, the A-Net learns hard samples with multi-scale occlusions and multi-angle rotated deformations which compete against the learning of accurate hashing codes. Second, we design a novel self-paced hard generation policy to gradually increase the hashing difficulty of generated samples. To make use of the semantic information in unlabeled ones, we propose a semi-supervised consistent loss. The experimental results show that our method can significantly improve <font color="red">state-of-the-art</font> models on both the widely-used hashing datasets and fine-grained datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.09243.pdf'>1911.09243</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.8518баллов, №155</br>
<b>Multi-Label Classification with Label Graph Superimposing</b></br>
Authors: , Wang, Ya, He, Dongliang, Li, Fu, Long, Xiang, Zhou, Zhichao, Ma, Jinwen, Wen, Shilei</br>
  Images or videos always contain multiple objects or actions. Multi-label recognition has been witnessed to achieve pretty performance attribute to the rapid development of deep learning technologies. Recently, graph convolution network (GCN) is leveraged to boost the performance of multi-label recognition. However, what is the best way for label correlation modeling and how feature learning can be improved with label system awareness are still unclear. In this paper, we propose a label graph superimposing framework to improve the conventional GCN+CNN framework developed for multi-label recognition in the following two aspects. Firstly, we model the label correlations by superimposing label graph built from statistical co-occurrence information into the graph constructed from knowledge priors of labels, and then multi-layer graph convolutions are applied on the final superimposed graph for label embedding abstraction. Secondly, we propose to leverage embedding of the whole label system for better representation learning. In detail, lateral connections between GCN and CNN are added at shallow, middle and deep layers to inject information of label system into backbone CNN for label-awareness in the feature learning process. Extensive experiments are carried out on MS-COCO and Charades datasets, showing that our proposed solution can greatly improve the recognition performance and achieves new <font color="red">state-of-the-art</font> recognition performance. </br></br>

<a href='http://arxiv.org/pdf/1911.07474.pdf'>1911.07474</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 0.8495баллов, №156</br>
<b>Deep and Dense Sarcasm Detection</b></br>
Authors: , Pelser, Devin, Murrell, Hugh</br>
  Recent work in automated sarcasm detection has placed a heavy focus on context and meta-data. Whilst certain utterances indeed require background knowledge and commonsense reasoning, previous works have only explored shallow models for capturing the lexical, syntactic and semantic cues present within a text. In this paper, we propose a deep 56 layer network, implemented with dense connectivity to model the isolated utterance and extract richer features therein. We compare our approach against recent <font color="red">state-of-the-art</font> architectures which make considerable use of extrinsic information, and demonstrate <font color="#960096">competitive</font> results whilst using only the local features of the text. Further, we provide an analysis of the dependency of prior convolution outputs in generating the final feature maps. Finally a case study is presented, supporting that our approach accurately classifies additional uses of clear sarcasm, which a standard CNN misclassifies. </br></br>

<a href='http://arxiv.org/pdf/1911.09509.pdf'>1911.09509</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.8436баллов, №157</br>
<b>Deep Representations for Cross-spectral Ocular Biometrics</b></br>
Authors: , Zanlorensi, Luiz A., Lucio, Diego R., Britto Jr., Alceu S., Proen&#xe7;a, Hugo, Menotti, David</br>
  One of the major challenges in ocular biometrics is the cross-spectral scenario, i.e., how to match images acquired in different wavelengths (typically visible (VIS) against near-infrared (NIR)). This article designs and extensively evaluates cross-spectral ocular verification methods, for both the closed and open-world settings, using well known deep learning representations based on the iris and periocular regions. Using as inputs the bounding boxes of non-normalized iris/periocular regions, we fine-tune Convolutional Neural Network(CNN) models (based either on VGG16 or ResNet-50 architectures), originally trained for<font color="#be00be"> face </font>recognition. Based on the experiments carried out in two <font color="#00be00">publicly available</font> cross-spectral ocular databases, we report results for intra-spectral and cross-spectral scenarios, with the best performance being observed when fusing ResNet-50 deep representations from both the periocular and iris regions. When compared to the <font color="red">state-of-the-art</font>, we observed that the proposed solution consistently reduces the Equal Error Rate(EER) values by 90% / 93% / 96% and 61% / 77% / 83% on the cross-spectral scenario and in the PolyU Bi-spectral and Cross-eye-cross-spectral datasets. Lastly, we evaluate the effect that the &quot;deepness&quot; factor of feature representations has in recognition effectiveness, and - based on a subjective analysis of the most problematic pairwise comparisons - we point out further directions for this field of research. </br></br>

<a href='http://arxiv.org/pdf/1911.06910.pdf'>1911.06910</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp 0.8366баллов, №158</br>
<b>CNN-based Dual-Chain Models for Knowledge Graph Learning</b></br>
Authors: , Peng, Bo, Min, Renqiang, Ning, Xia</br>
  <font color="#960096">Knowledge graph</font> learning plays a critical role in integrating domain specific knowledge bases when deploying machine learning and data mining models in practice. Existing methods on knowledge graph learning primarily focus on modeling the relations among entities as translations among the relations and entities, and many of these methods are not able to handle <font color="#00be00">zero-shot</font> problems, when new entities emerge. In this paper, we present a new convolutional neural network (CNN)-based dual-chain model. Different from translation based methods, in our model, interactions among relations and entities are directly captured via CNN over their embeddings. Moreover, a secondary chain of learning is conducted simultaneously to incorporate additional information and to enable better performance. We also present an extension of this model, which incorporates descriptions of entities and learns a second set of entity embeddings from the descriptions. As a result, the extended model is able to effectively handle zero-shot problems. We conducted comprehensive experiments, comparing our methods with 15 methods on 8 benchmark datasets. Extensive experimental results demonstrate that our proposed methods achieve or <font color="#00be00">outperform</font> the <font color="red">state-of-the-art</font> results on knowledge graph learning, and outperform other methods on zero-shot problems. In addition, our methods applied to <font color="#009600">real-world</font> bio<font color="blue">medic</font>al data are able to produce results that conform to expert domain knowledge. </br></br>

<a href='http://arxiv.org/pdf/1911.07918.pdf'>1911.07918</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp 0.8345баллов, №159</br>
<b>Improving Document Classification with Multi-Sense Embeddings</b></br>
Authors: , Gupta, Vivek, Saw, Ankit, Nokhiz, Pegah, Gupta, Harshit, Talukdar, Partha</br>
  Efficient representation of text documents is an important building block in many NLP tasks. Research on long text categorization has shown that simple weighted averaging of word vectors for sentence representation often <font color="#00be00">outperform</font>s more sophisticated neural models. Recently proposed Sparse Composite Document Vector (SCDV) (Mekala et. al, 2017) extends this approach from sentences to documents using soft <font color="#be00be">clustering</font> over word vectors. However, SCDV disregards the multi-sense nature of words, and it also suffers from the curse of higher dimensionality. In this work, we address these shortcomings and propose SCDV-MS. SCDV-MS utilizes multi-sense word embeddings and learns a lower dimensional manifold. Through extensive experiments on multiple <font color="#009600">real-world</font> datasets, we show that SCDV-MS embeddings outperform previous <font color="red">state-of-the-art</font> embeddings on multi-class and multi-label text categorization tasks. Furthermore, SCDV-MS embeddings are more efficient than SCDV in terms of time and space complexity on textual classification tasks. </br></br>

<a href='http://arxiv.org/pdf/1911.07524.pdf'>1911.07524</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.8337баллов, №160</br>
<b>The Devil is in the Details: Delving into Unbiased Data Processing for\n  Human Pose Estimation</b></br>
Authors: , Huang, Junjie, Zhu, Zheng, Guo, Feng, Huang, Guan</br>
  Recently, the leading performance of human pose estimation is dominated by top-down methods. Being a fundamental component in training and inference, data processing has not been systematically considered in pose estimation community, to the best of our knowledge. In this paper, we focus on this problem and find that the devil of top-down pose estimator is in the biased data processing. Specifically, by investigating the standard data processing in <font color="red">state-of-the-art</font> approaches mainly including data transformation and encoding-decoding, we find that the results obtained by common flipping strategy are unaligned with the original ones in inference. Moreover, there is statistical error in standard encoding-decoding during both training and inference. Two problems couple together and significantly degrade the pose estimation performance. Based on quantitative analyses, we then formulate a principled way to tackle this dilemma. Data is processed based on unit length instead of pixel, and an offset-based strategy is adopted to perform encoding-decoding. The Unbiased Data Processing (UDP) for human pose estimation can be achieved by combining the two together. UDP not only boosts the performance of existing methods by a large margin but also plays a important role in result reproducing and future exploration. As a model-agnostic approach, UDP promotes SimpleBaseline-ResNet-50-256x192 by 1.5 AP (70.2 to 71.7) and HRNet-W32-256x192 by 1.7 AP (73.5 to 75.2) on COCO test-dev set. The HRNet-W48-384x288 equipped with UDP achieves 76.5 AP and sets a new state-of-the-art for human pose estimation. The code will be released. </br></br>

<a href='http://arxiv.org/pdf/1911.08680.pdf'>1911.08680</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.8308баллов, №161</br>
<b>Discriminative Local Sparse Representation by Robust Adaptive Dictionary\n  Pair Learning</b></br>
Authors: , Sun, Yulin, Zhang, Zhao, Jiang, Weiming, Zhang, Zheng, Zhang, Li, Yan, Shuicheng, Wang, Meng</br>
  In this paper, we propose a structured Robust Adaptive Dic-tionary Pair Learning (RA-DPL) framework for the discrim-inative sparse representation learning. To achieve powerful representation ability of the available samples, the setting of RA-DPL seamlessly integrates the robust projective dictionary pair learning, locality-adaptive sparse representations and discriminative coding coefficients learning into a unified learning framework. Specifically, RA-DPL improves existing projective dictionary pair learning in four perspectives. First, it applies a sparse l2,1-norm based metric to encode the recon-struction error to deliver the robust projective dictionary pairs, and the l2,1-norm has the potential to minimize the error. Sec-ond, it imposes the robust l2,1-norm clearly on the analysis dictionary to ensure the sparse property of the coding coeffi-cients rather than using the costly l0/l1-norm. As such, the robustness of the data representation and the efficiency of the learning process are jointly considered to guarantee the effi-cacy of our RA-DPL. Third, RA-DPL conceives a structured reconstruction weight learning paradigm to preserve the local structures of the coding coefficients within each class clearly in an adaptive manner, which encourages to produce the locality preserving representations. Fourth, it also considers improving the discriminating ability of coding coefficients and dictionary by incorporating a discriminating function, which can ensure high intra-class compactness and inter-class separation in the code space. Extensive experiments show that our RA-DPL can obtain superior performance over other <font color="red">state-of-the-art</font>s. </br></br>

<a href='http://arxiv.org/pdf/1911.07489.pdf'>1911.07489</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.8241баллов, №162</br>
<b>Towards Making Deep Transfer Learning Never Hurt</b></br>
Authors: , Wan, Ruosi, Xiong, Haoyi, Li, Xingjian, Zhu, Zhanxing, Huan, Jun</br>
  Transfer learning have been frequently used to improve deep neural network training through incorporating weights of pre-trained networks as the starting-point of optimization for regularization. While deep transfer learning can usually boost the performance with better accuracy and faster convergence, transferring weights from inappropriate networks hurts training procedure and may lead to even lower accuracy. In this paper, we consider deep transfer learning as minimizing a linear combination of empirical loss and regularizer based on pre-trained weights, where the regularizer would restrict the training procedure from lowering the empirical loss, with conflicted descent directions (e.g., derivatives). Following the view, we propose a novel strategy making regularization-based Deep Transfer learning Never Hurt (DTNH) that, for each iteration of training procedure, computes the derivatives of the two terms separately, then re-estimates a new descent direction that does not hurt the empirical loss minimization while preserving the regularization affects from the pre-trained weights. Extensive experiments have been done using common transfer learning regularizers, such as L2-SP and knowledge distillation, on top of a wide range of deep transfer learning benchmarks including Caltech, MIT indoor 67, CIFAR-10 and ImageNet. The empirical results show that the proposed descent direction estimation strategy DTNH can always improve the performance of deep transfer learning tasks based on all above regularizers, even when transferring pre-trained weights from inappropriate networks. All in all, DTNH strategy can improve <font color="red">state-of-the-art</font> regularizers in all cases with 0.1%--7% higher accuracy in all experiments. </br></br>

<a href='http://arxiv.org/pdf/1911.08017.pdf'>1911.08017</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.8145баллов, №163</br>
<b>Implicit Generative Modeling for Efficient Exploration</b></br>
Authors: , Ratzlaff, Neale, Bai, Qinxun, Fuxin, Li, Xu, Wei</br>
  Efficient exploration remains a challenging problem in <font color="#00be00">reinforcement learning</font>, especially for those tasks where rewards from environments are sparse. A commonly used approach for exploring such environments is to introduce some &quot;intrinsic&quot; reward. In this work, we focus on model uncertainty estimation as an intrinsic reward for efficient exploration. In particular, we introduce an implicit generative modeling approach to estimate a <font color="blue">Bayes</font>ian uncertainty of the agent\'s belief of the environment dynamics. Each random draw from our generative model is a neural network that instantiates the dynamic function, hence multiple draws would approximate the posterior, and the variance in the future prediction based on this posterior is used as an intrinsic reward for exploration. We design a training algorithm for our generative model based on the amortized Stein Variational Gradient Descent. In experiments, we compare our implementation with <font color="red">state-of-the-art</font> intrinsic reward-based exploration approaches, including two recent approaches based on an ensemble of dynamic models. In challenging exploration tasks, our implicit generative model consistently <font color="#00be00">outperform</font>s competing approaches regarding data efficiency in exploration. </br></br>

<a href='http://arxiv.org/pdf/1911.07427.pdf'>1911.07427</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.8120баллов, №164</br>
<b>RotationOut as a Regularization Method for Neural Network</b></br>
Authors: , Hu, Kai, Poczos, Barnabas</br>
  In this paper, we propose a novel regularization method, RotationOut, for neural networks. Different from Dropout that handles each neuron/channel independently, RotationOut regards its input layer as an entire vector and introduces regularization by randomly rotating the vector. RotationOut can also be used in convolutional layers and recurrent layers with small modifications. We further use a noise analysis method to <font color="#be00be">interpret</font> the difference between RotationOut and Dropout in co-adaptation reduction. Using this method, we also show how to use RotationOut/Dropout together with Batch Normalization. Extensive experiments in vision and language tasks are conducted to show the effectiveness of the proposed method. Codes are available at \\url{<font color="#006400">http</font>s://<font color="red">github</font>.com/RotationOut/RotationOut}. </br></br>

<a href='http://arxiv.org/pdf/1911.08630.pdf'>1911.08630</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.8106баллов, №165</br>
<b>CUP: Cluster Pruning for Compressing Deep Neural Networks</b></br>
Authors: , Duggal, Rahul, Xiao, Cao, Vuduc, Richard, Sun, Jimeng</br>
  We propose Cluster Pruning (CUP) for compressing and accelerating deep neural networks. Our approach prunes similar filters by <font color="#be00be">clustering</font> them based on features derived from both the incoming and outgoing weight connections. With CUP, we overcome two limitations of prior work-(1) non-uniform pruning: CUP can efficiently determine the ideal number of filters to prune in each layer of a neural network. This is in contrast to prior methods that either prune all layers uniformly or otherwise use resource-intensive methods such as manual sensitivity analysis or <font color="#00be00">reinforcement learning</font> to determine the ideal number. (2) Single-shot operation: We extend CUP to CUP-SS (for CUP single shot) whereby pruning is integrated into the initial training phase itself. This leads to large savings in training time compared to traditional pruning pipelines. Through extensive evaluation on multiple datasets (MNIST, CIFAR-10, and Imagenet) and models(VGG-16, Resnets-18/34/56) we show that CUP <font color="#00be00">outperform</font>s recent <font color="red">state of the art</font>. Specifically, CUP-SS achieves 2.2x flops reduction for a Resnet-50 model trained on Imagenet while staying within 0.9% top-5 accuracy. It saves over 14 hours in training time with respect to the original Resnet-50. The code to reproduce results is available. </br></br>

<a href='http://arxiv.org/pdf/1911.07375.pdf'>1911.07375</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 0.8084баллов, №166</br>
<b>Top-down induction of decision trees: rigorous guarantees and inherent\n  limitations</b></br>
Authors: , Blanc, Guy, Lange, Jane, Tan, Li-Yang</br>
  Consider the following heuristic for building a decision tree for a function $f : \\{0,1\\}^n \\to \\{\\pm 1\\}$. Place the most influential variable $x_i$ of $f$ at the root, and recurse on the subfunctions $f_{x_i=0}$ and $f_{x_i=1}$ on the left and right subtrees respectively; terminate once the tree is an $\\varepsilon$-approximation of $f$. We analyze the quality of this heuristic, obtaining near-matching upper and lower bounds:   $\\circ$ Upper bound: For every $f$ with decision tree size $s$ and every $\\varepsilon \\in (0,\\frac1{2})$, this heuristic builds a decision tree of size at most $s^{O(\\log(s/\\varepsilon)\\log(1/\\varepsilon))}$.   $\\circ$ Lower bound: For every $\\varepsilon \\in (0,\\frac1{2})$ and $s \\le 2^{\\tilde{O}(\\sqrt{n})}$, there is an $f$ with decision tree size $s$ such that this heuristic builds a decision tree of size $s^{\\tilde{\\Omega}(\\log s)}$.   We also obtain upper and lower bounds for monotone functions: $s^{O(\\sqrt{\\log s}/\\varepsilon)}$ and $s^{\\tilde{\\Omega}(\\sqrt[4]{\\log s } )}$ respectively. The lower bound disproves conjectures of Fiat and Pechyony (2004) and Lee (2009).   Our upper bounds yield new algorithms for properly learning decision trees under the uniform distribution. We show that these algorithms---which are motivated by widely employed and empirically successful top-down decision tree learning heuristics such as ID3, C4.5, and CART---achieve provable guarantees that compare favorably with those of the current fastest algorithm (Ehrenfeucht and Haussler, 1989). Our lower bounds shed new light on the limitations of these heuristics.   Finally, we revisit the classic work of Ehrenfeucht and Haussler. We extend it to give the first uniform-distribution proper learning algorithm that achieves polynomial sample and memory complexity, while matching its <font color="red">state-of-the-art</font> quasipolynomial runtime. </br></br>

<a href='http://arxiv.org/pdf/1911.07527.pdf'>1911.07527</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.8083баллов, №167</br>
<b>SOGNet: Scene Overlap Graph Network for Panoptic Segmentation</b></br>
Authors: , Yang, Yibo, Li, Hongyang, Li, Xia, Zhao, Qijie, Wu, Jianlong, Lin, Zhouchen</br>
  The panoptic <font color="#be00be">segmentation</font> task requires a unified result from semantic and instance segmentation outputs that may contain overlaps. However, current studies widely ignore modeling overlaps. In this study, we aim to model overlap relations among instances and resolve them for panoptic segmentation. Inspired by scene graph representation, we formulate the overlapping problem as a simplified case, named scene overlap graph. We leverage each object\'s category, geometry and appearance features to perform relational embedding, and output a relation matrix that encodes overlap relations. In order to overcome the lack of supervision, we introduce a differentiable module to resolve the overlap between any pair of instances. The mask logits after removing overlaps are fed into per-pixel instance \\verb|id| classification, which leverages the panoptic supervision to assist in the modeling of overlap relations. Besides, we generate an approximate ground truth of overlap relations as the weak supervision, to quantify the accuracy of overlap relations predicted by our method. Experiments on COCO and Cityscapes demonstrate that our method is able to accurately predict overlap relations, and <font color="#00be00">outperform</font> the <font color="red">state-of-the-art</font> performance for panoptic segmentation. Our method also won the Innovation Award in COCO 2019 challenge. </br></br>

<a href='http://arxiv.org/pdf/1911.07160.pdf'>1911.07160</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.8018баллов, №168</br>
<b>Weakly Supervised Object Localization with Inter-Intra Regulated CAMs</b></br>
Authors: , Cui, Guofeng, Kou, Ziyi, Wang, Shaojie, Zhao, Wentian, Xu, Chenliang</br>
  Weakly supervised object localization (WSOL) aims to locate objects in images by learning only from image-level labels. Current methods are trying to obtain localization results relying on Class Activation Maps (CAMs). Usually, they propose additional CAMs or feature maps generated from internal layers of deep networks to encourage different CAMs to be either \\textbf{adversarial} or \\textbf{cooperated} with each other. In this work, instead of following one of the two main approaches before, we analyze their internal relationship and propose a novel intra-sample strategy which regulates two CAMs of the same sample, generated from different classifiers, to dynamically adapt each of their pixels involved in adversarial or cooperative process based on their own values. We mathematically demonstrate that our approach is a more general version of the current <font color="red">state-of-the-art</font> method with less hyper-parameters. Besides, we further develop an inter-sample criterion module for our WSOL task, which is originally proposed in co-<font color="#be00be">segmentation</font> problems, to refine generated CAMs of each sample. The module considers a subgroup of samples under the same category and regulates their object regions. With experiment on two widely-used datasets, we show that our proposed method significantly <font color="#00be00">outperform</font>s existing state-of-the-art, setting a new record for weakly-supervised object localization. </br></br>

<a href='http://arxiv.org/pdf/1911.07538.pdf'>1911.07538</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.8006баллов, №169</br>
<b>Finding Missing Children: Aging Deep Face Features</b></br>
Authors: , Deb, Debayan, Aggarwal, Divyansh, Jain, Anil K.</br>
  Given a gallery of<font color="#be00be"> face </font>images of missing children, <font color="red">state-of-the-art</font> face recognition systems fall short in identifying a child (probe) recovered at a later age. We propose an age-progression module that can age-progress deep face features output by any commodity face matcher. For time lapses larger than 10 years (the missing child is found after 10 or more years), the proposed age-progression module improves the closed-set identification accuracy of FaceNet from 40% to 49.56% and CosFace from 56.88% to 61.25% on a child celebrity dataset, namely ITWCC. The proposed method also <font color="#00be00">outperform</font>s state-of-the-art approaches with a rank-1 identification rate from 94.91% to 95.91% on a public aging dataset, FG-NET, and from 99.50% to 99.58% on CACD-VS. These results suggest that aging face features enhances the ability to identify young children who are possible victims of child trafficking or abduction. </br></br>

<a href='http://arxiv.org/pdf/1911.04053.pdf'>1911.04053</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 0.7947баллов, №170</br>
<b>Decompressing Knowledge Graph Representations for Link Prediction</b></br>
Authors: , Kong, Xiang, Chen, Xianyang, Hovy, Eduard</br>
  This paper studies the problem of predicting missing relationships between entities in <font color="#960096">knowledge graph</font>s through learning their representations. Currently, the majority of existing link prediction models employ simple but intuitive scoring functions and relatively small embedding size so that they could be applied to large-scale knowledge graphs. However, these properties also restrict the ability to learn more expressive and robust features. Therefore, diverging from most of the prior works which focus on designing new objective functions, we propose, DeCom, a simple but effective mechanism to boost the performance of existing link predictors such as DistMult, ComplEx, etc, through extracting more expressive features while preventing overfitting by adding just a few extra parameters. Specifically, embeddings of entities and relationships are first decompressed to a more expressive and robust space by decompressing functions, then knowledge graph embedding models are trained in this new feature space. Experimental results on several benchmark knowledge graphs and advanced link prediction systems demonstrate the generalization and effectiveness of our method. Especially, RESCAL + DeCom achieves <font color="red">state-of-the-art</font> performance on the FB15k-237 benchmark across all evaluation metrics. In addition, we also show that compared with DeCom, explicitly increasing the embedding size significantly increase the number of parameters but could not achieve promising performance improvement. </br></br>

<a href='http://arxiv.org/pdf/1911.08724.pdf'>1911.08724</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.7923баллов, №171</br>
<b>Fast and Flexible Image Blind Denoising via Competition of Experts</b></br>
Authors: , Maeda, Shunta</br>
  Fast and flexible processing are two essential requirements for a number of practical applications of <font color="#be00be">image denoising</font>. Current <font color="red">state-of-the-art</font> methods, however, still require either high computational cost or limited scopes of the target. We introduce an efficient ensemble network trained via a competition of expert networks, as an application for image blind denoising. We realize automatic division of unlabeled noisy datasets into clusters respectively optimized to enhance denoising performance. The architecture is scalable, can be extended to deal with diverse noise sources/levels without increasing the computation time. Taking advantage of this method, we save up to approximately 90% of computational cost without sacrifice of the denoising performance compared to single network models with identical architectures. We also compare the proposed method with several existing algorithms and observe significant <font color="#00be00">outperform</font>ance over prior arts in terms of computational efficiency. </br></br>

<a href='http://arxiv.org/pdf/1911.07844.pdf'>1911.07844</a> &nbsp&nbsp (cs:CV, cs:ML, stat:ML) &nbsp&nbsp 0.7884баллов, №172</br>
<b>Exploiting Human Social Cognition for the Detection of Fake and\n  Fraudulent Faces via Memory Networks</b></br>
Authors: , Fernando, Tharindu, Fookes, Clinton, Denman, Simon, Sridharan, Sridha</br>
  Advances in computer vision have brought us to the point where we have the ability to synthesise realistic fake content. Such approaches are seen as a source of disinformation and mistrust, and pose serious concerns to governments around the world. Convolutional Neural Networks (CNNs) demonstrate encouraging results when detecting fake images that arise from the specific type of manipulation they are trained on. However, this success has not transitioned to unseen manipulation types, resulting in a significant gap in the line-of-defense. We propose a <font color="#00be00">Hierarchical</font> Memory Network (HMN) architecture, which is able to successfully detect faked faces by utilising knowledge stored in neural memories as well as visual cues to reason about the perceived<font color="#be00be"> face </font>and anticipate its future semantic embeddings. This renders a generalisable face tampering detection framework. Experimental results demonstrate the proposed approach achieves superior performance for fake and fraudulent face detection compared to the <font color="red">state-of-the-art</font>. </br></br>

<a href='http://arxiv.org/pdf/1911.07750.pdf'>1911.07750</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp 0.7741баллов, №173</br>
<b>Beyond the Grounding Bottleneck: Datalog Techniques for Inference in\n  Probabilistic Logic Programs (Technical Report)</b></br>
Authors: , Tsamoura, Efthymia, Gutierrez-Basulto, Victor, Kimmig, Angelika</br>
  <font color="red">State-of-the-art</font> inference approaches in probabilistic logic programming typically start by computing the relevant ground program with respect to the queries of interest, and then use this program for probabilistic inference using knowledge compilation and weighted model counting. We propose an alternative approach that uses efficient Datalog techniques to integrate knowledge compilation with forward reasoning with a non-ground program. This effectively eliminates the grounding bottleneck that so far has prohibited the application of probabilistic logic programming in query answering scenarios over <font color="#960096">knowledge graph</font>s, while also providing fast approximations on classical benchmarks in the field. </br></br>

<a href='http://arxiv.org/pdf/1911.07347.pdf'>1911.07347</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.7739баллов, №174</br>
<b>Fast 3D Pose Refinement with RGB Images</b></br>
Authors: , Jain, Abhinav, Dellaert, Frank</br>
  Pose estimation is a vital step in many robotics and perception tasks such as robotic manipulation, autonomous vehicle navigation, etc. Current <font color="red">state-of-the-art</font> pose estimation methods rely on deep neural networks with complicated structures and long inference times. While highly robust, they require computing power often unavailable on<font color="#960096"> mobile </font>robots. We propose a CNN-based pose refinement system which takes a coarsely estimated 3D pose from a computationally cheaper algorithm along with a bounding box image of the object, and returns a highly refined pose. Our experiments on the YCB-Video dataset show that our system can refine 3D poses to an extremely high precision with minimal training data. </br></br>

<a href='http://arxiv.org/pdf/1911.07432.pdf'>1911.07432</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp 0.7709баллов, №175</br>
<b>Fast 2D Map Matching Based on Area Graphs</b></br>
Authors: , Hou, Jiawei, Kuang, Haofei, Schwertfeger, S&#xf6;ren</br>
  We present a novel area matching algorithm for merging two different 2D grid maps. There are many approaches to address this problem, nevertheless, most previous work is built on some assumptions, such as rigid transformation, or similar scale and modalities of two maps. In this work we propose a 2D map matching algorithm based on area <font color="#be00be">segmentation</font>. We transfer general 2D occupancy grid maps to an area graph representation, then compute the correct results by voting in that space. In the experiments, we compare with a <font color="red">state-of-the-art</font> method applied to the matching of sensor maps with ground truth layout maps. The experiment shows that our algorithm has a better performance on large-scale maps and a faster computation speed. </br></br>

<a href='http://arxiv.org/pdf/1911.02559.pdf'>1911.02559</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.7673баллов, №176</br>
<b>SCL: Towards Accurate Domain Adaptive Object Detection via Gradient\n  Detach Based Stacked Complementary Losses</b></br>
Authors: , Shen, Zhiqiang, Maheshwari, Harsh, Yao, Weichen, Savvides, Marios</br>
  Unsupervised domain adaptive <font color="#be00be">object detection</font> aims to learn a robust detector in the domain shift circumstance, where the training (source) domain is label-rich with bounding box annotations, while the testing (target) domain is label-agnostic and the feature distributions between training and testing domains are dissimilar or even totally different. In this paper, we propose a gradient detach based stacked complementary losses (SCL) method that uses detection losses as the primary objective, and cuts in several auxiliary losses in different network stages accompanying with gradient detach training to learn more discriminative representations. We argue that the prior methods mainly leverage more loss functions for training but ignore the interaction of different losses and also the compatible training strategy (gradient detach updating in our work). Thus, our proposed method is a more syncretic adaptation learning process. We conduct comprehensive experiments on seven datasets, the results demonstrate that our method performs favorably better than the <font color="red">state-of-the-art</font> methods by a significant margin. For instance, from Cityscapes to FoggyCityscapes, we achieve 37.9% mAP, <font color="#00be00">outperform</font>ing the previous art Strong-Weak by 3.6%. </br></br>

<a href='http://arxiv.org/pdf/1911.07104.pdf'>1911.07104</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.7652баллов, №177</br>
<b>RSM-GAN: A Convolutional Recurrent GAN for Anomaly Detection in\n  Contaminated Seasonal Multivariate Time Series</b></br>
Authors: , Khoshnevisan, Farzaneh, Fan, Zhewen</br>
  Robust <font color="#be00be">anomal</font>y detection is a requirement for monitoring complex modern systems with applications such as cyber-security, fraud prevention, and maintenance. These systems generate multiple correlated time series that are highly seasonal and noisy. This paper presents a novel unsupervised deep learning architecture for multivariate time series anomaly detection, called Robust Seasonal Multivariate Generative Adversarial Network (RSM-GAN). It extends recent advancements in GANs with adoption of convolutional-LSTM layers and an attention mechanism to produce <font color="red">state-of-the-art</font> performance. We conduct extensive experiments to demonstrate the strength of our architecture in adjusting for complex seasonality patterns and handling severe levels of training data contamination. We also propose a novel anomaly score assignment and causal inference framework. We compare RSM-GAN with existing classical and deep-learning based anomaly detection models, and the results show that our architecture is associated with the lowest false positive rate and improves precision by 30% and 16% in <font color="#009600">real-world</font> and synthetic data, respectively. Furthermore, we report the superiority of RSM-GAN regarding accurate root cause identification and NAB scores in all data settings. </br></br>

<a href='http://arxiv.org/pdf/1911.08623.pdf'>1911.08623</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.7544баллов, №178</br>
<b>Deep Anomaly Detection with Deviation Networks</b></br>
Authors: , Pang, Guansong, Shen, Chunhua, Hengel, Anton van den</br>
  Although deep learning has been applied to successfully address many data mining problems, relatively limited work has been done on deep learning for <font color="#be00be">anomal</font>y detection. Existing deep anomaly detection methods, which focus on learning new feature representations to enable downstream anomaly detection methods, perform indirect optimization of anomaly scores, leading to data-inefficient learning and suboptimal anomaly scoring. Also, they are typically designed as unsupervised learning due to the lack of large-scale labeled anomaly data. As a result, they are difficult to leverage prior knowledge (e.g., a few labeled anomalies) when such information is available as in many <font color="#009600">real-world</font> anomaly detection applications.   This paper introduces a novel anomaly detection framework and its instantiation to address these problems. Instead of representation learning, our method fulfills an end-to-end learning of anomaly scores by a neural deviation learning, in which we leverage a few (e.g., multiple to dozens) labeled anomalies and a prior probability to enforce statistically significant deviations of the anomaly scores of anomalies from that of normal data objects in the upper tail. Extensive results show that our method can be trained substantially more data-efficiently and achieves significantly better anomaly scoring than <font color="red">state-of-the-art</font> competing methods. </br></br>

<a href='http://arxiv.org/pdf/1911.07673.pdf'>1911.07673</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp 0.7524баллов, №179</br>
<b>Using Mapping Languages for Building Legal Knowledge Graphs from XML\n  Files</b></br>
Authors: , Junior, Ademar Crotti, Orlandi, Fabrizio, O\'Sullivan, Declan, Dirschl, Christian, Reul, Quentin</br>
  This paper presents our experience on building RDF <font color="#960096">knowledge graph</font>s for an industrial use case in the legal domain. The information contained in legal information systems are often accessed through simple keyword interfaces and presented as a simple list of hits. In order to improve search accuracy one may avail of knowledge graphs, where the semantics of the data can be made explicit. Significant research effort has been invested in the area of building knowledge graphs from semi-structured text documents, such as XML, with the prevailing approach being the use of mapping languages. In this paper, we present a semantic model for representing legal documents together with an industrial use case. We also present a set of use case requirements based on the proposed semantic model, which are used to compare and discuss the use of <font color="red">state-of-the-art</font> mapping languages for building knowledge graphs for legal data. </br></br>

<a href='http://arxiv.org/pdf/1911.08342.pdf'>1911.08342</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp 0.7384баллов, №180</br>
<b>Knowledge Graph Entity Alignment with Graph Convolutional Networks:\n  Lessons Learned</b></br>
Authors: , Berrendorf, Max, Faerman, Evgeniy, Melnychuk, Valentyn, Tresp, Volker, Seidl, Thomas</br>
  In this work, we focus on the problem of entity alignment in <font color="#960096">Knowledge Graph</font>s (KG) and we report on our experiences when applying a Graph Convolutional Network (GCN) based model for this task. Variants of GCN are used in multiple <font color="red">state-of-the-art</font> approaches and therefore it is important to understand the specifics and limitations of GCN-based models. Despite serious efforts, we were not able to fully reproduce the results from the original paper and after a thorough audit of the code provided by authors, we concluded, that their implementation is different from the architecture described in the paper. In addition, several tricks are required to make the model work and some of them are not very intuitive. We provide an extensive ablation study to quantify the effects these tricks and changes of architecture have on final performance. Furthermore, we examine current evaluation approaches and systematize available benchmark datasets. We believe that people interested in KG matching might profit from our work, as well as novices entering the field </br></br>

<a href='http://arxiv.org/pdf/1910.04875.pdf'>1910.04875</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.7381баллов, №181</br>
<b>FastEstimator: A Deep Learning Library for Fast Prototyping and\n  Productization</b></br>
Authors: , Dong, Xiaomeng, Hong, Junpyo, Chang, Hsi-Ming, Potter, Michael, Chowdhury, Aritra, Bahl, Purujit, Soni, Vivek, Tsai, Yun-Chan, Tamada, Rajesh, Kumar, Gaurav, Favart, Caroline, Saripalli, V. Ratna, Avinash, Gopal</br>
  As the complexity of <font color="red">state-of-the-art</font> deep learning models increases by the month, implementation, <font color="#be00be">interpret</font>ation, and traceability become ever-more-burdensome challenges for AI practitioners around the world. Several AI frameworks have risen in an effort to stem this tide, but the steady advance of the field has begun to test the bounds of their flexibility, expressiveness, and ease of use. To address these concerns, we introduce a radically flexible high-level open source deep learning framework for both research and industry. We introduce FastEstimator. </br></br>

<a href='http://arxiv.org/pdf/1911.08698.pdf'>1911.08698</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 0.7315баллов, №182</br>
<b>EmpGAN: Multi-resolution Interactive Empathetic Dialogue Generation</b></br>
Authors: , Li, Qintong, Chen, Hongshen, Ren, Zhaochun, Chen, Zhumin, Tu, Zhaopeng, Ma, Jun</br>
  Conventional <font color="#be00be">emotion</font>al dialogue system focuses on generating emotion-rich replies. Studies on emotional intelligence suggest that constructing a more empathetic dialogue system, which is sensitive to the users\' expressed emotion, is a crucial step towards a more humanized human-machine conversation. However, obstacles to establishing such an empathetic conversational system are still far beyond current progress: 1) Simply considering the sentence-level emotions while neglecting the more precise token-level emotions may lead to insufficient emotion perceptivity. 2) Merely relying on the dialogue history but overlooking the potential of user feedback for the generated responses further aggravates the insufficient emotion perceptivity deficiencies.   To address the above challenges, we propose the EmpGAN, a multi-resolution adversarial empathetic dialogue generation model to generate more appropriate and empathetic responses. To capture the nuances of user feelings sufficiently, EmpGAN generates responses by jointly taking both the coarse-grained sentence-level and fine-grained token-level emotions into account. Moreover, an interactive adversarial learning framework is introduced to further identify whether the generated responses evoke emotion perceptivity in dialogues regarding both the dialogue history and user feedback. Experiments show that our model <font color="#00be00">outperform</font>s the <font color="red">state-of-the-art</font> baseline by a significant margin in terms of both content quality as well as the emotion perceptivity. In particular, the distinctiveness on the DailyDialog dataset is increased up to 129%. </br></br>

<a href='http://arxiv.org/pdf/1910.12019.pdf'>1910.12019</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.7312баллов, №183</br>
<b>Describing Video with Multiple Descriptions</b></br>
Authors: , Xiao, Huanhou, Shi, Jinglun</br>
  Automatically describing video content with text description is challenging but important task, which has been attracting a lot of attention in CV community. Previous works mainly strive for the accuracy of the generated sentences, while ignoring the sentences diversity, which is inconsistent with human behavior. In this paper, we aim to caption each video with multiple descriptions and propose a novel framework. Concretely, for a given video, the intermediate latent variables of conventional encode-decode process are utilized as input to the conditional generative adversarial network (CGAN) with the purpose of generating diverse sentences. We adopt the combination of LSTM and CNN as our generator that produces descriptions conditioned on latent variables and the CNNs as discriminator that assesses the quality of generated sentences. Simultaneously, a novel DCE metric is designed to assess the diverse captions. We evaluate our method on the benchmark datasets, where it demonstrates its ability to generate diverse descriptions and achieves <font color="#960096">competitive</font> or even superior results against other <font color="red">state-of-the-art</font> methods. </br></br>

<a href='http://arxiv.org/pdf/1911.07320.pdf'>1911.07320</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.7304баллов, №184</br>
<b>Sparse $\\ell_1$ and $\\ell_2$ Center Classifiers</b></br>
Authors: , Calafiore, Giuseppe C., Fracastoro, Giulia</br>
  The nearest-centroid classifier is a simple linear-time classifier based on computing the centroids of the data classes in the training phase, and then assigning a new datum to the class corresponding to its nearest centroid. Thanks to its very low computational cost, the nearest-centroid classifier is still widely used in machine learning, despite the development of many other more sophisticated classification methods. In this paper, we propose two sparse variants of the nearest-centroid classifier, based respectively on $\\ell_1$ and $\\ell_2$ distance criteria. The proposed sparse classifiers perform simultaneous classification and feature selection, by detecting the features that are most relevant for the classification purpose. We show that training of the proposed sparse models, with both distance criteria, can be performed exactly (i.e., the globally optimal set of features is selected) and at a quasi-linear computational cost. The experimental results show that the proposed methods are <font color="#960096">competitive</font> in accuracy with <font color="red">state-of-the-art</font> feature selection techniques, while having a significantly lower computational cost. </br></br>

<a href='http://arxiv.org/pdf/1911.08077.pdf'>1911.08077</a> &nbsp&nbsp (cs:ML, cs:SD) &nbsp&nbsp 0.7303баллов, №185</br>
<b>Partial AUC optimization based deep speaker embeddings with class-center\n  learning for text-independent speaker verification</b></br>
Authors: , Bai, Zhongxin, Zhang, Xiao-Lei, Chen, Jingdong</br>
  Deep embedding based text-independent speaker verification has demonstrated superior performance to traditional methods in many challenging scenarios. Its loss functions can be generally categorized into two classes, i.e., verification and identification. The verification loss functions match the pipeline of speaker verification, but their implementations are difficult. Thus, most <font color="red">state-of-the-art</font> deep embedding methods use the identification loss functions with softmax output units or their variants. In this paper, we propose a verification loss function, named the maximization of partial area under the Receiver-operating-characteristic (ROC) curve (pAUC), for deep embedding based text-independent speaker verification. We also propose a class-center based training trial construction method to improve the training efficiency, which is critical for the proposed loss function to be comparable to the identification loss in performance. Experiments on the Speaker in the Wild (SITW) and NIST SRE 2016 datasets show that the proposed pAUC loss function is highly <font color="#960096">competitive</font> with the state-of-the-art identification loss functions. </br></br>

<a href='http://arxiv.org/pdf/1911.08621.pdf'>1911.08621</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.7249баллов, №186</br>
<b>Open Cross-Domain Visual Search</b></br>
Authors: , Thong, William, Mettes, Pascal, Snoek, Cees G. M.</br>
  This paper introduces open cross-domain visual search, where categories in any target domain are retrieved based on queries from any source domain. Current works usually tackle cross-domain visual search as a domain adaptation problem. This limits the search to a closed setting, with one fixed source domain and one fixed target domain. To make the step towards an open setting where multiple visual domains are available, we introduce a simple yet effective approach. We formulate the search as one of mapping examples from every visual domain to a common semantic space, where categories are represented by hyperspherical prototypes. Cross-domain search is then performed by searching in the common space, regardless of which domains are used as source or target. Having separate mappings for every domain allows us to search in an open setting, and to incrementally add new domains over time without retraining existing mapping functions. Experimentally, we show our capability to perform open cross-domain visual search. Our approach is <font color="#960096">competitive</font> with respect to traditional closed settings, where we obtain <font color="red">state-of-the-art</font> results on six benchmarks for three sketch-based search tasks. </br></br>

<a href='http://arxiv.org/pdf/1911.08691.pdf'>1911.08691</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.7191баллов, №187</br>
<b>DRNet: Dissect and Reconstruct the Convolutional Neural Network via\n  Interpretable Manners</b></br>
Authors: , Hu, Xiaolong, An, Zhulin, Yang, Chuanguang, Zhu, Hui, Xu, Kaiqaing, Xu, Yongjun</br>
  This paper proposes to use an <font color="#be00be">interpret</font>able method to dissect the channels of a large-scale convolutional neural networks (CNNs) into class-wise parts, and reconstruct a CNN using some of these parts. The dissection and reconstruction process can be done in very short time on <font color="red">state-of-the-art</font> networks such as VGG and MobileNetV2. This method allows users to run parts of a CNN according to specific application scenarios, instead of running the whole network or retraining a new one for every task. Experiments on Cifar and ILSVRC 2012 show that the reconstructed CNN runs more efficiently than the original one and achieve a better accuracy. Interpretability analyses show that our method is a new way of applying CNNs on tasks with given knowledge. </br></br>

<a href='http://arxiv.org/pdf/1911.07874.pdf'>1911.07874</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.7174баллов, №188</br>
<b>RWNE: A Scalable Random-Walk based Network Embedding Framework with\n  Personalized Higher-order Proximity Preserved</b></br>
Authors: , He, Yu, Li, Jianxin, Song, Yangqiu, Zhang, Xinmiao, Peng, Fanzhang, Peng, Hao</br>
  Higher-order proximity preserved network embedding has attracted increasing attention recently. In particular, due to the superior scalability, random-walk based network embedding has also been well developed, which could efficiently explore higher-order neighborhood via multi-hop random walks. However, despite the success of current random-walk based methods, most of them are usually not expressive enough to preserve the personalized higher-order proximity and lack a straightforward objective to <font color="blue">theor</font>etically articulate what and how network proximity is preserved. In this paper, to address the above issues, we present a general scalable random-walk based network embedding framework, in which random walk is explicitly incorporated into a sound objective designed theoretically to preserve arbitrary higher-order proximity. Further, we introduce the random walk with restart process into the framework to naturally and effectively achieve personalized-weighted preservation of proximities of different orders. We conduct extensive experiments on several <font color="#009600">real-world</font> networks and demonstrate that our proposed method consistently and substantially <font color="#00be00">outperform</font>s the <font color="red">state-of-the-art</font> network embedding methods. </br></br>

<a href='http://arxiv.org/pdf/1911.06971.pdf'>1911.06971</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.7137баллов, №189</br>
<b>BSP-Net: Generating Compact Meshes via Binary Space Partitioning</b></br>
Authors: , Chen, Zhiqin, Tagliasacchi, Andrea, Zhang, Hao</br>
  Polygonal meshes are ubiquitous in the digital 3D domain, yet they have only played a minor role in the deep learning revolution. Leading methods for learning generative models of shapes rely on implicit functions, and generate meshes only after expensive iso-surfacing routines. To overcome these challenges, we are inspired by a classical spatial data structure from computer graphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core ingredient of BSP is an operation for recursive subdivision of space to obtain convex sets. By exploiting this property, we devise BSP-Net, a network that learns to represent a 3D shape via convex decomposition. Importantly, BSP-Net is unsupervised since no convex shape decompositions are needed for training. The network is trained to reconstruct a shape using a set of convexes obtained from a BSP-tree built on a set of planes. The convexes inferred by BSP-Net can be easily extracted to form a polygon mesh, without any need for iso-surfacing. The generated meshes are compact (i.e., low-poly) and well suited to represent sharp geometry; they are guaranteed to be watertight and can be easily parameterized. We also show that the reconstruction quality by BSP-Net is <font color="#960096">competitive</font> with <font color="red">state-of-the-art</font> methods while using much fewer primitives. </br></br>

<a href='http://arxiv.org/pdf/1911.07928.pdf'>1911.07928</a> &nbsp&nbsp (cs:CV, cs:AI, cs:ML) &nbsp&nbsp 0.7073баллов, №190</br>
<b>Visual Dialogue State Tracking for Question Generation</b></br>
Authors: , Pang, Wei, Wang, Xiaojie</br>
  GuessWhat?! is a visual dialogue task between a guesser and an oracle. The guesser aims to locate an object supposed by the oracle oneself in an image by asking a sequence of Yes/No questions. Asking proper questions with the progress of dialogue is vital for achieving successful final guess. As a result, the progress of dialogue should be properly represented and tracked. Previous models for question generation pay less attention on the representation and <font color="#be00be">tracking</font> of dialogue states, and therefore are prone to asking low quality questions such as repeated questions. This paper proposes visual dialogue state tracking (VDST) based method for question generation. A visual dialogue state is defined as the distribution on objects in the image as well as representations of objects. Representations of objects are updated with the change of the distribution on objects. An object-difference based attention is used to decode new question. The distribution on objects is updated by comparing the question-answer pair and objects. Experimental results on GuessWhat?! dataset show that our model significantly <font color="#00be00">outperform</font>s existing methods and achieves new <font color="red">state-of-the-art</font> performance. It is also noticeable that our model reduces the rate of repeated questions from more than 50% to 18.85% compared with previous state-of-the-art methods. </br></br>

<a href='http://arxiv.org/pdf/1911.07478.pdf'>1911.07478</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.7070баллов, №191</br>
<b>Fine-Grained Neural Architecture Search</b></br>
Authors: , Kim, Heewon, Hong, Seokil, Han, Bohyung, Myeong, Heesoo, Lee, Kyoung Mu</br>
  We present an elegant framework of fine-grained neural <font color="#00be00">architecture search</font> (FGNAS), which allows to employ multiple heterogeneous operations within a single layer and can even generate compositional feature maps using several different base operations. FGNAS runs efficiently in spite of significantly large search space compared to other methods because it trains networks end-to-end by a stochastic gradient descent method. Moreover, the proposed framework allows to optimize the network under predefined resource constraints in terms of number of parameters, FLOPs and latency. FGNAS has been applied to two crucial applications in resource demanding computer vision tasks---large-scale image classification and image <font color="#be00be">super-resolution</font>---and demonstrates the <font color="red">state-of-the-art</font> performance through flexible operation search and channel pruning. </br></br>

<a href='http://arxiv.org/pdf/1911.09516.pdf'>1911.09516</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.7065баллов, №192</br>
<b>Learning Spatial Fusion for Single-Shot Object Detection</b></br>
Authors: , Liu, Songtao, Huang, Di, Wang, Yunhong</br>
  Pyramidal feature representation is the common practice to address the challenge of scale variation in <font color="#be00be">object detection</font>. However, the inconsistency across different feature scales is a primary limitation for the single-shot detectors based on feature pyramid. In this work, we propose a novel and data driven strategy for pyramidal feature fusion, referred to as adaptively spatial feature fusion (ASFF). It learns the way to spatially filter conflictive information to suppress the inconsistency, thus improving the scale-invariance of features, and introduces nearly free inference overhead. With the ASFF strategy and a solid baseline of YOLOv3, we achieve the best speed-accuracy trade-off on the MS COCO dataset, reporting 38.1% AP at 60 FPS, 42.4% AP at 45 FPS and 43.9% AP at 29 FPS. The code is available at <font color="#006400">http</font>s://<font color="red">github</font>.com/ruinmessi/ASFF </br></br>

<a href='http://arxiv.org/pdf/1911.07344.pdf'>1911.07344</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.7053баллов, №193</br>
<b>ELoPE: Fine-Grained Visual Classification with Efficient Localization,\n  Pooling and Embedding</b></br>
Authors: , Hanselmann, Harald, Ney, Hermann</br>
  The task of fine-grained visual classification (FGVC) deals with classification problems that display a small inter-class variance such as distinguishing between different bird species or car models. <font color="red">State-of-the-art</font> approaches typically tackle this problem by integrating an elaborate attention mechanism or (part-) localization method into a standard convolutional neural network (CNN). Also in this work the aim is to enhance the performance of a backbone CNN such as ResNet by including three efficient and <font color="#be00be">lightweight</font> components specifically designed for FGVC. This is achieved by using global k-max pooling, a discriminative embedding layer trained by optimizing class means and an efficient bounding box estimator that only needs class labels for training. The resulting model achieves new best state-of-the-art recognition accuracies on the Stanford cars and FGVC-Aircraft datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.06918.pdf'>1911.06918</a> &nbsp&nbsp (cs:NE) &nbsp&nbsp 0.6992баллов, №194</br>
<b>Towards Design Methodology of Efficient Fast Algorithms for Accelerating\n  Generative Adversarial Networks on FPGAs</b></br>
Authors: , Chang, Jung-Woo, Ahn, Saehyun, Kang, Keon-Woo, Kang, Suk-Ju</br>
  Generative adversarial networks (GANs) have shown excellent performance in image and speech applications. GANs create impressive data primarily through a new type of operator called deconvolution (DeConv) or transposed convolution (Conv). To implement the DeConv layer in hardware, the <font color="red">state-of-the-art</font> accelerator reduces the high computational complexity via the DeConv-to-Conv conversion and achieves the same results. However, there is a problem that the number of filters increases due to this conversion. Recently, Winograd minimal filtering has been recognized as an effective solution to improve the arithmetic complexity and resource efficiency of the Conv layer. In this paper, we propose an efficient Winograd DeConv accelerator that combines these two orthogonal approaches on <font color="#be00be">FPGA</font>s. Firstly, we introduce a new class of fast algorithm for DeConv layers using Winograd minimal filtering. Since there are regular sparse patterns in Winograd filters, we further amortize the computational complexity by skipping zero weights. Secondly, we propose a new dataflow to prevent resource underutilization by reorganizing the filter layout in the Winograd domain. Finally, we propose an efficient architecture for implementing Winograd DeConv by designing the line buffer and exploring the design space. Experimental results on various GANs show that our accelerator achieves up to 1.78x~8.38x speedup over the state-of-the-art DeConv accelerators. </br></br>

<a href='http://arxiv.org/pdf/1911.07198.pdf'>1911.07198</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp 0.6844баллов, №195</br>
<b>Smoothed Inference for Adversarially-Trained Models</b></br>
Authors: , Nemcovsky, Yaniv, Zheltonozhskii, Evgenii, Baskin, Chaim, Chmiel, Brian, Bronstein, Alex M., Mendelson, Avi</br>
  Deep neural networks are known to be vulnerable to inputs with maliciously constructed adversarial perturbations aimed at forcing misclassification. We study randomized smoothing as a way to both improve performance on unperturbed data as well as increase robustness to <font color="blue">adversarial att</font>acks. Moreover, we extend the method proposed by arXiv:1811.09310 by adding low-rank multivariate noise, which we then use as a base model for smoothing. The proposed method achieves 58.5% top-1 accuracy on CIFAR-10 under PGD attack and <font color="#00be00">outperform</font>s previous works by 4%. In addition, we consider a family of attacks, which were previously used for training purposes in the certified robustness scheme. We demonstrate that the proposed attacks are more effective than PGD against both smoothed and non-smoothed models. Since our method is based on sampling, it lends itself well for trading-off between the model inference complexity and its performance. A reference implementation of the proposed techniques is provided at <font color="#006400">http</font>s://<font color="red">github</font>.com/yanemcovsky/SIAM. </br></br>

<a href='http://arxiv.org/pdf/1911.06833.pdf'>1911.06833</a> &nbsp&nbsp (cs:ML, cs:AI, cs:RO, stat:ML) &nbsp&nbsp 0.6830баллов, №196</br>
<b>Improved Exploration through Latent Trajectory Optimization in Deep\n  Deterministic Policy Gradient</b></br>
Authors: , Luck, Kevin Sebastian, Vecerik, Mel, Stepputtis, Simon, Amor, Heni Ben, Scholz, Jonathan</br>
  Model-free <font color="#00be00">reinforcement learning</font> algorithms such as Deep Deterministic Policy Gradient (DDPG) often require additional exploration strategies, especially if the actor is of deterministic nature. This work evaluates the use of model-based trajectory optimization methods used for exploration in Deep Deterministic Policy Gradient when trained on a latent image embedding. In addition, an extension of DDPG is derived using a value function as critic, making use of a learned deep dynamics model to compute the policy gradient. This approach leads to a symbiotic relationship between the deep reinforcement learning algorithm and the latent trajectory optimizer. The trajectory optimizer benefits from the critic learned by the RL algorithm and the latter from the enhanced exploration generated by the planner. The developed methods are evaluated on two continuous control tasks, one in simulation and one in the <font color="#009600">real world</font>. In particular, a Baxter robot is trained to perform an insertion task, while only receiving <font color="#00be00">sparse reward</font>s and images as observations from the environment. </br></br>

<a href='http://arxiv.org/pdf/1911.07847.pdf'>1911.07847</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.6795баллов, №197</br>
<b>Efficient Hardware Implementation of Incremental Learning and Inference\n  on Chip</b></br>
Authors: , Hacene, Ghouthi Boukli, Gripon, Vincent, Farrugia, Nicolas, Arzel, Matthieu, Jezequel, Michel</br>
  In this paper, we tackle the problem of incrementally learning a classifier, one example at a time, directly on chip. To this end, we propose an efficient hardware implementation of a recently introduced incremental learning procedure that achieves <font color="red">state-of-the-art</font> performance by combining transfer learning with majority votes and quantization techniques. The proposed design is able to accommodate for both new examples and new classes directly on the chip. We detail the hardware implementation of the method (implemented on <font color="#be00be">FPGA</font> target) and show it requires limited resources while providing a significant acceleration compared to using a CPU. </br></br>

<a href='http://arxiv.org/pdf/1910.00458.pdf'>1910.00458</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp 0.6794баллов, №198</br>
<b>MMM: Multi-stage Multi-task Learning for Multi-choice Reading\n  Comprehension</b></br>
Authors: , Jin, Di, Gao, Shuyang, Kao, Jiun-Yu, Chung, Tagyoung, Hakkani-tur, Dilek</br>
  Machine Reading Comprehension (MRC) for question answering (QA), which aims to answer a question given the relevant context passages, is an important way to test the ability of intelligence systems to understand human language. Multiple-Choice QA (MCQA) is one of the most difficult tasks in MRC because it often requires more advanced reading comprehension skills such as logical reasoning, <font color="#be00be">summarization</font>, and arithmetic operations, compared to the extractive counterpart where answers are usually spans of text within given passages. Moreover, most existing MCQA datasets are small in size, making the learning task even harder. We introduce MMM, a Multi-stage Multi-task learning framework for Multi-choice reading comprehension. Our method involves two sequential stages: coarse-tuning stage using out-of-domain datasets and multi-task learning stage using a larger in-domain dataset to help model generalize better with limited data. Furthermore, we propose a novel multi-step attention network (MAN) as the top-level classifier for this task. We demonstrate MMM significantly advances the <font color="red">state-of-the-art</font> on four representative MCQA datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.07246.pdf'>1911.07246</a> &nbsp&nbsp (cs:RO, cs:AI, cs:CV, cs:ML) &nbsp&nbsp 0.6791баллов, №199</br>
<b>IKEA Furniture Assembly Environment for Long-Horizon Complex\n  Manipulation Tasks</b></br>
Authors: , Lee, Youngwoon, Hu, Edward S., Yang, Zhengyu, Yin, Alex, Lim, Joseph J.</br>
  The IKEA Furniture Assembly Environment is one of the first benchmarks for testing and accelerating the automation of complex manipulation tasks. The environment is designed to advance <font color="#00be00">reinforcement learning</font> from simple toy tasks to complex tasks requiring both long-term planning and sophisticated low-level control. Our environment supports over 80 different furniture models, Sawyer and Baxter robot simulation, and domain randomization. The IKEA Furniture Assembly Environment is a testbed for methods aiming to solve complex manipulation tasks. The environment is <font color="#00be00">publicly available</font> at <font color="#006400">http</font>s://clvrai.com/furniture </br></br>

<a href='http://arxiv.org/pdf/1911.08299.pdf'>1911.08299</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.6776баллов, №200</br>
<b>Learning Modulated Loss for Rotated Object Detection</b></br>
Authors: , Qian, Wen, Yang, Xue, Peng, Silong, Guo, Yue, Yan, Chijun</br>
  Popular rotated detection methods usually use five parameters (coordinates of the central point, width, height, and rotation angle) to describe the rotated bounding box and l1-loss as the loss function. In this paper, we argue that the aforementioned integration can cause training instability and performance degeneration, due to the loss discontinuity resulted from the inherent periodicity of angles and the associated sudden exchange of width and height. This problem is further pronounced given the <font color="#be00be">regression</font> inconsistency among five parameters with different measurement units. We refer to the above issues as rotation sensitivity error (RSE) and propose a modulated rotation loss to dismiss the loss discontinuity. Our new loss is combined with the eight-parameter regression to further solve the problem of inconsistent parameter regression. Experiments show the state-of-art performances of our method on the public aerial image benchmark DOTA and UCAS-AOD. Its generalization abilities are also verified on ICDAR2015, HRSC2016, and FDDB. Qualitative improvements can be seen in Fig 1, and the <font color="red">source code</font> will be released with the publication of the paper. </br></br>

<a href='http://arxiv.org/pdf/1911.08183.pdf'>1911.08183</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp 0.6774баллов, №201</br>
<b>Nonlinear Model Predictive Control with Actuator Constraints for\n  Multi-Rotor Aerial Vehicles</b></br>
Authors: , Bicego, Davide, Mazzetto, Jacopo, Carli, Ruggero, Farina, Marcello, Franchi, Antonio</br>
  In this paper we propose, test, and validate an online Nonlinear Model Predictive Control (NMPC) method applied to multi-rotor aerial systems with arbitrarily positioned and oriented rotors. This work brings into question some common modeling and control design choices that are typically adopted in order to guarantee robustness and reliability but which may severely limit the attainable performance. In particular the proposed method \\emph{does not} resort to common simplifications such as: 1) linear model approximation, 2) cascaded control paradigm used to decouple the translational and the rotational dynamics of the rigid body, and 3) use of low level reactive <font color="#be00be">tracker</font>s for stabilization, 4) unconstrained system or use of fictitious constraints. The method addresses simultaneously the problem of local reference trajectory planning and that of stabilizing the vehicle dynamics. Furthermore, by considering as control inputs the derivatives of the forces generated by the multi-rotor vehicle and by means of a novel actuator modeling approach, the method avoids conservative -- and often fictitious -- input/state saturations which are present, e.g., in cascaded approaches. The control algorithm is implemented using a <font color="red">state-of-the-art</font> Real Time Iteration (RTI) scheme with partial sensitivity update method. The performances of the control system are finally validated by means of real-time simulations and in real experiments, with a large spectrum of multi-rotor systems: an \\emph{under-actuated} quadrotor, a \\emph{fully actuated} hexarotor, a multi-rotor with \\emph{orientable} propellers, and a multi-rotor with an unexpected \\emph{rotor failure}. </br></br>

<a href='http://arxiv.org/pdf/1911.07602.pdf'>1911.07602</a> &nbsp&nbsp (cs:CV, cs:ML, cs:RO) &nbsp&nbsp 0.6774баллов, №202</br>
<b>The inD Dataset: A Drone Dataset of Naturalistic Road User Trajectories\n  at German Intersections</b></br>
Authors: , Bock, Julian, Krajewski, Robert, Moers, Tobias, Runde, Steffen, Vater, Lennart, Eckstein, Lutz</br>
  Automated vehicles rely heavily on data-driven methods, especially for complex urban environments. Large datasets of <font color="#009600">real world</font> measurement data in the form of road user trajectories are crucial for several tasks like road user prediction models or scenario-based safety validation. So far, though, this demand is unmet as no public dataset of urban road user trajectories is available in an appropriate size, quality and variety. By contrast, the highway drone dataset (highD) has recently shown that drones are an efficient method for acquiring naturalistic road user trajectories. Compared to driving studies or ground-level infrastructure sensors, one major advantage of using a drone is the possibility to record naturalistic behavior, as road users do not notice measurements taking place. Due to the ideal viewing angle, an entire intersection scenario can be measured with significantly less occlusion than with sensors at ground level. Both the class and the trajectory of each road user can be extracted from the video recordings with high precision using <font color="red">state-of-the-art</font> deep neural networks. Therefore, we propose the creation of a comprehensive, large-scale urban intersection dataset with naturalistic road user behavior using camera-equipped drones as successor of the highD dataset. The resulting dataset contains more than 11500 road users including vehicles, bicyclists and <font color="#be00be">pedestrian</font>s at intersections in Germany and is called inD. The dataset consists of 10 hours of measurement data from four intersections and is available online for non-commercial research at: <font color="#006400">http</font>://www.inD-dataset.com </br></br>

<a href='http://arxiv.org/pdf/1911.07574.pdf'>1911.07574</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.6765баллов, №203</br>
<b>Bias-Aware Heapified Policy for Active Learning</b></br>
Authors: , Chang, Wen-Yen, Chiang, Wen-Huan, Lu, Shao-Hao, Wu, Tingfan, Sun, Min</br>
  The data efficiency of learning-based algorithms is more and more important since high-quality and clean data is expensive as well as hard to collect. In order to achieve high model performance with the least number of samples, active learning is a technique that queries the most important subset of data from the original dataset. In active learning domain, one of the mainstream research is the heuristic uncertainty-based method which is useful for the learning-based system. Recently, a few works propose to apply policy <font color="#00be00">reinforcement learning</font> (PRL) for querying important data. It seems more general than heuristic uncertainty-based method owing that PRL method depends on data feature which is reliable than human prior. However, there have two problems - sample inefficiency of policy learning and overconfidence, when applying PRL on active learning. To be more precise, sample inefficiency of policy learning occurs when sampling within a large action space, in the meanwhile, class imbalance can lead to the overconfidence. In this paper, we propose a bias-aware policy network called Heapified Active Learning (HAL), which prevents overconfidence, and improves <font color="#00be00">sample efficien</font>cy of policy learning by heapified structure without ignoring global inforamtion(overview of the whole unlabeled set). In our experiment, HAL <font color="#00be00">outperform</font>s other baseline methods on MNIST dataset and duplicated MNIST. Last but not least, we investigate the generalization of the HAL policy learned on MNIST dataset by directly applying it on MNIST-M. We show that the agent can generalize and outperform directly-learned policy under constrained labeled sets. </br></br>

<a href='http://arxiv.org/pdf/1911.07423.pdf'>1911.07423</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.6757баллов, №204</br>
<b>Learning to Predict More Accurate Text Instances for Scene Text\n  Detection</b></br>
Authors: , Li, XiaoQian, Liu, Jie, Zhang, ShuWu, Zhang, GuiXuan</br>
  At present, multi-oriented text detection methods based on deep neural network have achieved promising performances on various benchmarks. Nevertheless, there are still some difficulties for arbitrary shape text detection, especially for a simple and proper representation of arbitrary shape text instances. In this paper, a pixel-based text detector is proposed to facilitate the representation and prediction of text instances with arbitrary shapes in a simple manner. Firstly, to alleviate the effect of the target vertex sorting and achieve the direct <font color="#be00be">regression</font> of arbitrary shape text instances, the starting-point-independent coordinates regression loss is proposed. Furthermore, to predict more accurate text instances, the text instance accuracy loss is proposed as an assistant task to refine the predicted coordinates under the guidance of IoU. To evaluate the effectiveness of our detector, extensive experiments have been carried on public benchmarks. On the ICDAR 2015 Incidental Scene Text benchmark, our method achieves 86.5% of F-measure, and we obtain 84.8% of F-measure on Total-Text benchmark. The results show that our method can reach <font color="red">state-of-the-art</font> performance. </br></br>

<a href='http://arxiv.org/pdf/1911.09550.pdf'>1911.09550</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.6677баллов, №205</br>
<b>All You Need Is Boundary: Toward Arbitrary-Shaped Text Spotting</b></br>
Authors: , Wang, Hao, Lu, Pu, Zhang, Hui, Yang, Mingkun, Bai, Xiang, Xu, Yongchao, He, Mengchao, Wang, Yongpan, Liu, Wenyu</br>
  Recently, end-to-end text spotting that aims to detect and recognize text from cluttered images simultaneously has received particularly growing interest in computer vision. Different from the existing approaches that formulate text detection as bounding box extraction or instance <font color="#be00be">segmentation</font>, we localize a set of points on the boundary of each text instance. With the representation of such boundary points, we establish a simple yet effective scheme for end-to-end text spotting, which can read the text of arbitrary shapes. Experiments on three challenging datasets, including ICDAR2015, TotalText and COCO-Text demonstrate that the proposed method consistently surpasses the <font color="red">state-of-the-art</font> in both scene text detection and end-to-end text recognition tasks. </br></br>

<a href='http://arxiv.org/pdf/1911.08655.pdf'>1911.08655</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.6628баллов, №206</br>
<b>Towards Physics-informed Deep Learning for Turbulent Flow Prediction</b></br>
Authors: , Wang, Rui, Kashinath, Karthik, Mustafa, Mustafa, Albert, Adrian, Yu, Rose</br>
  While deep learning has shown tremendous success in a wide range of domains, it remains a grand challenge to incorporate physical principles in a systematic manner to the design, training, and inference of such models. In this paper, we aim to predict turbulent flow by learning its highly nonlinear dynamics from spatiotemporal velocity fields of large-scale fluid flow simulations of relevance to turbulence modeling and <font color="#be00be">climate</font> modeling. We adopt a hybrid approach by marrying two well-established turbulent flow simulation techniques with deep learning. Specifically, we introduce trainable spectral filters in a coupled model of Reynolds-averaged Navier-Stokes (RANS) and Large Eddy Simulation (LES), followed by a specialized U-net for prediction. Our approach, which we call turbulent-Flow Net (TF-Net), is grounded in a principled physics model, yet offers the flexibility of learned representations. We compare our model, TF-Net, with <font color="red">state-of-the-art</font> baselines and observe significant reductions in error for predictions60frames ahead. Most importantly, our method predicts physical fields that obey desirable physical characteristics, such as conservation of mass, whilst faithfully emulating the turbulent kinetic energy field and spectrum, which are critical for accurate prediction of turbulent flows. </br></br>

<a href='http://arxiv.org/pdf/1911.08855.pdf'>1911.08855</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.6524баллов, №207</br>
<b>RefineDetLite: A Lightweight One-stage Object Detection Framework for\n  CPU-only Devices</b></br>
Authors: , Chen, Chen, Liu, Mengyuan, Meng, Xiandong, Xiao, Wanpeng, Ju, Qi</br>
  Previous <font color="red">state-of-the-art</font> real-time object detectors have been reported on GPUs which are extremely expensive for processing massive data and in resource-restricted scenarios. Therefore, high efficiency object detectors on CPU-only devices are urgently-needed in industry. The floating-point operations (FLOPs) of networks are not strictly proportional to the running speed on CPU devices, which inspires the design of an exactly &quot;fast&quot; and &quot;accurate&quot; object detector. After investigating the concern gaps between classification networks and detection backbones, and following the design principles of efficient networks, we propose a <font color="#be00be">lightweight</font> residual-like backbone with large receptive fields and wide dimensions for low-level features, which are crucial for detection tasks. Correspondingly, we also design a light-head detection part to match the backbone capability. Furthermore, by analyzing the drawbacks of current one-stage detector training strategies, we also propose three orthogonal training strategies---IOU-guided loss, classes-aware weighting method and balanced multi-task training approach. Without bells and whistles, our proposed RefineDetLite achieves 26.8 mAP on the MSCOCO benchmark at a speed of 130 ms/pic on a single-thread CPU. The detection accuracy can be further increased to 29.6 mAP by integrating all the proposed training strategies, without apparent speed drop. </br></br>

<a href='http://arxiv.org/pdf/1911.07698.pdf'>1911.07698</a> &nbsp&nbsp (cs:ML, cs:NE) &nbsp&nbsp 0.6491баллов, №208</br>
<b>A Troubling Analysis of Reproducibility and Progress in Recommender\n  Systems Research</b></br>
Authors: , Dacrema, Maurizio Ferrari, Boglio, Simone, Cremonesi, Paolo, Jannach, Dietmar</br>
  The design of algorithms that generate personalized ranked item lists is a central topic of research in the field of recommender systems. In the past few years, in particular, approaches based on deep learning (neural) techniques have become dominant in the literature. For all of them, substantial progress over the <font color="red">state-of-the-art</font> is claimed. However, indications exist of certain problems in today\'s research practice, e.g., with respect to the choice and optimization of the baselines used for comparison, raising questions about the published claims. In order to obtain a better understanding of the actual progress, we have tried to reproduce recent results in the area of neural <font color="blue">recommendat</font>ion approaches based on collaborative filtering. The worrying outcome of the analysis of these recent works-all were published at prestigious scientific conferences between 2015 and 2018-is that 11 out of the 12 reproducible neural approaches can be <font color="#00be00">outperform</font>ed by conceptually simple methods, e.g., based on the nearest-neighbor heuristics. None of the computationally complex neural methods was actually consistently better than already existing learning-based techniques, e.g., using matrix factorization or linear models. In our analysis, we discuss common issues in today\'s research practice, which, despite the many papers that are published on the topic, have apparently led the field to a certain level of stagnation. </br></br>

<a href='http://arxiv.org/pdf/1911.08079.pdf'>1911.08079</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.6477баллов, №209</br>
<b>Two-Stream FCNs to Balance Content and Style for Style Transfer</b></br>
Authors: , Vo, Duc Minh, Sugimoto, Akihiro</br>
  <font color="#be00be">Style</font> transfer is to render given image contents in given styles, and it has an important role in both computer vision fundamental research and industrial applications. Following the success of deep learning based approaches, this problem has been re-launched very recently, but still remains a difficult task because of trade-off between preserving contents and faithful rendering of styles. In this paper, we propose an end-to-end two-stream Fully Convolutional Networks (FCNs) aiming at balancing the contributions of the content and the style in rendered images. Our proposed network consists of the encoder and decoder parts. The encoder part utilizes a FCN for content and a FCN for style where the two FCNs have feature injections and are independently trained to preserve the semantic content and to learn the faithful style representation in each. The semantic content feature and the style representation feature are then concatenated adaptively and fed into the decoder to generate style-transferred (stylized) images. In order to train our proposed network, we employ a loss network, the pre-trained VGG-16, to compute content loss and style loss, both of which are efficiently used for the feature injection as well as the feature concatenation. Our intensive experiments show that our proposed model generates more balanced stylized images in content and style than <font color="red">state-of-the-art</font> methods. Moreover, our proposed network achieves efficiency in speed. </br></br>

<a href='http://arxiv.org/pdf/1911.07732.pdf'>1911.07732</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.6459баллов, №210</br>
<b>Oriented Boxes for Accurate Instance Segmentation</b></br>
Authors: , Follmann, Patrick, K&#xf6;nig, Rebecca</br>
  <font color="red">State-of-the-art</font> instance-aware semantic <font color="#be00be">segmentation</font> algorithms use axis-aligned bounding boxes as an intermediate processing step to infer the final instance mask output. This leads to coarse and inaccurate mask proposals due to the following reasons: Axis-aligned boxes have a high background to foreground pixel-ratio, there is a strong variation of mask targets with respect to the underlying box, and neighboring instances frequently reach into the axis-aligned bounding box of the instance mask of interest.   In this work, we overcome these problems and propose using oriented boxes as the basis to infer instance masks. We show that oriented instance segmentation leads to very accurate mask predictions, especially when objects are diagonally aligned, touching, or overlapping each other. We evaluate our model on the D2S and Screws datasets and show that we can significantly improve the mask accuracy by 7% and 11% mAP (14.9% and 27.5% relative improvement), respectively. </br></br>

<a href='http://arxiv.org/pdf/1911.01616.pdf'>1911.01616</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 0.6427баллов, №211</br>
<b>Knowing What, How and Why: A Near Complete Solution for Aspect-based\n  Sentiment Analysis</b></br>
Authors: , Peng, Haiyun, Xu, Lu, Bing, Lidong, Huang, Fei, Lu, Wei, Si, Luo</br>
  Target-based <font color="#be00be">sentiment</font> analysis or aspect-based sentiment analysis (ABSA) refers to addressing various sentiment analysis tasks at a fine-grained level, which includes but is not limited to aspect extraction, aspect sentiment classification, and opinion extraction. There exist many solvers of the above individual subtasks or a combination of two subtasks, and they can work together to tell a complete story, i.e. the discussed aspect, the sentiment on it, and the cause of the sentiment. However, no previous ABSA research tried to provide a complete solution in one shot. In this paper, we introduce a new subtask under ABSA, named aspect sentiment triplet extraction (ASTE). Particularly, a solver of this task needs to extract triplets (What, How, Why) from the inputs, which show WHAT the targeted aspects are, HOW their sentiment polarities are and WHY they have such polarities (i.e. opinion reasons). For instance, one triplet from &quot;Waiters are very friendly and the pasta is simply average&quot; could be (\'Waiters\', positive, \'friendly\'). We propose a two-stage framework to address this task. The first stage predicts what, how and why in a unified model, and then the second stage pairs up the predicted what (how) and why from the first stage to output triplets. In the experiments, our framework has set a benchmark performance in this novel triplet extraction task. Meanwhile, it <font color="#00be00">outperform</font>s a few strong baselines adapted from <font color="red">state-of-the-art</font> related methods. </br></br>

<a href='http://arxiv.org/pdf/1911.09430.pdf'>1911.09430</a> &nbsp&nbsp (cs:ML, cs:RO, stat:ML) &nbsp&nbsp 0.6362баллов, №212</br>
<b>Visual Tactile Fusion Object Clustering</b></br>
Authors: , Zhang, Tao, Cong, Yang, Sun, Gan, Wang, Qianqian, Ding, Zhenming</br>
  Object <font color="#be00be">clustering</font>, aiming at grouping similar objects into one cluster with an unsupervised strategy, has been extensivelystudied among various data-driven applications. However, most existing <font color="red">state-of-the-art</font> object clustering methods (e.g., single-view or multi-view clustering methods) only explore visual information, while ignoring one of most important sensing modalities, i.e., tactile information which can help capture different object properties and further boost the performance of object clustering task. To effectively benefit both visual and tactile modalities for object clustering, in this paper, we propose a deep Auto-Encoder-like Non-negative Matrix Factorization framework for visual-tactile fusion clustering. Specifically, deep matrix factorization constrained by an under-complete Auto-Encoder-like architecture is employed to jointly learn <font color="#00be00">hierarchical</font> expression of visual-tactile fusion data, and preserve the local structure of data generating distribution of visual and tactile modalities. Meanwhile, a graph regularizer is introduced to capture the intrinsic relations of data samples within each modality. Furthermore, we propose a modality-level consensus regularizer to effectively align thevisual and tactile data in a common subspace in which the gap between visual and tactile data is mitigated. For the model optimization, we present an efficient alternating minimization strategy to solve our proposed model. Finally, we conduct extensive experiments on public datasets to verify the effectiveness of our framework. </br></br>

<a href='http://arxiv.org/pdf/1911.08142.pdf'>1911.08142</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.6224баллов, №213</br>
<b>GraphTER: Unsupervised Learning of Graph Transformation Equivariant\n  Representations via Auto-Encoding Node-wise Transformations</b></br>
Authors: , Gao, Xiang, Hu, Wei, Qi, Guo-Jun</br>
  Recent advances in Graph Convolutional Neural Networks (GCNNs) have shown their efficiency for non-Euclidean data on graphs, which often require a large amount of labeled data with high cost. It it thus critical to learn graph feature representations in an unsupervised manner in practice. To this end, we propose a novel unsupervised learning of Graph Transformation Equivariant Representations (GraphTER), aiming to capture intrinsic patterns of graph structure under both global and local transformations. Specifically, we allow to sample different groups of nodes from a graph and then transform them node-wise isotropically or anisotropically. Then, we self-train a representation encoder to capture the graph structures by reconstructing these node-wise transformations from the feature representations of the original and transformed graphs. In experiments, we apply the learned GraphTER to graphs of 3D <font color="#be00be">point cloud</font> data, and results on point cloud <font color="#be00be">segmentation</font>/classification show that GraphTER significantly <font color="#00be00">outperform</font>s <font color="red">state-of-the-art</font> unsupervised approaches and pushes greatly closer towards the upper bound set by the fully supervised counterparts. </br></br>

<a href='http://arxiv.org/pdf/1911.08402.pdf'>1911.08402</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.6198баллов, №214</br>
<b>A Boost Strategy to the Generative Error Based Video Anomaly Detection\n  Algorithms</b></br>
Authors: , Wang, Zhiguo, Zhang, Yu-Jin</br>
  The generation error (GE) based algorithms show excellent performances in the task of video <font color="#be00be">anomal</font>y detection. However, in the step of anomaly detection, they have two problems: (1) Abnormal events usually occur in local areas. It reduces the saliencies of the abnormal events to use the frame-level GE as the anomaly-score. (2) Every discrimination has both advantages and disadvantages, it is difficult to aggregate multiple discriminations effectively. To address these problems, we propose a promotion strategy which is consisted of two modules. Firstly, we replace the frame-level GE with the maximum of the block-level GEs in a frame as the anomaly score. Secondly, assuming that the stricter the anomaly threshold the more reliable the anomaly detected, we propose a reliable-anomaly (R-anomaly) based multiple discriminations aggregation method. In this method, we set a strict anomaly detection threshold (SADT) for each auxiliary discrimination to detect R-anomalies. Then we use the detected R-anomalies to enhance their anomaly scores in the main discrimination. Experiments are carried out on UCSD and CUHK Avenue datasets. The results demonstrate the effectiveness of the proposed strategy and achieve <font color="red">state-of-the-art</font> performance. </br></br>

<a href='http://arxiv.org/pdf/1910.04602.pdf'>1910.04602</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 0.6156баллов, №215</br>
<b>Multi-label Categorization of Accounts of Sexism using a Neural\n  Framework</b></br>
Authors: , Parikh, Pulkit, Abburi, Harika, Badjatiya, Pinkesh, Krishnan, Radhika, Chhaya, Niyati, Gupta, Manish, Varma, Vasudeva</br>
  Sexism, an injustice that subjects women and girls to enormous suffering, manifests in blatant as well as subtle ways. In the wake of growing documentation of experiences of sexism on the web, the automatic categorization of accounts of sexism has the potential to assist social scientists and policy makers in studying and countering sexism better. The existing work on sexism classification, which is different from sexism detection, has certain limitations in terms of the categories of sexism used and/or whether they can co-occur. To the best of our knowledge, this is the first work on the multi-label classification of sexism of any kind(s), and we contribute the largest dataset for sexism categorization. We develop a neural solution for this multi-label classification that can combine sentence representations obtained using models such as<font color="#00be00"> BERT </font>with distributional and linguistic word embeddings using a flexible, <font color="#00be00">hierarchical</font> architecture involving recurrent components and optional convolutional ones. Further, we leverage unlabeled accounts of sexism to infuse domain-specific elements into our framework. The best proposed method <font color="#00be00">outperform</font>s several deep learning as well as traditional machine learning baselines by an appreciable margin. </br></br>

<a href='http://arxiv.org/pdf/1911.09092.pdf'>1911.09092</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.6123баллов, №216</br>
<b>Superpixel Soup: Monocular Dense 3D Reconstruction of a Complex Dynamic\n  Scene</b></br>
Authors: , Kumar, Suryansh, Dai, Yuchao, Li, Hongdong</br>
  This work addresses the task of dense 3D reconstruction of a complex dynamic scene from images. The prevailing idea to solve this task is composed of a sequence of steps and is dependent on the success of several pipelines in its execution. To overcome such limitations with the existing algorithm, we propose a unified approach to solve this problem. We assume that a dynamic scene can be approximated by numerous piecewise planar surfaces, where each planar surface enjoys its own rigid motion, and the global change in the scene between two frames is as-rigid-as-possible (ARAP). Consequently, our model of a dynamic scene reduces to a soup of planar structures and rigid motion of these local planar structures. Using planar over-<font color="#be00be">segmentation</font> of the scene, we reduce this task to solving a &quot;3D jigsaw puzzle&quot; problem. Hence, the task boils down to correctly assemble each rigid piece to construct a 3D shape that complies with the geometry of the scene under the ARAP assumption. Further, we show that our approach provides an effective solution to the inherent scale-ambiguity in structure-from-motion under perspective projection. We provide extensive experimental results and evaluation on several benchmark datasets. Quantitative comparison with competing approaches shows <font color="red">state-of-the-art</font> performance. </br></br>

<a href='http://arxiv.org/pdf/1911.08080.pdf'>1911.08080</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.5942баллов, №217</br>
<b>DebFace: De-biasing Face Recognition</b></br>
Authors: , Gong, Sixue, Liu, Xiaoming, Jain, Anil K.</br>
  We address the problem of bias in automated<font color="#be00be"> face </font>recognition algorithms, where errors are consistently lower on certain cohorts belonging to specific demographic groups. We present a novel de-biasing adversarial network that learns to extract disentangled feature representations for both unbiased face recognition and demographics estimation. The proposed network consists of one identity classifier and three demographic classifiers (for gender, age, and race) that are trained to distinguish identity and demographic attributes, respectively. Adversarial learning is adopted to minimize correlation among feature factors so as to abate bias influence from other factors. We also design a new scheme to combine demographics with identity features to strengthen robustness of face representation in different demographic groups. The experimental results show that our approach is able to reduce bias in face recognition as well as demographics estimation while achieving <font color="red">state-of-the-art</font> performance. </br></br>

<a href='http://arxiv.org/pdf/1911.07420.pdf'>1911.07420</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.5922баллов, №218</br>
<b>A Graph Autoencoder Approach to Causal Structure Learning</b></br>
Authors: , Ng, Ignavier, Zhu, Shengyu, Chen, Zhitang, Fang, Zhuangyan</br>
  Causal structure learning has been a challenging task in the past decades and several mainstream approaches such as constraint- and score-based methods have been studied with <font color="blue">theor</font>etical guarantees. Recently, a new approach has transformed the combinatorial structure learning problem into a continuous one and then solved it using gradient-based optimization methods. Following the recent <font color="red">state-of-the-art</font>s, we propose a new gradient-based method to learn causal structures from observational data. The proposed method generalizes the recent gradient-based methods to a graph autoencoder framework that allows nonlinear structural equation models and is easily applicable to vector-valued variables. We demonstrate that on synthetic datasets, our proposed method <font color="#00be00">outperform</font>s other gradient-based methods significantly, especially on large causal graphs. We further investigate the scalability and efficiency of our method, and observe a near linear training time when scaling up the graph size. </br></br>

<a href='http://arxiv.org/pdf/1910.14488.pdf'>1910.14488</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.5900баллов, №219</br>
<b>NAT: Neural Architecture Transformer for Accurate and Compact\n  Architectures</b></br>
Authors: , Guo, Yong, Zheng, Yin, Tan, Mingkui, Chen, Qi, Chen, Jian, Zhao, Peilin, Huang, Junzhou</br>
  Designing effective architectures is one of the key factors behind the success of deep neural networks. Existing deep architectures are either manually designed or automatically searched by some Neural <font color="#00be00">Architecture Search</font> (NAS) methods. However, even a well-searched architecture may still contain many non-significant or redundant modules or operations (e.g., convolution or pooling), which may not only incur substantial memory consumption and computation cost but also deteriorate the performance. Thus, it is necessary to optimize the operations inside an architecture to improve the performance without introducing extra computation cost. Unfortunately, such a constrained optimization problem is NP-hard. To make the problem feasible, we cast the optimization problem into a Markov decision process (MDP) and seek to learn a Neural Architecture Transformer (NAT) to replace the redundant operations with the more computationally efficient ones (e.g., skip connection or directly removing the connection). Based on MDP, we learn NAT by exploiting <font color="#00be00">reinforcement learning</font> to obtain the optimization policies w.r.t. different architectures. To verify the effectiveness of the proposed strategies, we apply NAT on both hand-crafted architectures and NAS based architectures. Extensive experiments on two benchmark datasets, i.e., CIFAR-10 and ImageNet, demonstrate that the transformed architecture by NAT significantly <font color="#00be00">outperform</font>s both its original form and those architectures optimized by existing methods. </br></br>

<a href='http://arxiv.org/pdf/1911.08541.pdf'>1911.08541</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.5868баллов, №220</br>
<b>Deep Motion Blur Removal Using Noisy/Blurry Image Pairs</b></br>
Authors: , Zhang, Shuang, Zhen, Ada, Stevenson, Robert L.</br>
  Removing spatially variant motion blur from a blurry image is a challenging problem as blur sources are complicated and difficult to model accurately. Recent progress in deep neural networks suggests that <font color="blue">kernel</font> free single image deblurring can be efficiently performed, but questions about deblurring performance persist. Thus, we propose to restore a sharp image by fusing a pair of noisy/blurry images captured in a burst. Two neural network structures, DeblurRNN and DeblurMerger, are presented to exploit the pair of images in a sequential manner or parallel manner. To boost the training, gradient loss, adversarial loss and spectral normalization are leveraged. The training dataset that consists of pairs of noisy/blurry images and the corresponding ground truth sharp image is synthesized based on the benchmark dataset GOPRO. We evaluated the trained networks on a variety of synthetic datasets and real image pairs. The results demonstrate that the proposed approach <font color="#00be00">outperform</font>s the <font color="red">state-of-the-art</font> both qualitatively and quantitatively. </br></br>

<a href='http://arxiv.org/pdf/1911.09310.pdf'>1911.09310</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.5830баллов, №221</br>
<b>Improving Unsupervised Domain Adaptation with Variational Information\n  Bottleneck</b></br>
Authors: , Song, Yuxuan, Yu, Lantao, Cao, Zhangjie, Zhou, Zhiming, Shen, Jian, Shao, Shuo, Zhang, Weinan, Yu, Yong</br>
  Domain adaptation aims to leverage the supervision signal of source domain to obtain an accurate model for target domain, where the labels are not available. To leverage and adapt the label information from source domain, most existing methods employ a feature extracting function and match the marginal distributions of source and target domains in a shared feature space. In this paper, from the perspective of information <font color="blue">theor</font>y, we show that representation matching is actually an insufficient constraint on the feature space for obtaining a model with good generalization performance in target domain. We then propose variational bottleneck domain adaptation (VBDA), a new domain adaptation method which improves feature transferability by explicitly enforcing the feature extractor to ignore the task-irrelevant factors and focus on the information that is essential to the task of interest for both source and target domains. Extensive experimental results demonstrate that VBDA significantly <font color="#00be00">outperform</font>s <font color="red">state-of-the-art</font> methods across three domain adaptation benchmark datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.06285.pdf'>1911.06285</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.5723баллов, №222</br>
<b>DomainGAN: Generating Adversarial Examples to Attack Domain Generation\n  Algorithm Classifiers</b></br>
Authors: , Corley, Isaac, Lwowski, Jonathan, Hoffman, Justin</br>
  Domain Generation Algorithms (DGAs) are frequently used to generate numerous domains for use by botnets. These domains are often utilized as rendezvous points for servers that <font color="#be00be">malware</font> has command and control over. There are many algorithms that are used to generate domains, however many of these algorithms are simplistic and easily detected by traditional machine learning techniques. In this paper, three variants of Generative Adversarial Networks (GANs) are optimized to generate domains which have similar characteristics of benign domains, resulting in domains which greatly evade several <font color="red">state-of-the-art</font> deep learning based DGA classifiers. We additionally provide a detailed analysis into offensive usability for each variant with respect to repeated and existing domain collisions. Finally, we fine-tune the state-of-the-art DGA classifiers by adding GAN generated samples to their original training datasets and analyze the changes in performance. Our results conclude that GAN based DGAs are superior in evading DGA classifiers in comparison to traditional DGAs, and of the variants, the Wasserstein GAN with Gradient Penalty (WGANGP) is the highest performing DGA for uses both offensively and defensively. </br></br>

<a href='http://arxiv.org/pdf/1911.06791.pdf'>1911.06791</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 0.5698баллов, №223</br>
<b>Non-Monotone Submodular Maximization with Multiple Knapsacks in Static\n  and Dynamic Settings</b></br>
Authors: , Dosko&#x10d;, Vanja, Friedrich, Tobias, G&#xf6;bel, Andreas, Neumann, Frank, Neumann, Aneta, Quinzan, Francesco</br>
  We study the problem of maximizing a non-monotone submodular function under multiple knapsack constraints. We propose a simple discrete greedy algorithm to approach this problem, and prove that it yields strong approximation guarantees for functions with bounded curvature. In contrast to other heuristics, this requires no problem relaxation to continuous domains and it maintains a constant-factor approximation guarantee in the problem size. In the case of a single knapsack, our analysis suggests that the standard greedy can be used in non-monotone settings.   Additionally, we study this problem in a dynamic setting, by which knapsacks change during the optimization process. We modify our greedy algorithm to avoid a complete restart at each constraint update. This modification retains the approximation guarantees of the static case.   We evaluate our results experimentally on a video <font color="#be00be">summarization</font> and sensor placement task. We show that our proposed algorithm competes with the <font color="red">state-of-the-art</font> in static settings. Furthermore, we show that in dynamic settings with tight computational time budget, our modified greedy yields significant improvements over starting the greedy from scratch, in terms of the solution quality achieved. </br></br>

<a href='http://arxiv.org/pdf/1911.07167.pdf'>1911.07167</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.5695баллов, №224</br>
<b>Low-Weight and Learnable Image Denoising</b></br>
Authors: , Vaksman, Gregory, Elad, Michael, Milanfar, Peyman</br>
  <font color="#be00be">Image denoising</font> is a well studied problem with an extensive activity that has spread over several decades. Despite the many available denoising algorithms, the quest for simple, powerful and fast denoisers is still an active and vibrant topic of research. Leading classical denoising methods are typically designed to exploit the inner structure in images by modeling local overlapping patches. In contrast, recent newcomers to this arena are supervised neural-network-based methods that bypass this modeling altogether, targeting the inference goal directly and globally, while tending to be very deep and parameter heavy.   This work proposes a novel low-weight learnable architecture that embeds in it several of the main concepts from the classical methods, while being trained for best denoising performance. More specifically, our proposed network relies on patch processing, leveraging non-local self-similarity, representation sparsity and a multiscale treatment. The proposed architecture achieves near <font color="red">state-of-the-art</font> denoising results, while using a small fraction of the typical number of parameters. Furthermore, we demonstrate the ability of the proposed network to adapt itself to an incoming image by leveraging similar clean ones. </br></br>

<a href='http://arxiv.org/pdf/1911.07982.pdf'>1911.07982</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp 0.5628баллов, №225</br>
<b>Unsupervised Domain Adaptation via Structured Prediction Based Selective\n  Pseudo-Labeling</b></br>
Authors: , Wang, Qian, Breckon, Toby P.</br>
  Unsupervised domain adaptation aims to address the problem of classifying unlabeled samples from the target domain whilst labeled samples are only available from the source domain and the data distributions are different in these two domains. As a result, classifiers trained from labeled samples in the source domain suffer from significant performance drop when directly applied to the samples from the target domain. To address this issue, different approaches have been proposed to learn domain-invariant features or domain-specific classifiers. In either case, the lack of labeled samples in the target domain can be an issue which is usually overcome by pseudo-labeling. Inaccurate pseudo-labeling, however, could result in catastrophic error accumulation during learning. In this paper, we propose a novel selective pseudo-labeling strategy based on structured prediction. The idea of structured prediction is inspired by the fact that samples in the target domain are well clustered within the deep feature space so that unsupervised <font color="#be00be">clustering</font> analysis can be used to facilitate accurate pseudo-labeling. Experimental results on four datasets (i.e. Office-Caltech, Office31, ImageCLEF-DA and Office-Home) validate our approach <font color="#00be00">outperform</font>s contemporary <font color="red">state-of-the-art</font> methods. </br></br>

<a href='http://arxiv.org/pdf/1911.01366.pdf'>1911.01366</a> &nbsp&nbsp (stat:ML, cs:AI, cs:ML) &nbsp&nbsp 0.5599баллов, №226</br>
<b>Inferring Coordination Strategies from Time Series of Movement Data</b></br>
Authors: , Amornbunchornvej, Chainarong, Berger-Wolf, Tanya</br>
  How do groups of individuals achieve consensus in movement decisions? Do individuals follow their friends, the one predetermined leader, or whomever just happens to be nearby? To address these questions computationally, we formalize Coordination Strategy Inference Problem. In this setting, a group of multiple individuals moves in a coordinated manner towards a target path. Each individual uses a specific strategy to follow others (e.g. <font color="#be00be">nearest neighbo</font>rs, pre-defined leaders, preferred friends). Given a set of time series that includes coordinated movement and a set of candidate strategies as inputs, we provide the first methodology (to the best of our knowledge) to infer the set of strategies that each individual uses to achieve movement coordination at the group level. We evaluate and demonstrate the performance of the proposed framework by predicting the direction of movement of an individual in a group in both simulated datasets as well as two <font color="#009600">real-world</font> datasets: a school of fish and a troop of baboons. Moreover, since there is no prior methodology for inferring individual-level strategies, we compare our framework with the <font color="red">state-of-the-art</font> approach for the task of classification of group-level-coordination models. The results show that our approach is highly accurate in inferring the correct strategy in simulated datasets even in complicated mixed strategy settings, which no existing method can infer. In the task of classification of group-level-coordination models, our framework performs better than the state-of-the-art approach in all datasets. Animal data experiments show that fish, as expected, follow their neighbors, while baboons have a preference to follow specific individuals. Our methodology generalizes to arbitrary time series data of real numbers, beyond movement data. </br></br>

<a href='http://arxiv.org/pdf/1911.09273.pdf'>1911.09273</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp 0.5553баллов, №227</br>
<b>Attention-Informed Mixed-Language Training for Zero-shot Cross-lingual\n  Task-oriented Dialogue Systems</b></br>
Authors: , Liu, Zihan, Winata, Genta Indra, Lin, Zhaojiang, Xu, Peng, Fung, Pascale</br>
  Recently, data-driven task-oriented dialogue systems have achieved promising performance in English. However, developing dialogue systems that support <font color="#be00be">low-resource</font> languages remains a long-standing challenge due to the absence of high-quality data. In order to circumvent the expensive and time-consuming data collection, we introduce Attention-Informed Mixed-Language Training (MLT), a novel <font color="#00be00">zero-shot</font> adaptation method for cross-lingual task-oriented dialogue systems. It leverages very few task-related parallel word pairs to generate code-switching sentences for learning the inter-lingual semantics across languages. Instead of manually selecting the word pairs, we propose to extract source words based on the scores computed by the attention layer of a trained English task-related model and then generate word pairs using existing bilingual dictionaries. Furthermore, intensive experiments with different cross-lingual embeddings demonstrate the effectiveness of our approach. Finally, with very few word pairs, our model achieves significant zero-shot adaptation performance improvements in both cross-lingual dialogue state <font color="#be00be">tracking</font> and natural language understanding (i.e., intent detection and slot filling) tasks compared to the current <font color="red">state-of-the-art</font> approaches, which utilize a much larger amount of bilingual data. </br></br>

<a href='http://arxiv.org/pdf/1911.07979.pdf'>1911.07979</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.5543баллов, №228</br>
<b>ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph\n  Representations</b></br>
Authors: , Ranjan, Ekagra, Sanyal, Soumya, Talukdar, Partha Pratim</br>
  Graph Neural Networks (GNN) have been shown to work effectively for modeling graph structured data to solve tasks such as node classification, link prediction and graph classification. There has been some recent progress in defining the notion of pooling in graphs whereby the model tries to generate a graph level representation by downsampling and summarizing the information present in the nodes. Existing pooling methods either fail to effectively capture the graph substructure or do not easily scale to large graphs. In this work, we propose ASAP (Adaptive Structure Aware Pooling), a sparse and differentiable pooling method that addresses the limitations of previous graph pooling architectures. ASAP utilizes a novel self-attention network along with a modified GNN formulation to capture the importance of each node in a given graph. It also learns a sparse soft cluster assignment for nodes at each layer to effectively pool the subgraphs to form the pooled graph. Through extensive experiments on multiple datasets and <font color="blue">theor</font>etical analysis, we motivate our choice of the components used in ASAP. Our experimental results show that combining existing GNN architectures with ASAP leads to <font color="red">state-of-the-art</font> results on multiple graph classification benchmarks. ASAP has an average improvement of 4\\%, compared to current sparse <font color="#00be00">hierarchical</font> state-of-the-art method. </br></br>

<a href='http://arxiv.org/pdf/1911.02237.pdf'>1911.02237</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.5510баллов, №229</br>
<b>Localization-aware Channel Pruning for Object Detection</b></br>
Authors: , Xie, Zihao, Tao, Wenbing, Zhu, Li, Zhao, Lin</br>
  Channel pruning is one of the important methods for deep model compression. Most of existing pruning methods mainly focus on classification. Few of them conduct systematic research on <font color="#be00be">object detection</font>. However, object detection is different from classification, which requires not only semantic information but also localization information. In this paper, based on DCP \\cite{zhuang2018discrimination} which is <font color="red">state-of-the-art</font> pruning method for classification, we propose a localization-aware auxiliary network to find out the channels with key information for classification and <font color="#be00be">regression</font> so that we can conduct channel pruning directly for object detection, which saves lots of time and computing resources. In order to capture the localization information, we first design the auxiliary network with a contextual ROIAlign layer which can obtain precise localization information of the default boxes by pixel alignment and enlarges the receptive fields of the default boxes when pruning shallow layers. Then, we construct a loss function for object detection task which tends to keep the channels that contain the key information for classification and regression. Extensive experiments demonstrate the effectiveness of our method. On MS COCO, we prune 70\\% parameters of the SSD based on ResNet-50 with modest accuracy drop, which <font color="#00be00">outperform</font>s the-state-of-art method. </br></br>

<a href='http://arxiv.org/pdf/1911.08610.pdf'>1911.08610</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp 0.5458баллов, №230</br>
<b>Efficient decorrelation of features using Gramian in Reinforcement\n  Learning</b></br>
Authors: , Mavrin, Borislav, Graves, Daniel, Chan, Alan</br>
  Learning good representations is a long standing problem in <font color="#00be00">reinforcement learning</font> (RL). One of the conventional ways to achieve this goal in the supervised setting is through regularization of the parameters. Extending some of these ideas to the RL setting has not yielded similar improvements in learning. In this paper, we develop an online regularization framework for decorrelating features in RL and demonstrate its utility in several test environments. We prove that the proposed algorithm converges in the linear function approximation setting and does not change the main objective of maximizing cumulative reward. We demonstrate how to scale the approach to deep RL using the Gramian of the features achieving linear computational complexity in the number of features and squared complexity in size of the batch. We conduct an extensive empirical study of the new approach on Atari 2600 games and show a significant improvement in <font color="#00be00">sample efficien</font>cy in 40 out of 49 games. </br></br>

<a href='http://arxiv.org/pdf/1911.08651.pdf'>1911.08651</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.5386баллов, №231</br>
<b>Unified Multifaceted Feature Learning for Person Re-Identification</b></br>
Authors: , Yan, Cheng, Pang, Guansong, Bai, Xiao, Shen, Chunhua</br>
  Person <font color="blue">re-identification</font> (ReID) aims at re-identifying persons from different viewpoints across multiple cameras, of which it is of great importance to learn multifaceted features expressed in different parts of a person, e.g., clothes, bags, and other accessories in the main body, appearance in the head, and shoes in the foot. To learn such features, existing methods are focused on the striping-based approach that builds multi-branch neural networks to learn local features in each part of the identities, with one-branch network dedicated to one part. This results in complex models with a large number of parameters. To address this issue, this paper proposes to learn the multifaceted features in a simple unified single-branch neural network. The Unified Multifaceted Feature Learning (UMFL) framework is introduced to fulfill this goal, which consists of two key collaborative modules: compound batch image erasing (including batch constant erasing and random erasing) and <font color="#00be00">hierarchical</font> structured loss. The loss structures the augmented images resulted by the two types of image erasing in a two-level hierarchy and enforces multifaceted attention to different parts. As we show in the extensive experimental results on four benchmark person ReID datasets, despite the use of significantly simplified network structure, our method performs substantially better than <font color="red">state-of-the-art</font> competing methods. Our method can also effectively generalize to vehicle ReID, achieving similar improvement on two vehicle ReID datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.07783.pdf'>1911.07783</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.5363баллов, №232</br>
<b>AIM 2019 Challenge on Real-World Image Super-Resolution: Methods and\n  Results</b></br>
Authors: , Lugmayr, Andreas, Danelljan, Martin, Timofte, Radu, Fritsche, Manuel, Gu, Shuhang, Purohit, Kuldeep, Kandula, Praveen, Suin, Maitreya, Rajagopalan, A N, Joon, Nam Hyung, Won, Yu Seung, Kim, Guisik, Kwon, Dokyeong, Hsu, Chih-Chung, Lin, Chia-Hsiang, Huang, Yuanfei, Sun, Xiaopeng, Lu, Wen, Li, Jie, Gao, Xinbo, Bell-Kligler, Sefi</br>
  This paper reviews the AIM 2019 challenge on <font color="#009600">real world</font> <font color="#be00be">super-resolution</font>. It focuses on the participating methods and final results. The challenge addresses the real world setting, where paired true high and low-resolution images are unavailable. For training, only one set of source input images is therefore provided in the challenge. In Track 1: Source Domain the aim is to super-resolve such images while preserving the low level image characteristics of the source input domain. In Track 2: Target Domain a set of high-quality images is also provided for training, that defines the output domain and desired quality of the super-resolved images. To allow for quantitative evaluation, the source input images in both tracks are constructed using artificial, but realistic, image degradations. The challenge is the first of its kind, aiming to advance the <font color="red">state-of-the-art</font> and provide a standard benchmark for this newly emerging task. In total 7 teams competed in the final testing phase, demonstrating new and innovative solutions to the problem. </br></br>

<a href='http://arxiv.org/pdf/1910.00652.pdf'>1910.00652</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.5293баллов, №233</br>
<b>Automated Weed Detection in Aerial Imagery with Context</b></br>
Authors: , Bullock, Delia, Mangeni, Andrew, Wiesner-Hanks, Tyr, DeChant, Chad, Stewart, Ethan L., Kaczmar, Nicholas, Kolkman, Judith M., Nelson, Rebecca J., Gore, Michael A., Lipson, Hod</br>
  In this paper, we demonstrate the ability to discriminate between cultivated maize plant and grass or grass-like weed image segments using the context surrounding the image segments. While convolutional neural networks have brought <font color="red">state of the art</font> accuracies within <font color="#be00be">object detection</font>, errors arise when objects in different classes share similar features. This scenario often occurs when objects in images are viewed at too small of a scale to discern distinct differences in features, causing images to be incorrectly classified or localized. To solve this problem, we will explore using context when classifying image segments. This technique involves feeding a convolutional neural network a central square image along with a border of its direct surroundings at train and test times. This means that although images are labelled at a smaller scale to preserve accurate localization, the network classifies the images and learns features that include the wider context. We demonstrate the benefits of this context technique in the object detection task through a case study of grass (foxtail) and grass-like (yellow nutsedge) weed detection in maize fields. In this standard situation, adding context alone nearly halved the error of the neural network from 7.1% to 4.3%. After only one epoch with context, the network also achieved a higher accuracy than the network without context did after 50 epochs. The benefits of using the context technique are likely to particularly evident in agricultural contexts in which parts (such as leaves) of several plants may appear similar when not taking into account the context in which those parts appear. </br></br>

<a href='http://arxiv.org/pdf/1911.08363.pdf'>1911.08363</a> &nbsp&nbsp (cs:AI, cs:ML) &nbsp&nbsp 0.5279баллов, №234</br>
<b>Attention Privileged Reinforcement Learning For Domain Transfer</b></br>
Authors: , Salter, Sasha, Rao, Dushyant, Wulfmeier, Markus, Hadsell, Raia, Posner, Ingmar</br>
  Applying <font color="#00be00">reinforcement learning</font> (RL) to physical systems presents notable challenges, given requirements regarding <font color="#00be00">sample efficien</font>cy, safety, and physical constraints compared to simulated environments. To enable transfer of policies trained in simulation, randomising simulation parameters leads to more robust policies, but also significantly extends training time. In this paper, we exploit access to privileged information (such as environment states) often available in simulation, in order to improve and accelerate learning over randomised environments. We introduce Attention Privileged Reinforcement Learning (APRiL), which equips the agent with an attention mechanism and makes use of state information in simulation, learning to align attention between state- and image-based policies while additionally sharing generated data. During deployment we can apply the image-based policy to remove the requirement of access to additional information. We experimentally demonstrate accelerated and more robust learning on a number of diverse domains, leading to improved final performance for environments both within and outside the training distribution. </br></br>

<a href='http://arxiv.org/pdf/1910.01426.pdf'>1910.01426</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.5184баллов, №235</br>
<b>High-dimensional Dense Residual Convolutional Neural Network for Light\n  Field Reconstruction</b></br>
Authors: , Meng, Nan, So, Hayden K. -H., Sun, Xing, Lam, Edmund Y.</br>
  We consider the problem of high-dimensional light field reconstruction and develop a learning-based framework for spatial and angular <font color="#be00be">super-resolution</font>. Many current approaches either require disparity clues or restore the spatial and angular details separately. Such methods have difficulties with non-Lambertian surfaces or occlusions. In contrast, we formulate light field super-resolution (LFSR) as tensor restoration and develop a learning framework based on a two-stage restoration with 4-dimensional (4D) convolution. This allows our model to learn the features capturing the geometry information encoded in multiple adjacent views. Such geometric features vary near the occlusion regions and indicate the foreground object border. To train a feasible network, we propose a novel normalization operation based on a group of views in the feature maps, design a stage-wise loss function, and develop the multi-range training strategy to further improve the performance. Evaluations are conducted on a number of light field datasets including <font color="#009600">real-world</font> scenes, synthetic data, and microscope light fields. The proposed method achieves superior performance and less execution time comparing with other <font color="red">state-of-the-art</font> schemes. </br></br>

<a href='http://arxiv.org/pdf/1911.05939.pdf'>1911.05939</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.5106баллов, №236</br>
<b>SimVODIS: Simultaneous Visual Odometry, Object Detection, and Instance\n  Segmentation</b></br>
Authors: , Kim, Ue-Hwan, Kim, Se-Ho, Kim, Jong-Hwan</br>
  Intelligent agents need to understand the surrounding environment to provide meaningful services to or interact intelligently with humans. The agents should perceive geometric features as well as semantic entities inherent in the environment. Contemporary methods in general provide one type of information regarding the environment at a time, making it difficult to conduct high-level tasks. Moreover, running two types of methods and associating two resultant information requires a lot of computation and complicates the software architecture. To overcome these limitations, we propose a neural architecture that simultaneously performs both geometric and semantic tasks in a single thread: simultaneous visual odometry, <font color="#be00be">object detection</font>, and instance <font color="#be00be">segmentation</font> (SimVODIS). Training SimVODIS requires unlabeled video sequences and the photometric consistency between input image frames generates self-supervision signals. The performance of SimVODIS <font color="#00be00">outperform</font>s or matches the <font color="red">state-of-the-art</font> performance in pose estimation, depth map prediction, object detection, and instance segmentation tasks while completing all the tasks in a single thread. We expect SimVODIS would enhance the autonomy of intelligent agents and let the agents provide effective services to humans. </br></br>

<a href='http://arxiv.org/pdf/1911.06928.pdf'>1911.06928</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.5077баллов, №237</br>
<b>Generalized Maximum Causal Entropy for Inverse Reinforcement Learning</b></br>
Authors: , Mai, Tien, Chan, Kennard, Jaillet, Patrick</br>
  We consider the problem of learning from demonstrated trajectories with inverse <font color="#00be00">reinforcement learning</font> (IRL). Motivated by a limitation of the classical maximum entropy model in capturing the structure of the network of states, we propose an IRL model based on a generalized version of the causal entropy maximization problem, which allows us to generate a class of maximum entropy IRL models. Our generalized model has an advantage of being able to recover, in addition to a reward function, another expert\'s function that would (partially) capture the impact of the connecting structure of the states on experts\' decisions. Empirical evaluation on a <font color="#009600">real-world</font> dataset and a grid-world dataset shows that our generalized model <font color="#00be00">outperform</font>s the classical ones, in terms of recovering reward functions and demonstrated trajectories. </br></br>

<a href='http://arxiv.org/pdf/1911.09391.pdf'>1911.09391</a> &nbsp&nbsp (cs:ML, cs:AI, cs:RO, stat:ML) &nbsp&nbsp 0.5035баллов, №238</br>
<b>Accelerating Reinforcement Learning with Suboptimal Guidance</b></br>
Authors: , B&#xf8;hn, Eivind, Moe, Signe, Johansen, Tor Arne</br>
  <font color="#00be00">Reinforcement Learning</font> in domains with <font color="#00be00">sparse reward</font>s is a difficult problem, and a large part of the training process is often spent searching the state space in a more or less random fashion for any learning signals. For control problems, we often have some controller readily available which might be suboptimal but nevertheless solves the problem to some degree. This controller can be used to guide the initial exploration phase of the learning controller towards reward yielding states, reducing the time before refinement of a viable policy can be initiated. In our work, the agent is guided through an auxiliary behaviour cloning loss which is made conditional on a Q-filter, i.e. it is only applied in situations where the critic deems the guiding controller to be better than the agent. The Q-filter provides a natural way to adjust the guidance throughout the training process, allowing the agent to exceed the guiding controller in a manner that is adaptive to the task at hand and the proficiency of the guiding controller. The contribution of this paper lies in identifying shortcomings in previously proposed implementations of the Q-filter concept, and in suggesting some ways these issues can be mitigated. These modifications are tested on the OpenAI Gym Fetch environments, showing clear improvements in adaptivity and yielding increased performance in all robotic environments tested. </br></br>

<a href='http://arxiv.org/pdf/1911.07401.pdf'>1911.07401</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.4912баллов, №239</br>
<b>TSRNet: Scalable 3D Surface Reconstruction Network for Point Clouds\n  using Tangent Convolution</b></br>
Authors: , Mi, Zhenxing, Luo, Yiming, Tao, Wenbing</br>
  Existing learning-based surface reconstruction methods from <font color="#be00be">point cloud</font>s are still facing challenges in terms of scalability and preservation of details on point clouds of large scales. In this paper, we propose the TSRNet, a novel scalable learning-based method for surface reconstruction. It first takes a point cloud and its related octree vertices as input and learns to classify whether the octree vertices are in front or at back of the implicit surface. Then the Marching Cubes (MC) is applied to extract a surface from the binary labeled octree. In our method, we design a scalable learning-based pipeline for surface reconstruction. It does not consider the whole input data at once. It allows to divide the point cloud and octree vertices and to process different parts in parallel. Our network captures local geometry details by constructing local geometry-aware features for octree vertices. The local geometry-aware features enhance the predication accuracy greatly for the relative position among the vertices and the implicit surface. They also boost the generalization capability of our network. Our method is able to reconstruct local geometry details from point clouds of different scales, especially for point clouds with millions of points. More importantly, the time consumption on such point clouds is acceptable and <font color="#960096">competitive</font>. Experiments show that our method achieves a significant breakthrough in scalability and quality compared with <font color="red">state-of-the-art</font> learning-based methods. </br></br>

<a href='http://arxiv.org/pdf/1911.08444.pdf'>1911.08444</a> &nbsp&nbsp (cs:ML, cs:AI, cs:RO, stat:ML) &nbsp&nbsp 0.4836баллов, №240</br>
<b>MANGA: Method Agnostic Neural-policy Generalization and Adaptation</b></br>
Authors: , Bharadhwaj, Homanga, Yamaguchi, Shoichiro, Maeda, Shin-ichi</br>
  In this paper we target the problem of transferring policies across multiple environments with different dynamics parameters and motor noise variations, by introducing a framework that decouples the processes of policy learning and system identification. Efficiently transferring learned policies to an unknown environment with changes in dynamics configurations in the presence of motor noise is very important for operating robots in the <font color="#009600">real world</font>, and our work is a novel attempt in that direction. We introduce MANGA: Method Agnostic Neural-policy Generalization and Adaptation, that trains dynamics conditioned policies and efficiently learns to estimate the dynamics parameters of the environment given off-policy state-transition rollouts in the environment. Our scheme is agnostic to the type of training method used - both <font color="#00be00">reinforcement learning</font> (RL) and imitation learning (IL) strategies can be used. We demonstrate the effectiveness of our approach by experimenting with four different <font color="#006400">MuJoCo</font> agents and comparing against previously proposed transfer baselines. </br></br>

<a href='http://arxiv.org/pdf/1911.07247.pdf'>1911.07247</a> &nbsp&nbsp (cs:ML, cs:NE, stat:ML) &nbsp&nbsp 0.4758баллов, №241</br>
<b>Hebbian Synaptic Modifications in Spiking Neurons that Learn</b></br>
Authors: , Bartlett, Peter L., Baxter, Jonathan</br>
  In this paper, we derive a new model of synaptic plasticity, based on recent algorithms for <font color="#00be00">reinforcement learning</font> (in which an agent attempts to learn appropriate actions to maximize its long-term average reward). We show that these direct reinforcement learning algorithms also give locally optimal performance for the problem of reinforcement learning with multiple agents, without any explicit communication between agents. By considering a network of spiking neurons as a collection of agents attempting to maximize the long-term average of a reward signal, we derive a synaptic update rule that is qualitatively similar to Hebb\'s postulate. This rule requires only simple computations, such as addition and leaky integration, and involves only quantities that are available in the vicinity of the synapse. Furthermore, it leads to synaptic connection strengths that give locally optimal values of the long term average reward. The reinforcement learning paradigm is sufficiently broad to encompass many learning problems that are solved by the <font color="#00be00">brain</font>. We illustrate, with simulations, that the approach is effective for simple pattern classification and motor learning tasks. </br></br>

<a href='http://arxiv.org/pdf/1911.09655.pdf'>1911.09655</a> &nbsp&nbsp (cs:CL, cs:ML, cs:SD) &nbsp&nbsp 0.4725баллов, №242</br>
<b>Temporal Reasoning via Audio Question Answering</b></br>
Authors: , Fayek, Haytham M., Johnson, Justin</br>
  Multimodal question answering tasks can be used as proxy tasks to study systems that can perceive and reason about the world. Answering questions about different types of input modalities stresses different aspects of reasoning such as visual reasoning, reading comprehension, story understanding, or navigation. In this paper, we use the task of Audio Question Answering (AQA) to study the temporal reasoning abilities of machine learning models. To this end, we introduce the <font color="blue">Diagnos</font>tic Audio Question Answering (DAQA) dataset comprising audio sequences of natural sound events and programmatically generated questions and answers that probe various aspects of temporal reasoning. We adapt several recent <font color="red">state-of-the-art</font> methods for visual question answering to the AQA task, and use DAQA to demonstrate that they perform poorly on questions that require in-depth temporal reasoning. Finally, we propose a new model, Multiple Auxiliary Controllers for Linear Modulation (MALiMo) that extends the recent Feature-wise Linear Modulation (FiLM) model and significantly improves its temporal reasoning capabilities. We envisage DAQA to foster research on AQA and temporal reasoning and MALiMo a step towards models for AQA. </br></br>

<a href='http://arxiv.org/pdf/1911.06832.pdf'>1911.06832</a> &nbsp&nbsp (cs:ML, cs:AI, cs:NE, cs:RO, stat:ML) &nbsp&nbsp 0.4621баллов, №243</br>
<b>Data-efficient Co-Adaptation of Morphology and Behaviour with Deep\n  Reinforcement Learning</b></br>
Authors: , Luck, Kevin Sebastian, Amor, Heni Ben, Calandra, Roberto</br>
  Humans and animals are capable of quickly learning new behaviours to solve new tasks. Yet, we often forget that they also rely on a highly specialized morphology that co-adapted with motor control throughout thousands of years. Although compelling, the idea of co-adapting morphology and behaviours in robots is often unfeasible because of the long manufacturing times, and the need to re-design an appropriate controller for each morphology. In this paper, we propose a novel approach to automatically and efficiently co-adapt a robot morphology and its controller. Our approach is based on recent advances in deep <font color="#00be00">reinforcement learning</font>, and specifically the soft actor critic algorithm. Key to our approach is the possibility of leveraging previously tested morphologies and behaviors to estimate the performance of new candidate morphologies. As such, we can make full use of the information available for making more informed decisions, with the ultimate goal of achieving a more data-efficient co-adaptation (i.e., reducing the number of morphologies and behaviors tested). Simulated experiments show that our approach requires drastically less design prototypes to find good morphology-behaviour combinations, making this method particularly suitable for future co-adaptation of robot designs in the <font color="#009600">real world</font>. </br></br>

<a href='http://arxiv.org/pdf/1911.08701.pdf'>1911.08701</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.4487баллов, №244</br>
<b>Bayesian Curiosity for Efficient Exploration in Reinforcement Learning</b></br>
Authors: , Blau, Tom, Ott, Lionel, Ramos, Fabio</br>
  Balancing exploration and exploitation is a fundamental part of <font color="#00be00">reinforcement learning</font>, yet most <font color="red">state-of-the-art</font> algorithms use a naive exploration protocol like $\\epsilon$-greedy. This contributes to the problem of high sample complexity, as the algorithm wastes effort by repeatedly visiting parts of the state space that have already been explored. We introduce a novel method based on <font color="blue">Bayes</font>ian linear <font color="#be00be">regression</font> and latent space embedding to generate an intrinsic reward signal that encourages the learning agent to seek out unexplored parts of the state space. This method is computationally efficient, simple to implement, and can extend any state-of-the-art reinforcement learning algorithm. We evaluate the method on a range of algorithms and challenging control tasks, on both simulated and physical robots, demonstrating how the proposed method can significantly improve sample complexity. </br></br>

<a href='http://arxiv.org/pdf/1911.08039.pdf'>1911.08039</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.4412баллов, №245</br>
<b>Reliability Does Matter: An End-to-End Weakly Supervised Semantic\n  Segmentation Approach</b></br>
Authors: , Zhang, Bingfeng, Xiao, Jimin, Wei, Yunchao, Sun, Mingjie, Huang, Kaizhu</br>
  Weakly supervised semantic <font color="#be00be">segmentation</font> is a challenging task as it only takes image-level information as supervision for training but produces pixel-level predictions for testing. To address such a challenging task, most recent <font color="red">state-of-the-art</font> approaches propose to adopt two-step solutions, \\emph{i.e. } 1) learn to generate pseudo pixel-level masks, and 2) engage FCNs to train the semantic segmentation networks with the pseudo masks. However, the two-step solutions usually employ many bells and whistles in producing high-quality pseudo masks, making this kind of methods complicated and inelegant. In this work, we harness the image-level labels to produce reliable pixel-level annotations and design a fully end-to-end network to learn to predict segmentation maps. Concretely, we firstly leverage an image classification branch to generate class activation maps for the annotated categories, which are further pruned into confident yet tiny object/background regions. Such reliable regions are then directly served as ground-truth labels for the parallel segmentation branch, where a newly designed dense energy loss function is adopted for optimization. Despite its apparent simplicity, our one-step solution achieves <font color="#960096">competitive</font> mIoU scores (\\emph{val}: 62.6, \\emph{test}: 62.9) on Pascal VOC compared with those two-step state-of-the-arts. By extending our one-step method to two-step, we get a new state-of-the-art performance on the Pascal VOC (\\emph{val}: 66.3, \\emph{test}: 66.5). </br></br>

<a href='http://arxiv.org/pdf/1911.06953.pdf'>1911.06953</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.4319баллов, №246</br>
<b>Dynamic Instance Normalization for Arbitrary Style Transfer</b></br>
Authors: , Jing, Yongcheng, Liu, Xiao, Ding, Yukang, Wang, Xinchao, Ding, Errui, Song, Mingli, Wen, Shilei</br>
  Prior normalization methods rely on affine transformations to produce arbitrary image <font color="#be00be">style</font> transfers, of which the parameters are computed in a pre-defined way. Such manually-defined nature eventually results in the high-cost and shared encoders for both style and content encoding, making style transfer systems cumbersome to be deployed in resource-constrained environments like on the mobile-terminal side. In this paper, we propose a new and generalized normalization module, termed as Dynamic Instance Normalization (DIN), that allows for flexible and more efficient arbitrary style transfers. Comprising an instance normalization and a dynamic convolution, DIN encodes a style image into learnable convolution parameters, upon which the content image is stylized. Unlike conventional methods that use shared complex encoders to encode content and style, the proposed DIN introduces a sophisticated style encoder, yet comes with a compact and <font color="#be00be">lightweight</font> content encoder for fast inference. Experimental results demonstrate that the proposed approach yields very encouraging results on challenging style patterns and, to our best knowledge, for the first time enables an arbitrary style transfer using MobileNet-based lightweight architecture, leading to a reduction factor of more than twenty in computational cost as compared to existing approaches. Furthermore, the proposed DIN provides flexible support for <font color="red">state-of-the-art</font> convolutional operations, and thus triggers novel functionalities, such as uniform-stroke placement for non-natural images and automatic spatial-stroke control. </br></br>

<a href='http://arxiv.org/pdf/1911.05889.pdf'>1911.05889</a> &nbsp&nbsp (cs:AI, cs:CL) &nbsp&nbsp 0.4283баллов, №247</br>
<b>Generating Persona Consistent Dialogues by Exploiting Natural Language\n  Inference</b></br>
Authors: , Song, Haoyu, Zhang, Wei-Nan, Hu, Jingwen, Liu, Ting</br>
  Consistency is one of the major challenges faced by dialogue agents. A human-like dialogue agent should not only respond naturally, but also maintain a consistent persona. In this paper, we exploit the advantages of natural language inference (NLI) technique to address the issue of generating persona consistent dialogues. Different from existing work that re-ranks the retrieved responses through an NLI model, we cast the task as a <font color="#00be00">reinforcement learning</font> problem and propose to exploit the NLI signals from response-persona pairs as rewards for the process of dialogue generation. Specifically, our generator employs an attention-based encoder-decoder to generate persona-based responses. Our evaluator consists of two components: an adversarially trained naturalness module and an NLI based consistency module. Moreover, we use another well-performed NLI model in the evaluation of persona-consistency. Experimental results on both human and automatic metrics, including the model-based consistency evaluation, demonstrate that the proposed approach <font color="#00be00">outperform</font>s strong generative baselines, especially in the persona-consistency of generated responses. </br></br>

<a href='http://arxiv.org/pdf/1911.07389.pdf'>1911.07389</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.4241баллов, №248</br>
<b>Towards Visually Explaining Variational Autoencoders</b></br>
Authors: , Liu, Wenqian, Li, Runze, Zheng, Meng, Karanam, Srikrishna, Wu, Ziyan, Bhanu, Bir, Radke, Richard J., Camps, Octavia</br>
  Recent advances in Convolutional Neural Network (CNN) model <font color="#be00be">interpret</font>ability have led to impressive progress in visualizing and understanding model predictions. In particular, gradient-based visual attention methods have driven much recent effort in using visual attention maps as a means for visual explanations. A key problem, however, is these methods are designed for classification and categorization tasks, and their extension to explaining generative models, \\eg, variational autoencoders (VAE) is not trivial. In this work, we take a step towards bridging this crucial gap, proposing the first technique to visually explain VAEs by means of gradient-based attention. We present methods to generate visual attention from the learned latent space, and also demonstrate such attention explanations serve more than just explaining VAE predictions. We show how these attention maps can be used to localize <font color="#be00be">anomal</font>ies in images, demonstrating <font color="red">state-of-the-art</font> performance on the MVTec-AD dataset. We also show how they can be infused into model training, helping bootstrap the VAE into learning improved latent space disentanglement, demonstrated on the Dsprites dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.07971.pdf'>1911.07971</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.4176баллов, №249</br>
<b>vqSGD: Vector Quantized Stochastic Gradient Descent</b></br>
Authors: , Gandikota, Venkata, Maity, Raj Kumar, Mazumdar, Arya</br>
  In this work, we present a family of vector quantization schemes vqSGD (Vector-Quantized Stochastic Gradient Descent) that provide asymptotic reduction in the communication cost with convergence guarantees in distributed computation and learning settings. In particular, we consider a randomized scheme, based on convex hull of a point set, that returns an unbiased estimator of a d-dimensional gradient vector with bounded variance. We provide multiple efficient instances of our scheme that require only O(logd) bits of communication. Further, we show that vqSGD also provides strong <font color="#be00be">privacy</font> guarantees. Experimentally, we show vqSGD performs equally well compared to other <font color="red">state-of-the-art</font> quantization schemes, while substantially reducing the communication cost. </br></br>

<a href='http://arxiv.org/pdf/1911.07729.pdf'>1911.07729</a> &nbsp&nbsp (cs:NE, cs:ML, stat:ML) &nbsp&nbsp 0.4143баллов, №250</br>
<b>ImmuNeCS: Neural Committee Search by an Artificial Immune System</b></br>
Authors: , Frachon, Luc, Pang, Wei, Coghill, George M.</br>
  Current Neural <font color="#00be00">Architecture Search</font> techniques can suffer from a few shortcomings, including high computational cost, excessive bias from the search space, conceptual complexity or uncertain empirical benefits over random search. In this paper, we present ImmuNeCS, an attempt at addressing these issues with a method that offers a simple, flexible, and efficient way of building deep learning models automatically, and we demonstrate its effectiveness in the context of convolutional neural networks. Instead of searching for the 1-best architecture for a given task, we focus on building a population of neural networks that are then ensembled into a neural network committee, an approach we dub \'Neural Committee Search\'. To ensure sufficient performance from the committee, our search algorithm is based on an artificial immune system that balances individual performance with population diversity. This allows us to stop the search when accuracy starts to plateau, and to bridge the performance gap through ensembling. In order to justify our method, we first verify that the chosen search space exhibits the locality property. To further improve efficiency, we also combine partial evaluation, weight inheritance, and progressive search. First, experiments are run to verify the validity of these techniques. Then, preliminary experimental results on two popular computer vision benchmarks show that our method consistently <font color="#00be00">outperform</font>s random search and yields promising results within reasonable GPU budgets. An additional experiment also shows that ImmuNeCS\'s solutions transfer effectively to a more difficult task, where they achieve results comparable to a direct search on the new task. We believe these findings can open the way for new, accessible alternatives to traditional NAS. </br></br>

<a href='http://arxiv.org/pdf/1911.07806.pdf'>1911.07806</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.4106баллов, №251</br>
<b>Action Anticipation with RBF Kernelized Feature Mapping RNN</b></br>
Authors: , Shi, Yuge, Fernando, Basura, Hartley, Richard</br>
  We introduce a novel Recurrent Neural Network-based algorithm for future video feature generation and action anticipation called feature mapping RNN. Our novel RNN architecture builds upon three effective principles of machine learning, namely parameter sharing, Radial Basis Function <font color="blue">kernel</font>s and adversarial training. Using only some of the earliest frames of a video, the feature mapping RNN is able to generate future features with a fraction of the parameters needed in traditional RNN. By feeding these future features into a simple multi-layer perceptron facilitated with an RBF kernel layer, we are able to accurately predict the action in the video. In our experiments, we obtain 18% improvement on JHMDB-21 dataset, 6% on UCF101-24 and 13% improvement on UT-Interaction datasets over prior <font color="red">state-of-the-art</font> for action anticipation. </br></br>

<a href='http://arxiv.org/pdf/1911.07033.pdf'>1911.07033</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.4049баллов, №252</br>
<b>S2DNAS: Transforming Static CNN Model for Dynamic Inference via Neural\n  Architecture Search</b></br>
Authors: , Yuan, Zhihang, Wu, Bingzhe, Liang, Zheng, Zhao, Shiwan, Bi, Weichen, Sun, Guangyu</br>
  Recently, dynamic inference has emerged as a promising way to reduce the computational cost of deep convolutional neural networks (CNNs). In contrast to static methods (e.g., weight pruning), dynamic inference adaptively adjusts the inference process according to each input sample, which can considerably reduce the computational cost on &quot;easy&quot; samples while maintaining the overall model performance. In this paper, we introduce a general framework, S2DNAS, which can transform various static CNN models to support dynamic inference via neural <font color="#00be00">architecture search</font>. To this end, based on a given CNN model, we first generate a CNN architecture space in which each architecture is a multi-stage CNN generated from the given model using some predefined transformations. Then, we propose a <font color="#00be00">reinforcement learning</font> based approach to automatically search for the optimal CNN architecture in the generated space. At last, with the searched multi-stage network, we can perform dynamic inference by adaptively choosing a stage to evaluate for each sample. Unlike previous works that introduce irregular computations or complex controllers in the inference or re-design a CNN model from scratch, our method can generalize to most of the popular CNN architectures and the searched dynamic network can be directly deployed using existing deep learning frameworks in various hardware devices. </br></br>

<a href='http://arxiv.org/pdf/1911.08842.pdf'>1911.08842</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp 0.4033баллов, №253</br>
<b>Neural Approximate Dynamic Programming for On-Demand Ride-Pooling</b></br>
Authors: , Shah, Sanket, Lowalekar, Meghna, Varakantham, Pradeep</br>
  On-demand ride-pooling (e.g., UberPool) has recently become popular because of its ability to lower costs for passengers while simultaneously increasing revenue for drivers and aggregation companies. Unlike in Taxi on Demand (ToD) services -- where a vehicle is only assigned one passenger at a time -- in on-demand ride-pooling, each (possibly partially filled) vehicle can be assigned a group of passenger requests with multiple different origin and destination pairs. To ensure near real-time response, existing solutions to the real-time ride-pooling problem are myopic in that they optimise the objective (e.g., maximise the number of passengers served) for the current time step without considering its effect on future assignments. This is because even a myopic assignment in ride-pooling involves considering what combinations of passenger requests that can be assigned to vehicles, which adds a layer of combinatorial complexity to the ToD problem.   A popular approach that addresses the limitations of myopic assignments in ToD problems is Approximate Dynamic Programming (ADP). Existing ADP methods for ToD can only handle Linear Program (LP) based assignments, however, while the assignment problem in ride-pooling requires an Integer Linear Program (ILP) with bad LP relaxations. To this end, our key technical contribution is in providing a general ADP method that can learn from ILP-based assignments. Additionally, we handle the extra combinatorial complexity from combinations of passenger requests by using a Neural Network based approximate value function and show a connection to Deep <font color="#00be00">Reinforcement Learning</font> that allows us to learn this value-function with increased stability and sample-efficiency. We show that our approach <font color="#00be00">outperform</font>s past approaches on a <font color="#009600">real-world</font> dataset by up to 16%, a significant improvement in city-scale transportation problems. </br></br>

<a href='http://arxiv.org/pdf/1911.07921.pdf'>1911.07921</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.3967баллов, №254</br>
<b>Privacy Leakage Avoidance with Switching Ensembles</b></br>
Authors: , Izmailov, Rauf, Lin, Peter, Mesterharm, Chris, Basu, Samyadeep</br>
  We consider membership inference attacks, one of the main <font color="#be00be">privacy</font> issues in machine learning. These recently developed attacks have been proven successful in determining, with confidence better than a random guess, whether a given sample belongs to the dataset on which the attacked machine learning model was trained. Several approaches have been developed to mitigate this privacy leakage but the tradeoff performance implications of these defensive mechanisms (i.e., accuracy and utility of the defended machine learning model) are not well studied yet. We propose a novel approach of privacy leakage avoidance with switching ensembles (PASE), which both protects against current membership inference attacks and does that with very small accuracy penalty, while requiring acceptable increase in training and inference time. We test our PASE method, along with the the current <font color="red">state-of-the-art</font> PATE approach, on three calibration image datasets and analyze their tradeoffs. </br></br>

<a href='http://arxiv.org/pdf/1911.09290.pdf'>1911.09290</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp 0.3964баллов, №255</br>
<b>Large-scale Multi-view Subspace Clustering in Linear Time</b></br>
Authors: , Kang, Zhao, Zhou, Wangtao, Zhao, Zhitong, Shao, Junming, Han, Meng, Xu, Zenglin</br>
  A plethora of multi-view subspace <font color="#be00be">clustering</font> (MVSC) methods have been proposed over the past few years. Researchers manage to boost clustering accuracy from different points of view. However, many <font color="red">state-of-the-art</font> MVSC algorithms, typically have a quadratic or even cubic complexity, are inefficient and inherently difficult to apply at large scales. In the era of big data, the computational issue becomes critical. To fill this gap, we propose a large-scale MVSC (LMVSC) algorithm with linear order complexity. Inspired by the idea of anchor graph, we first learn a smaller graph for each view. Then, a novel approach is designed to integrate those graphs so that we can implement spectral clustering on a smaller graph. Interestingly, it turns out that our model also applies to single-view scenario. Extensive experiments on various large-scale benchmark data sets validate the effectiveness and efficiency of our approach with respect to state-of-the-art clustering methods. </br></br>

<a href='http://arxiv.org/pdf/1911.09179.pdf'>1911.09179</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 0.3917баллов, №256</br>
<b>Scalable and Generalizable Social Bot Detection through Data Selection</b></br>
Authors: , Yang, Kai-Cheng, Varol, Onur, Hui, Pik-Mai, Menczer, Filippo</br>
  Efficient and reliable social bot classification is crucial for detecting information manipulation on social media. Despite rapid development, <font color="red">state-of-the-art</font> bot detection models still<font color="#be00be"> face </font>generalization and scalability challenges, which greatly limit their applications. In this paper we propose a framework that uses minimal account metadata, enabling efficient analysis that scales up to handle the full stream of public tweets of Twitter in real time. To ensure model accuracy, we build a rich collection of labeled datasets for training and validation. We deploy a strict validation system so that model performance on unseen datasets is also optimized, in addition to traditional cross-validation. We find that strategically selecting a subset of training data yields better model accuracy and generalization than exhaustively training on all available data. Thanks to the simplicity of the proposed model, its logic can be <font color="#be00be">interpret</font>ed to provide insights into social bot characteristics. </br></br>

<a href='http://arxiv.org/pdf/1911.09117.pdf'>1911.09117</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 0.3895баллов, №257</br>
<b>Automatic Differentiable Monte Carlo: Theory and Application</b></br>
Authors: , Zhang, Shi-Xin, Wan, Zhou-Quan, Yao, Hong</br>
  Differentiable programming has emerged as a key programming paradigm empowering rapid developments of deep learning while its applications to important computational methods such as Monte Carlo remain largely unexplored. Here we present the general <font color="blue">theor</font>y enabling infinite-order automatic differentiation on expectations computed by Monte Carlo with unnormalized probability distributions, which we call &quot;automatic differentiable Monte Carlo&quot; (ADMC). By implementing ADMC algorithms on computational graphs, one can also leverage <font color="red">state-of-the-art</font> machine learning frameworks and techniques to traditional Monte Carlo applications in statistics and physics. We illustrate the versatility of ADMC by showing some applications: fast search of phase transitions and accurately finding ground states of interacting many-body models in two dimensions. ADMC paves a promising way to innovate Monte Carlo in various aspects to achieve higher accuracy and efficiency, e.g. easing or solving the sign problem of quantum many-body models through ADMC. </br></br>

<a href='http://arxiv.org/pdf/1911.06502.pdf'>1911.06502</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.3864баллов, №258</br>
<b>Simple iterative method for generating targeted universal adversarial\n  perturbations</b></br>
Authors: , Hirano, Hokuto, Takemoto, Kazuhiro</br>
  Deep neural networks (DNNs) are vulnerable to <font color="blue">adversarial att</font>acks. In particular, a single perturbation known as the universal adversarial perturbation (UAP) can foil most classification tasks conducted by DNNs. Thus, different methods for generating UAPs are required to fully evaluate the vulnerability of DNNs. A realistic evaluation would be with cases that consider targeted attacks; wherein the generated UAP causes DNN to classify an input into a specific class. However, the development of UAPs for targeted attacks has largely fallen behind that of UAPs for non-targeted attacks. Therefore, we propose a simple iterative method to generate UAPs for targeted attacks. Our method combines the simple iterative method for generating non-targeted UAPs and the fast gradient sign method for generating a targeted adversarial perturbation for an input. We applied the proposed method to <font color="red">state-of-the-art</font> DNN models for image classification and proved the existence of almost imperceptible UAPs for targeted attacks; further, we demonstrated that such UAPs are easily generatable. </br></br>

<a href='http://arxiv.org/pdf/1911.06904.pdf'>1911.06904</a> &nbsp&nbsp (cs:AI, cs:ML) &nbsp&nbsp 0.3857баллов, №259</br>
<b>Improving Graph Neural Network Representations of Logical Formulae with\n  Subgraph Pooling</b></br>
Authors: , Crouse, Maxwell, Abdelaziz, Ibrahim, Cornelio, Cristina, Thost, Veronika, Wu, Lingfei, Forbus, Kenneth, Fokoue, Achille</br>
  Recent advances in the integration of deep learning with automated <font color="blue">theor</font>em proving have centered around the representation of logical formulae as inputs to deep learning systems. In particular, there has been a shift from character and token-level representations to graph-structured representations, in large part driven by the rapidly emerging body of research on geometric deep learning. Typically, structure-aware neural methods for embedding logical formulae have been variants of either Tree LSTMs or GNNs. While more effective than character and token-level approaches, such methods have often made representational trade-offs that limited their ability to effectively represent the global structure of their inputs. In this work, we introduce a novel approach for embedding logical formulae using DAG LSTMs that is designed to overcome the limitations of both Tree LSTMs and GNNs. The effectiveness of the proposed framework is demonstrated on the tasks of premise selection and proof step classification where it achieves the <font color="red">state-of-the-art</font> performance on two standard datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.07482.pdf'>1911.07482</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp 0.3835баллов, №260</br>
<b>Object Finding in Cluttered Scenes Using Interactive Perception</b></br>
Authors: , Novkovic, Tonci, Pautrat, Remi, Furrer, Fadri, Breyer, Michel, Siegwart, Roland, Nieto, Juan</br>
  Object finding in clutter is a skill that requires both perception of the environment and in many cases physical interaction. In robotics, interactive perception defines a set of algorithms that leverage actions to improve the perception of the environment, and vice versa use perception to guide the next action. Scene interactions are difficult to model, therefore, most of the current systems use predefined heuristics. This limits their ability to efficiently search for the target object in a complex environment. In order to remove heuristics and the need for explicit models of the interactions, in this work we propose a <font color="#00be00">reinforcement learning</font> based active and interactive perception system for scene exploration and object search. We evaluate our work both in simulated and in <font color="#009600">real world</font> experiments using a robotic manipulator equipped with an RGB and a depth camera, and compared our system to two baselines. The results indicate that our approach, trained in simulation only, transfers smoothly to reality and can solve the object finding task efficiently and with more than 90% success rate. </br></br>

<a href='http://arxiv.org/pdf/1911.07472.pdf'>1911.07472</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.3795баллов, №261</br>
<b>Learning to Synthesize Fashion Textures</b></br>
Authors: , Shi, Wu, Hui, Tak-Wai, Liu, Ziwei, Lin, Dahua, Loy, Chen Change</br>
  Existing unconditional generative models mainly focus on modeling general objects, such as faces and indoor scenes. Fashion textures, another important type of visual elements around us, have not been extensively studied. In this work, we propose an effective generative model for fashion textures and also comprehensively investigate the key components involved: internal representation, latent space sampling and the generator architecture. We use Gram matrix as a suitable internal representation for modeling realistic fashion textures, and further design two dedicated modules for modulating Gram matrix into a low-dimension vector. Since fashion textures are scale-dependent, we propose a recursive auto-encoder to capture the dependency between multiple granularity levels of texture feature. Another important observation is that fashion textures are multi-modal. We fit and sample from a <font color="blue">Gaussi</font>an mixture model in the latent space to improve the diversity of the generated textures. Extensive experiments demonstrate that our approach is capable of synthesizing more realistic and diverse fashion textures over other <font color="red">state-of-the-art</font> methods. </br></br>

<a href='http://arxiv.org/pdf/1911.08916.pdf'>1911.08916</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.3788баллов, №262</br>
<b>Deep Learning based HEp-2 Image Classification: A Comprehensive Review</b></br>
Authors: , Rahman, Saimunur, Wang, Lei, Sun, Changming, Zhou, Luping</br>
  Classification of HEp-2 cell patterns plays a significant role in the indirect immunofluorescence test for identifying autoimmune <font color="blue">diseas</font>es in the human body. Many automatic HEp-2 cell classification methods have been proposed in recent years, amongst which deep learning based methods have shown impressive performance. This paper provides a comprehensive review of the existing deep learning based HEp-2 cell image classification methods. These methods perform HEp-2 image classification in two levels, namely, cell-level and specimen-level. Both levels are covered in this review. In each level, the methods are organized with a deep network usage based taxonomy. The core idea, notable achievements, and key advantages and weakness of each method are critically analyzed. Furthermore, a concise review of the existing HEp-2 datasets that are commonly used in the literature is given. The paper ends with an overview of the current <font color="red">state-of-the-art</font>s and a discussion on novel opportunities and future research directions in this field. It is hoped that this paper would give readers a comprehensive reference of this novel, challenging, and thriving field. </br></br>

<a href='http://arxiv.org/pdf/1911.07712.pdf'>1911.07712</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp 0.3781баллов, №263</br>
<b>Inducing Cooperation via Team Regret Minimization based Multi-Agent Deep\n  Reinforcement Learning</b></br>
Authors: , Yu, Runsheng, Shi, Zhenyu, Wang, Xinrun, Wang, Rundong, Liu, Buhong, Hou, Xinwen, Lai, Hanjiang, An, Bo</br>
  Existing value-factorized based Multi-Agent deep Reinforce-ment Learning (MARL) approaches are well-performing invarious multi-agent cooperative environment under thecen-tralized training and decentralized execution(CTDE) scheme,where all agents are trained together by the centralized valuenetwork and each agent execute its policy independently. How-ever, an issue remains open: in the centralized training process,when the environment for the team is partially observable ornon-stationary, i.e., the observation and action informationof all the agents cannot represent the global states, existingmethods perform poorly and sample inefficiently. Regret Min-imization (RM) can be a promising approach as it performswell in partially observable and fully <font color="#960096">competitive</font> settings.However, it tends to model others as opponents and thus can-not work well under the CTDE scheme. In this work, wepropose a novel team RM based <font color="blue">Bayes</font>ian MARL with threekey contributions: (a) we design a novel RM method to traincooperative agents as a team and obtain a team regret-basedpolicy for that team; (b) we introduce a novel method to de-compose the team regret to generate the policy for each agentfor decentralized execution; (c) to further improve the perfor-mance, we leverage a differential particle filter (a SequentialMonte Carlo method) network to get an accurate estimation ofthe state for each agent. Experimental results on two-step ma-trix games (cooperative game) and battle games (large-scalemixed cooperative-competitive games) demonstrate that ouralgorithm significantly <font color="#00be00">outperform</font>s <font color="red">state-of-the-art</font> methods. </br></br>

<a href='http://arxiv.org/pdf/1911.09224.pdf'>1911.09224</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.3756баллов, №264</br>
<b>FLNet: Landmark Driven Fetching and Learning Network for Faithful\n  Talking Facial Animation Synthesis</b></br>
Authors: , Gu, Kuangxiao, Zhou, Yuqian, Huang, Thomas</br>
  Talking<font color="#be00be"> face </font>synthesis has been widely studied in either appearance-based or warping-based methods. Previous works mostly utilize single face image as a source, and generate novel<font color="#be00be"> facial </font>animations by merging other person\'s facial features. However, some facial regions like eyes or teeth, which may be hidden in the source image, can not be synthesized faithfully and stably. In this paper, We present a landmark driven two-stream network to generate faithful talking facial animation, in which more facial details are created, preserved and transferred from multiple source images instead of a single one. Specifically, we propose a network consisting of a learning and fetching stream. The fetching sub-net directly learns to attentively warp and merge facial regions from five source images of distinctive landmarks, while the learning pipeline renders facial organs from the training face space to compensate. Compared to baseline algorithms, extensive experiments demonstrate that the proposed method achieves a higher performance both quantitatively and qualitatively. Codes are at <font color="#006400">http</font>s://<font color="red">github</font>.com/kgu3/FLNet_AAAI2020. </br></br>

<a href='http://arxiv.org/pdf/1911.09153.pdf'>1911.09153</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp 0.3695баллов, №265</br>
<b>Gradient-based Optimization for Bayesian Preference Elicitation</b></br>
Authors: , Vendrov, Ivan, Lu, Tyler, Huang, Qingqing, Boutilier, Craig</br>
  Effective techniques for eliciting user preferences have taken on added importance as recommender systems (RSs) become increasingly interactive and conversational. A common and conceptually appealing <font color="blue">Bayes</font>ian criterion for selecting queries is expected value of information (EVOI). Unfortunately, it is computationally prohibitive to construct queries with maximum EVOI in RSs with large item spaces. We tackle this issue by introducing a continuous formulation of EVOI as a differentiable network that can be optimized using gradient methods available in modern machine learning (ML) computational frameworks (e.g., TensorFlow, PyTorch). We exploit this to develop a novel, scalable Monte Carlo method for EVOI optimization, which is more scalable for large item spaces than methods requiring explicit enumeration of items. While we emphasize the use of this approach for pairwise (or k-wise) comparisons of items, we also demonstrate how our method can be adapted to queries involving subsets of item attributes or &quot;partial items,&quot; which are often more cognitively manageable for users. Experiments show that our gradient-based EVOI technique achieves <font color="red">state-of-the-art</font> performance across several domains while scaling to large item spaces. </br></br>

<a href='http://arxiv.org/pdf/1911.08914.pdf'>1911.08914</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 0.3530баллов, №266</br>
<b>Nonconvex Nonsmooth Low-Rank Minimization for Generalized Image\n  Compressed Sensing via Group Sparse Representation</b></br>
Authors: , Li, Yunyi, Liu, Li, Zhao, Yu, Cheng, Xiefeng, Gui, Guan</br>
  Group sparse representation (GSR) based method has led to great successes in various image recovery tasks, which can be converted into a low-rank matrix minimization problem. As a widely used surrogate function of low-rank, the nuclear norm based convex surrogate usually leads to over-shrinking problem, since the standard soft-thresholding operator shrinks all singular values equally. To improve traditional sparse representation based image compressive sensing (CS) performance, we propose a generalized CS framework based on GSR model, leading to a nonconvex nonsmooth low-rank minimization problem. The popular L_2-norm and M-estimator are employed for standard image CS and robust CS problem to fit the data respectively. For the better approximation of the rank of group-matrix, a family of nuclear norms are employed to address the over-shrinking problem. Moreover, we also propose a flexible and effective iteratively-weighting strategy to control the weighting and contribution of each singular value. Then we develop an iteratively reweighted nuclear norm algorithm for our generalized framework via an alternating direction method of multipliers framework, namely, GSR-ADMM-IRNN. Experimental results demonstrate that our proposed CS framework can achieve favorable reconstruction performance compared with current <font color="red">state-of-the-art</font> methods and the RCS framework can suppress the <font color="#be00be">outlier</font>s effectively. </br></br>

<a href='http://arxiv.org/pdf/1911.07654.pdf'>1911.07654</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.3505баллов, №267</br>
<b>Deep Discriminative Fine-Tuning for Cancer Type Classification</b></br>
Authors: , Harley, Alena</br>
  Determining the primary site of origin for metastatic tumors is one of the open problems in <font color="#be00be">cancer</font> care because the efficacy of treatment often depends on the cancer tissue of origin. Classification methods that can leverage tumor genomic data and predict the site of origin are therefore of great value. Because tumor DNA point mutation data is very sparse, only limited accuracy (64.5% for 12 tumor classes) was previously demonstrated by methods that rely on point mutations as features (1). Tumor classification accuracy can be greatly improved (to over 90% for 33 classes) by relying on gene expression data (2). However, this additional data is often not readily available in <font color="blue">clinic</font>al setting, because point mutations are better profiled and targeted by clinical mutational profiling.   Here we sought to develop an accurate deep transfer learning and fine-tuning method for tumor sub-type classification, where predicted class is indicative of the primary site of origin. Our method significantly <font color="#00be00">outperform</font>s the <font color="red">state-of-the-art</font> for tumor classification using DNA point mutations, reducing the error by more than 30% at the same time discriminating over many more classes on The Cancer Genome Atlas (TCGA) dataset. Using our method, we achieve state-of-the-art tumor type classification accuracy of 78.3% for 29 tumor classes relying on DNA point mutations in the tumor only. </br></br>

<a href='http://arxiv.org/pdf/1911.09615.pdf'>1911.09615</a> &nbsp&nbsp (cs:ML, cs:NE, stat:ML) &nbsp&nbsp 0.3478баллов, №268</br>
<b>Sample-Efficient Reinforcement Learning with Maximum Entropy Mellowmax\n  Episodic Control</b></br>
Authors: , Sarrico, Marta, Arulkumaran, Kai, Agostinelli, Andrea, Richemond, Pierre, Bharath, Anil Anthony</br>
  Deep networks have enabled <font color="#00be00">reinforcement learning</font> to scale to more complex and challenging domains, but these methods typically require large quantities of training data. An alternative is to use sample-efficient episodic control methods: neuro-inspired algorithms which use non-/semi-parametric models that predict values based on storing and retrieving previously experienced transitions. One way to further improve the <font color="#00be00">sample efficien</font>cy of these approaches is to use more principled exploration strategies. In this work, we therefore propose maximum entropy mellowmax episodic control (MEMEC), which samples actions according to a <font color="blue">Boltzmann</font> policy with a state-dependent temperature. We demonstrate that MEMEC <font color="#00be00">outperform</font>s other uncertainty- and softmax-based exploration methods on classic reinforcement learning environments and Atari games, achieving both more rapid learning and higher final rewards. </br></br>

<a href='http://arxiv.org/pdf/1911.07158.pdf'>1911.07158</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.3452баллов, №269</br>
<b>Unsupervised Domain Adaptation for Object Detection via Cross-Domain\n  Semi-Supervised Learning</b></br>
Authors: , Yu, Fuxun, Wang, Di, Chen, Yinpeng, Karianakis, Nikolaos, Yu, Pei, Lymberopoulos, Dimitrios, Chen, Xiang</br>
  Current <font color="red">state-of-the-art</font> object detectors can have significant performance drop when deployed in the wild due to domain gaps with training data. Unsupervised Domain Adaptation (UDA) is a promising approach to adapt models for new domains/environments without any expensive label cost. However, without ground truth labels, most prior works on UDA for <font color="#be00be">object detection</font> tasks can only perform coarse image-level and/or feature-level adaptation by using adversarial learning methods. In this work, we show that such adversarial-based methods can only reduce the domain <font color="#be00be">style</font> gap, but cannot address the domain content distribution gap that is shown to be important for object detectors. To overcome this limitation, we propose the Cross-Domain Semi-Supervised Learning (CDSSL) framework by leveraging high-quality pseudo labels to learn better representations from the target domain directly. To enable SSL for cross-domain object detection, we propose fine-grained domain transfer, progressive-confidence-based label sharpening and imbalanced sampling strategy to address two challenges: (i) non-identical distribution between source and target domain data, (ii) error amplification/accumulation due to noisy pseudo labeling on the target domain. Experiment results show that our proposed approach consistently achieves new state-of-the-art performance (2.2% - 9.5% better than prior best work on mAP) under various domain gap scenarios. The code will be released. </br></br>

<a href='http://arxiv.org/pdf/1911.08479.pdf'>1911.08479</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.3450баллов, №270</br>
<b>Modal-aware Features for Multimodal Hashing</b></br>
Authors: , Zeng, Haien, Lai, Hanjiang, Chu, Hanlu, Tang, Yong, Yin, Jian</br>
  Many retrieval applications can benefit from multiple modalities, e.g., text that contains images on Wikipedia, for which how to represent multimodal data is the critical component. Most deep multimodal learning methods typically involve two steps to construct the joint representations: 1) learning of multiple intermediate features, with each intermediate feature corresponding to a modality, using separate and independent deep models; 2) merging the intermediate features into a joint representation using a fusion strategy. However, in the first step, these intermediate features do not have previous knowledge of each other and cannot fully exploit the information contained in the other modalities. In this paper, we present a modal-aware operation as a generic building block to capture the non-linear dependences among the heterogeneous intermediate features that can learn the underlying correlation structures in other multimodal data as soon as possible. The modal-aware operation consists of a <font color="blue">kernel</font> network and an attention network. The kernel network is utilized to learn the non-linear relationships with other modalities. Then, to learn better representations for binary hash codes, we present an attention network that finds the informative regions of these modal-aware features that are favorable for retrieval. Experiments conducted on three public benchmark datasets demonstrate significant improvements in the performance of our method relative to <font color="red">state-of-the-art</font> methods. </br></br>

<a href='http://arxiv.org/pdf/1911.08712.pdf'>1911.08712</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.3448баллов, №271</br>
<b>Instance-Invariant Adaptive Object Detection via Progressive\n  Disentanglement</b></br>
Authors: , Wu, Aming, Han, Yahong, Zhu, Linchao, Yang, Yi</br>
  Most <font color="red">state-of-the-art</font> methods of <font color="#be00be">object detection</font> suffer from poor generalization ability when the training and test data are from different domains, e.g., with different <font color="#be00be">style</font>s. To address this problem, previous methods mainly use holistic representations to align feature-level and pixel-level distributions of different domains, which may neglect the instance-level characteristics of objects in images. Besides, when transferring detection ability across different domains, it is important to obtain the instance-level features that are domain-invariant, instead of the styles that are domain-specific. Therefore, in order to extract instance-invariant features, we should disentangle the domain-invariant features from the domain-specific features. To this end, a progressive disentangled framework is first proposed to solve domain adaptive object detection. Particularly, base on disentangled learning used for feature decomposition, we devise two disentangled layers to decompose domain-invariant and domain-specific features. And the instance-invariant features are extracted based on the domain-invariant features. Finally, to enhance the disentanglement, a three-stage training mechanism including multiple loss functions is devised to optimize our model. In the experiment, we verify the effectiveness of our method on three domain-shift scenes. Our method is separately 2.3\\%, 3.6\\%, and 4.0\\% higher than the baseline method \\cite{saito2019strong}. </br></br>

<a href='http://arxiv.org/pdf/1911.07794.pdf'>1911.07794</a> &nbsp&nbsp (cs:ML, cs:AI) &nbsp&nbsp 0.3376баллов, №272</br>
<b>Gamma-Nets: Generalizing Value Estimation over Timescale</b></br>
Authors: , Sherstan, Craig, Dohare, Shibhansh, MacGlashan, James, G&#xfc;nther, Johannes, Pilarski, Patrick M.</br>
  We present $\\Gamma$-nets, a method for generalizing value function estimation over timescale. By using the timescale as one of the estimator\'s inputs we can estimate value for arbitrary timescales. As a result, the prediction target for any timescale is available and we are free to train on multiple timescales at each timestep. Here we empirically evaluate $\\Gamma$-nets in the policy evaluation setting. We first demonstrate the approach on a square wave and then on a robot arm using linear function approximation. Next, we consider the deep <font color="#00be00">reinforcement learning</font> setting using several Atari video games. Our results show that $\\Gamma$-nets can be effective for predicting arbitrary timescales, with only a small cost in accuracy as compared to learning estimators for fixed timescales. $\\Gamma$-nets provide a method for compactly making predictions at many timescales without requiring a priori knowledge of the task, making it a valuable contribution to ongoing work on model-based planning, representation learning, and <font color="#00be00">lifelong</font> learning algorithms. </br></br>

<a href='http://arxiv.org/pdf/1911.07537.pdf'>1911.07537</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.3374баллов, №273</br>
<b>Fast Machine Learning with Byzantine Workers and Servers</b></br>
Authors: , Mhamdi, El Mahdi El, Guerraoui, Rachid, Guirguis, Arsany</br>
  Machine Learning (ML) solutions are nowadays distributed and are prone to various types of component failures, which can be encompassed in so-called Byzantine behavior. This paper introduces LiuBei, a Byzantine-resilient ML algorithm that does not trust any individual component in the network (neither workers nor servers), nor does it induce additional communication rounds (on average), compared to standard non-Byzantine resilient algorithms. LiuBei builds upon gradient aggregation rules (GARs) to tolerate a minority of Byzantine workers. Besides, LiuBei replicates the parameter server on multiple machines instead of trusting it. We introduce a novel filtering mechanism that enables workers to filter out replies from Byzantine server replicas without requiring communication with all servers. Such a filtering mechanism is based on network synchrony, Lipschitz continuity of the loss function, and the GAR used to aggregate workers\' gradients. We also introduce a protocol, scatter/gather, to bound drifts between models on correct servers with a small number of communication messages. We <font color="blue">theor</font>etically prove that LiuBei achieves Byzantine resilience to both servers and workers and guarantees convergence. We build LiuBei using TensorFlow, and we show that LiuBei tolerates Byzantine behavior with an accuracy loss of around 5% and around 24% convergence overhead compared to vanilla TensorFlow. We moreover show that the throughput gain of LiuBei compared to another <font color="red">state-of-the-art</font> Byzantine-resilient ML algorithm (that assumes network asynchrony) is 70%. </br></br>

<a href='http://arxiv.org/pdf/1911.08797.pdf'>1911.08797</a> &nbsp&nbsp (cs:CV, cs:RO) &nbsp&nbsp 0.3285баллов, №274</br>
<b>You Are Here: Geolocation by Embedding Maps and Images</b></br>
Authors: , Abonce, Obed Samano, Zhou, Mengjie, Calway, Andrew</br>
  We present a novel approach to geolocating images on a 2-D map based on learning a low dimensional embedded space, which allows a comparison between an image captured at a location and local neighbourhoods of the map. The representation is not sufficiently discriminatory to allow localisation from a single image but when concatenated along a route, localisation converges quickly, with over 90% accuracy being achieved for routes up to 200m in length when using <font color="#00be00">Google</font> Street View and Open Street Map data. The approach generalises a previous fixed semantic feature based approach and achieves faster convergence and higher accuracy without the need for including turn information. </br></br>

<a href='http://arxiv.org/pdf/1910.04041.pdf'>1910.04041</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 0.3157баллов, №275</br>
<b>Hierarchical Deep Double Q-Routing</b></br>
Authors: , Ali, Ramy E., Erman, Bilgehan, Ba&#x15f;tu&#x11f;, Ejder, Cilli, Bruce</br>
  This paper explores a deep <font color="#00be00">reinforcement learning</font> approach applied to the packet routing problem with high-dimensional constraints instigated by dynamic and autonomous communication networks. Our approach is motivated by the fact that centralized path calculation approaches are often not scalable, whereas the distributed approaches with locally acting nodes are not fully aware of the end-to-end performance. We instead <font color="#00be00">hierarchical</font>ly distribute the path calculation over designated nodes in the network while taking into account the end-to-end performance. Specifically, we develop a hierarchical cluster-oriented adaptive per-flow path calculation mechanism by leveraging the Deep Double Q-network (DDQN) algorithm, where the end-to-end paths are calculated by the source nodes with the assistance of cluster (group) leaders at different hierarchical levels. In our approach, a deferred composite reward is designed to capture the end-to-end performance through a feedback signal from the source nodes to the group leaders and captures the local network performance through the local resource assessments by the group leaders. This approach scales in large networks, adapts to the dynamic demand, utilizes the network resources efficiently and can be applied to segment routing. </br></br>

<a href='http://arxiv.org/pdf/1911.08777.pdf'>1911.08777</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.3154баллов, №276</br>
<b>Hierarchical Attention Networks for Medical Image Segmentation</b></br>
Authors: , Ding, Fei, Yang, Gang, Liu, Jinlu, Wu, Jun, Ding, Dayong, Xv, Jie, Cheng, Gangwei, Li, Xirong</br>
  The <font color="blue">medic</font>al image is characterized by the inter-class indistinction, high variability, and noise, where the recognition of pixels is challenging. Unlike previous self-attention based methods that capture context information from one level, we reformulate the self-attention mechanism from the view of the high-order graph and propose a novel method, namely <font color="#00be00">Hierarchical</font> Attention Network (HANet), to address the problem of medical image <font color="#be00be">segmentation</font>. Concretely, an HA module embedded in the HANet captures context information from neighbors of multiple levels, where these neighbors are extracted from the high-order graph. In the high-order graph, there will be an edge between two nodes only if the correlation between them is high enough, which naturally reduces the noisy attention information caused by the inter-class indistinction. The proposed HA module is robust to the variance of input and can be flexibly inserted into the existing convolution neural networks. We conduct experiments on three medical image segmentation tasks including optic disc/cup segmentation, blood vessel segmentation, and lung segmentation. Extensive results show our method is more effective and robust than the existing <font color="red">state-of-the-art</font> methods. </br></br>

<a href='http://arxiv.org/pdf/1911.07262.pdf'>1911.07262</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.3078баллов, №277</br>
<b>Leveraging Multi-view Image Sets for Unsupervised Intrinsic Image\n  Decomposition and Highlight Separation</b></br>
Authors: , Yi, Renjiao, Tan, Ping, Lin, Stephen</br>
  We present an unsupervised approach for factorizing object appearance into highlight, shading, and albedo layers, trained by multi-view real images. To do so, we construct a multi-view dataset by collecting numerous <font color="#be00be">customer</font> product photos online, which exhibit large illumination variations that make them suitable for training of reflectance separation and can facilitate object-level decomposition. The main contribution of our approach is a proposed image representation based on local color distributions that allows training to be insensitive to the local misalignments of multi-view images. In addition, we present a new guidance cue for unsupervised training that exploits synergy between highlight separation and intrinsic image decomposition. Over a broad range of objects, our technique is shown to yield <font color="red">state-of-the-art</font> results for both of these tasks. </br></br>

<a href='http://arxiv.org/pdf/1911.07643.pdf'>1911.07643</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.3035баллов, №278</br>
<b>Influence-aware Memory for Deep Reinforcement Learning</b></br>
Authors: , de Castro, Miguel Suau, Congeduti, Elena, Starre, Rolf, Czechowski, Aleksander, Olihoek, Frans</br>
  Making the right decisions when some of the state variables are hidden, involves reasoning about all the possible states of the environment. An agent receiving only partial observations needs to infer the true values of these hidden variables based on the history of experiences. Recent deep <font color="#00be00">reinforcement learning</font> methods use recurrent models to keep track of past information. However, these models are sometimes expensive to train and have convergence difficulties, especially when dealing with high dimensional input spaces. Taking inspiration from influence-based abstraction, we show that effective policies can be learned in the presence of uncertainty by only memorizing a small subset of input variables. We also incorporate a mechanism in our network that learns to automatically choose the important pieces of information that need to be remembered. The results indicate that, by forcing the agent\'s internal memory to focus on the selected regions while treating the rest of the observable variables as Markovian, we can <font color="#00be00">outperform</font> ordinary recurrent architectures in situations where the amount of information that the agent needs to retain represents a small fraction of the entire observation input. The method also reduces training time and obtains better scores than methods that stack multiple observations to remove partial observability in domains where long-term memory is required. </br></br>

<a href='http://arxiv.org/pdf/1911.06446.pdf'>1911.06446</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.3026баллов, №279</br>
<b>CASTER: Predicting Drug Interactions with Chemical Substructure\n  Representation</b></br>
Authors: , Huang, Kexin, Xiao, Cao, Hoang, Trong Nghia, Glass, Lucas M., Sun, Jimeng</br>
  Adverse drug-drug interactions (DDIs) remain a leading cause of morbidity and mortality. Identifying potential DDIs during the drug design process is critical for <font color="blue">patient</font>s and society. Although several computational models have been proposed for DDI prediction, there are still limitations: (1) specialized design of drug representation for DDI predictions is lacking; (2) predictions are based on limited labelled data and do not generalize well to unseen drugs or DDIs; and (3) models are characterized by a large number of parameters, thus are hard to <font color="#be00be">interpret</font>. In this work, we develop a ChemicAl SubstrucTurE Representation (CASTER) framework that predicts DDIs given chemical structures of drugs.CASTER aims to mitigate these limitations via (1) a sequential pattern mining module rooted in the DDI mechanism to efficiently characterize functional sub-structures of drugs; (2) an auto-encoding module that leverages both labelled and unlabelled chemical structure data to improve predictive accuracy and generalizability; and (3) a dictionary learning module that explains the prediction via a small set of coefficients which measure the relevance of each input sub-structures to the DDI outcome. We evaluated CASTER on two <font color="#009600">real-world</font> DDI datasets and showed that it performed better than <font color="red">state-of-the-art</font> baselines and provided interpretable predictions. </br></br>

<a href='http://arxiv.org/pdf/1911.08340.pdf'>1911.08340</a> &nbsp&nbsp (cs:CL, cs:AI) &nbsp&nbsp 0.2801баллов, №280</br>
<b>Unsupervised Natural Question Answering with a Small Model</b></br>
Authors: , Andrews, Martin, Witteveen, Sam</br>
  The recent (2019-02) demonstration of the power of huge language models such as<font color="#00be00"> GPT</font>-2 to memorise the answers to factoid questions raises questions about the extent to which knowledge is being embedded directly within these large models. This short paper describes an architecture through which much smaller models can also answer such questions - by making use of \'raw\' external knowledge. The contribution of this work is that the methods presented here rely on unsupervised learning techniques, complementing the unsupervised training of the Language Model. The goal of this line of research is to be able to add knowledge explicitly, without extensive training. </br></br>

<a href='http://arxiv.org/pdf/1911.09318.pdf'>1911.09318</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.2591баллов, №281</br>
<b>Relation Network for Person Re-identification</b></br>
Authors: , Park, Hyunjong, Ham, Bumsub</br>
  Person <font color="blue">re-identification</font> (reID) aims at retrieving an image of the person of interest from a set of images typically captured by multiple cameras. Recent reID methods have shown that exploiting local features describing body parts, together with a global feature of a person image itself, gives robust feature representations, even in the case of missing body parts. However, using the individual part-level features directly, without considering relations between body parts, confuses differentiating identities of different persons having similar attributes in corresponding parts. To address this issue, we propose a new relation network for person reID that considers relations between individual body parts and the rest of them. Our model makes a single part-level feature incorporate partial information of other body parts as well, supporting it to be more discriminative. We also introduce a global contrastive pooling (GCP) method to obtain a global feature of a person image. We propose to use contrastive features for GCP to complement conventional max and averaging pooling techniques. We show that our model <font color="#00be00">outperform</font>s the <font color="red">state of the art</font> on the <font color="#be00be">Market</font>1501, DukeMTMC-reID and CUHK03 datasets, demonstrating the effectiveness of our approach on discriminative person representations. </br></br>

<a href='http://arxiv.org/pdf/1911.06993.pdf'>1911.06993</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.2568баллов, №282</br>
<b>Automatic Design of CNNs via Differentiable Neural Architecture Search\n  for PolSAR Image Classification</b></br>
Authors: , Dong, Hongwei, Zhang, Siyu, Zou, Bin, Zhang, Lamei</br>
  Convolutional neural networks (CNNs) have shown good performance in polarimetric synthetic aperture radar (PolSAR) image classification due to the automation of feature engineering. Excellent hand-crafted architectures of CNNs incorporated the wisdom of human experts, which is an important reason for CNN\'s success. However, the design of the architectures is a difficult problem, which needs a lot of professional knowledge as well as computational resources. Moreover, the architecture designed by hand might be suboptimal, because it is only one of thousands of unobserved but objective existed paths. Considering that the success of deep learning is largely due to its automation of the feature engineering process, how to design automatic <font color="#00be00">architecture search</font>ing methods to replace the hand-crafted ones is an interesting topic. In this paper, we explore the application of neural architecture search (NAS) in PolSAR area for the first time. Different from the utilization of existing NAS methods, we propose a differentiable architecture search (DAS) method which is customized for PolSAR classification. The proposed DAS is equipped with a PolSAR tailored search space and an improved <font color="#009600">one-shot</font> search strategy. By DAS, the weights parameters and architecture parameters (corresponds to the hyperparameters but not the topologies) can be optimized by stochastic gradient descent method during the training. The optimized architecture parameters should be transformed into corresponding CNN architecture and re-train to achieve high-precision PolSAR classification. In addition, complex-valued DAS is developed to take into account the characteristics of PolSAR images so as to further improve the performance. Experiments on three PolSAR benchmark datasets show that the CNNs obtained by searching have better classification performance than the hand-crafted ones. </br></br>

<a href='http://arxiv.org/pdf/1911.03594.pdf'>1911.03594</a> &nbsp&nbsp (cs:ML, cs:AI, cs:RO, stat:ML) &nbsp&nbsp 0.2549баллов, №283</br>
<b>Robo-PlaNet: Learning to Poke in a Day</b></br>
Authors: , Chevalier-Boisvert, Maxime, Alain, Guillaume, Golemo, Florian, Nowrouzezahrai, Derek</br>
  Recently, the Deep Planning Network (PlaNet) approach was introduced as a model-based <font color="#00be00">reinforcement learning</font> method that learns environment dynamics directly from pixel observations. This architecture is useful for learning tasks in which either the agent does not have access to meaningful states (like position/velocity of robotic joints) or where the observed states significantly deviate from the physical state of the agent (which is commonly the case in low-cost robots in the form of backlash or noisy joint readings). PlaNet, by design, interleaves phases of training the dynamics model with phases of collecting more data on the target environment, leading to long training times. In this work, we introduce Robo-PlaNet, an asynchronous version of PlaNet. This algorithm consistently reaches higher performance in the same amount of time, which we demonstrate in both a simulated and a real robotic experiment. </br></br>

<a href='http://arxiv.org/pdf/1911.07968.pdf'>1911.07968</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.2514баллов, №284</br>
<b>Improving the Robustness of Capsule Networks to Image Affine\n  Transformations</b></br>
Authors: , Gu, Jindong, Tresp, Volker</br>
  Convolutional neural networks (CNNs) achieve translational invariance using pooling operations, which do not maintain the spatial relationship in the learned representations. Hence, they cannot extrapolate their understanding of the geometric transformation of inputs. Recently, Capsule Networks (CapsNets) have been proposed to tackle this problem. In CapsNets, each entity is represented by a vector and routed to high-level entities by a dynamic routing algorithm. The CapsNets have been shown to be more robust than CNNs to affine transformations of inputs. However, there is still a huge gap between their performance on transformed inputs compared to untransformed versions. In this work, we first revisit the routing procedure by (un)rolling its forward and backward passes. Our investigation reveals that the routing procedure contributes neither to generalization ability nor to the affine robustness of the CapsNets.   Furthermore, we explore the limitations of capsule transformations and propose affine CapsNets (Aff-CapsNets) that are more robust to affine transformations. On our benchmark task where models are trained on the MNIST dataset and tested on the AffNIST dataset, our Aff-CapsNets improve the benchmark performance by a large margin (from 79\\% to 93.21\\%), without using a routing mechanism. We also demonstrate the superiority of Aff-CapsNets on a <font color="#009600">real-world</font> <font color="#00be00">Brain</font> Tumor Type classification task. </br></br>

<a href='http://arxiv.org/pdf/1911.07050.pdf'>1911.07050</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.2446баллов, №285</br>
<b>All-In-One: Facial Expression Transfer, Editing and Recognition Using A\n  Single Network</b></br>
Authors: , Ali, Kamran, Hughes, Charles E.</br>
  In this paper, we present a unified architecture known as Transfer-Editing and Recognition Generative Adversarial Network (TER-GAN) which can be used: 1. to transfer<font color="#be00be"> facial </font>expressions from one identity to another identity, known as Facial Expression Transfer (FET), 2. to transform the expression of a given image to a target expression, while preserving the identity of the image, known as Facial Expression Editing (FEE), and 3. to recognize the facial expression of a<font color="#be00be"> face </font>image, known as Facial Expression Recognition (FER). In TER-GAN, we combine the capabilities of generative models to generate synthetic images, while learning important information about the input images during the reconstruction process. More specifically, two encoders are used in TER-GAN to encode identity and expression information from two input images, and a synthetic expression image is generated by the decoder part of TER-GAN. To improve the feature disentanglement and extraction process, we also introduce a novel expression consistency loss and an identity consistency loss which exploit extra expression and identity information from generated images. Experimental results show that the proposed method can be used for efficient facial expression transfer, facial expression editing and facial expression recognition. In order to evaluate the proposed technique and to compare our results with <font color="red">state-of-the-art</font> methods, we have used the Oulu-CASIA dataset for our experiments. </br></br>

<a href='http://arxiv.org/pdf/1911.08747.pdf'>1911.08747</a> &nbsp&nbsp (cs:ML, cs:SD, stat:ML) &nbsp&nbsp 0.2387баллов, №286</br>
<b>CAT: CRF-based ASR Toolkit</b></br>
Authors: , An, Keyu, Xiang, Hongyu, Ou, Zhijian</br>
  In this paper, we present a new open source toolkit for automatic <font color="#be00be">speech recognition</font> (ASR), named CAT (CRF-based ASR Toolkit). A key feature of CAT is discriminative training in the framework of conditional random field (CRF), particularly with connectionist temporal classification (CTC) inspired state topology. CAT contains a full-fledged implementation of CTC-CRF and provides a complete workflow for CRF-based end-to-end speech recognition. Evaluation results on <font color="#be00be">Chinese</font> and English benchmarks such as Switchboard and Aishell show that CAT obtains the <font color="red">state-of-the-art</font> results among existing end-to-end models with less parameters, and is <font color="#960096">competitive</font> compared with the hybrid DNN-HMM models. Towards flexibility, we show that i-vector based speaker-adapted recognition and latency control mechanism can be explored easily and effectively in CAT. We hope CAT, especially the CRF-based framework and software, will be of broad interest to the community, and can be further explored and improved. </br></br>

<a href='http://arxiv.org/pdf/1911.09007.pdf'>1911.09007</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.2363баллов, №287</br>
<b>Exponential Family Graph Embeddings</b></br>
Authors: , &#xc7;elikkanat, Abdulkadir, Malliaros, Fragkiskos D.</br>
  Representing networks in a low dimensional latent space is a crucial task with many interesting applications in graph learning problems, such as link prediction and node classification. A widely applied network representation learning paradigm is based on the combination of random walks for sampling context nodes and the traditional \\textit{Skip-Gram} model to capture center-context node relationships. In this paper, we emphasize on exponential family distributions to capture rich interaction patterns between nodes in random walk sequences. We introduce the generic \\textit{exponential family graph embedding} model, that generalizes random walk-based network representation learning techniques to exponential family conditional distributions. We study three particular instances of this model, analyzing their properties and showing their relationship to existing unsupervised learning models. Our experimental evaluation on <font color="#009600">real-world</font> datasets demonstrates that the proposed techniques <font color="#00be00">outperform</font> well-known baseline methods in two downstream machine learning tasks. </br></br>

<a href='http://arxiv.org/pdf/1911.06845.pdf'>1911.06845</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.2335баллов, №288</br>
<b>Handwritten and Machine printed OCR for Geez Numbers Using Artificial\n  Neural Network</b></br>
Authors: , Beyene, Eyob Gebretinsae</br>
  Researches have been done on Ethiopic scripts. However studies excluded the Geez numbers from the studies because of different reasons. This paper presents offline handwritten and machine printed Geez number recognition using feed forward back propagation artificial neural network. On this study, different Geez image characters were collected from <font color="#00be00">google</font> image search and three persons are instructed to write the numbers using pencil. In total we have collected 560 numbers of characters. We have used 460 of the characters for training and 100 are used for testing. Accordingly we have achieved overall all classification ~89:88% </br></br>

<a href='http://arxiv.org/pdf/1911.07932.pdf'>1911.07932</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.2331баллов, №289</br>
<b>Copy-Move Forgery Classification via Unsupervised Domain Adaptation</b></br>
Authors: , Kumar, Akash, Bhavsar, Arnav</br>
  In the current era, image manipulation is becoming increasingly easier, yielding more natural looking images, owing to the modern tools in image processing and computer vision techniques. The task of the segregation of forged images has become very challenging. To tackle such problems, <font color="#00be00">publicly available</font> datasets are insufficient. In this paper, we propose to create a synthetic forged dataset using deep semantic image inpainting algorithm. Furthermore, we use an unsupervised domain adaptation network to detect copy-move forgery in images. Our approach can be helpful in those cases, where the classification of data is unavailable. </br></br>

<a href='http://arxiv.org/pdf/1911.07224.pdf'>1911.07224</a> &nbsp&nbsp (cs:ML, cs:RO, stat:ML) &nbsp&nbsp 0.2327баллов, №290</br>
<b>Learning from Trajectories via Subgoal Discovery</b></br>
Authors: , Paul, Sujoy, van Baar, Jeroen, Roy-Chowdhury, Amit K.</br>
  Learning to solve complex goal-oriented tasks with sparse terminal-only rewards often requires an enormous number of samples. In such cases, using a set of expert trajectories could help to learn faster. However, Imitation Learning (IL) via supervised pre-training with these trajectories may not perform as well and generally requires additional finetuning with expert-in-the-loop. In this paper, we propose an approach which uses the expert trajectories and learns to decompose the complex main task into smaller sub-goals. We learn a function which partitions the state-space into sub-goals, which can then be used to design an extrinsic reward function. We follow a strategy where the agent first learns from the trajectories using IL and then switches to <font color="#00be00">Reinforcement Learning</font> (RL) using the identified sub-goals, to alleviate the errors in the IL step. To deal with states which are under-represented by the trajectory set, we also learn a function to modulate the sub-goal predictions. We show that our method is able to solve complex goal-oriented tasks, which other RL, IL or their combinations in literature are not able to solve. </br></br>

<a href='http://arxiv.org/pdf/1911.06815.pdf'>1911.06815</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 0.2322баллов, №291</br>
<b>Experiments in Detecting Persuasion Techniques in the News</b></br>
Authors: , Yu, Seunghak, Martino, Giovanni Da San, Nakov, Preslav</br>
  Many recent political events, like the 2016 US Presidential elections or the 2018 Brazilian elections have raised the attention of institutions and of the general public on the role of Internet and social media in influencing the outcome of these events. We argue that a safe democracy is one in which citizens have tools to make them aware of propaganda campaigns. We propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. We further design a novel multi-granularity neural network, and we show that it <font color="#00be00">outperform</font>s several strong BERT-based baselines. </br></br>

<a href='http://arxiv.org/pdf/1911.07893.pdf'>1911.07893</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp 0.2255баллов, №292</br>
<b>Temporal Knowledge Graph Embedding Model based on Additive Time Series\n  Decomposition</b></br>
Authors: , Xu, Chengjin, Nayyeri, Mojtaba, Alkhoury, Fouad, Lehmann, Jens, Yazdi, Hamed Shariat</br>
  <font color="#960096">Knowledge Graph</font> (KG) embedding has attracted more attention in recent years. Most of KG embedding models learn from time-unaware triples. However, the inclusion of temporal information beside triples would further improve the performance of a KGE model. In this regard, we propose ATiSE, a temporal KG embedding model which incorporates time information into entity/relation representations by using Additive Time Series decomposition. Moreover, considering the temporal uncertainty during the evolution of entity/relation representations over time, we map the representations of temporal KGs into the space of multi-dimensional <font color="blue">Gaussi</font>an distributions. The mean of each entity/relation embedding at a time step shows the current expected position, whereas its covariance (which is temporally stationary) represents its temporal uncertainty. Experimental results show that ATiSE not only achieves the <font color="red">state-of-the-art</font> on link prediction over temporal KGs, but also can predict the occurrence time of facts with missing time annotations, as well as the existence of future events. To the best of our knowledge, no other model is capable to perform all these tasks. </br></br>

<a href='http://arxiv.org/pdf/1911.08927.pdf'>1911.08927</a> &nbsp&nbsp (cs:RO, cs:AI, cs:ML) &nbsp&nbsp 0.2248баллов, №293</br>
<b>On Policy Learning Robust to Irreversible Events: An Application to\n  Robotic In-Hand Manipulation</b></br>
Authors: , Falco, Pietro, Attawia, Abdallah, Saveriano, Matteo, Lee, Dongheui</br>
  In this letter, we present an approach for learning in-hand manipulation skills with a low-cost, underactuated prosthetic hand in the presence of irreversible events. Our approach combines <font color="#00be00">reinforcement learning</font> based on visual perception with low-level reactive control based on tactile perception, which aims to avoid slipping. The objective of the reinforcement learning level consists not only in fulfilling the in-hand manipulation goal, but also in minimizing the intervention of the tactile reactive control. This way, the occurrence of object slipping during the learning procedure, which we consider an irreversible event, is significantly reduced. When an irreversible event occurs, the learning process is considered failed. We show the performance in two tasks, which consist in reorienting a cup and a bottle only using the fingers. The experimental results show that the proposed architecture allows reaching the goal in the Cartesian space and reduces significantly the occurrence of object slipping during the learning procedure. Moreover, without the proposed synergy between reactive control and reinforcement learning it was not possible to avoid irreversible events and, therefore, to learn the task. </br></br>

<a href='http://arxiv.org/pdf/1911.08862.pdf'>1911.08862</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.2221баллов, №294</br>
<b>D3S -- A Discriminative Single Shot Segmentation Tracker</b></br>
Authors: , Luke&#x17e;i&#x10d;, Alan, Matas, Ji&#x159;&#xed;, Kristan, Matej</br>
  Template-based discriminative <font color="#be00be">tracker</font>s are currently the dominant <font color="#be00be">tracking</font> paradigm due to their robustness, but are restricted to bounding box tracking and a limited range of transformation models, which reduces their localization accuracy. We propose a discriminative single-shot <font color="#be00be">segmentation</font> tracker - D3S, which narrows the gap between visual object tracking and video object segmentation. A single-shot network applies two target models with complementary geometric properties, one invariant to a broad range of transformations, including non-rigid deformations, the other assuming a rigid object to simultaneously achieve high robustness and online target segmentation. Without per-dataset finetuning and trained only for segmentation as the primary output, D3S <font color="#00be00">outperform</font>s all trackers on VOT2016, VOT2018 and GOT-10k benchmarks and performs close to the <font color="red">state-of-the-art</font> trackers on the TrackingNet. D3S outperforms the leading segmentation tracker SiamMask on video object segmentation benchmark and performs on par with top video object segmentation algorithms, while running an order of magnitude faster, close to real-time. </br></br>

<a href='http://arxiv.org/pdf/1911.06998.pdf'>1911.06998</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.2088баллов, №295</br>
<b>Revisiting Shadow Detection: A New Benchmark Dataset for Complex World</b></br>
Authors: , Hu, Xiaowei, Wang, Tianyu, Fu, Chi-Wing, Jiang, Yitong, Wang, Qiong, Heng, Pheng-Ann</br>
  Shadow detection in general photos is a nontrivial problem, due to the complexity of the <font color="#009600">real world</font>. Though recent shadow detectors have already achieved remarkable performance on various benchmark data, their performance is still limited for general <font color="#009600">real-world</font> situations. In this work, we collected shadow images for multiple scenarios and compiled a new dataset of 10,500 shadow images, each with labeled ground-truth mask, for supporting shadow detection in the complex world. Our dataset covers a rich variety of scene categories, with diverse shadow sizes, locations, contrasts, and types. Further, we comprehensively analyze the complexity of the dataset, present a fast shadow detection network with a detail enhancement module to harvest shadow details, and demonstrate the effectiveness of our method to detect shadows in general situations. </br></br>

<a href='http://arxiv.org/pdf/1910.00486.pdf'>1910.00486</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 0.2054баллов, №296</br>
<b>Dialogue Transformers</b></br>
Authors: , Vlasov, Vladimir, Mosig, Johannes E. M., Nichol, Alan</br>
  We introduce a dialogue policy based on a transformer architecture, where the self-attention mechanism operates over the sequence of dialogue turns. Recent work has used <font color="#00be00">hierarchical</font> recurrent neural networks to encode multiple utterances in a dialogue context, but we argue that a pure self-attention mechanism is more suitable. By default, an RNN assumes that every item in a sequence is relevant for producing an encoding of the full sequence, but a single conversation can consist of multiple overlapping discourse segments as speakers interleave multiple topics. A transformer picks which turns to include in its encoding of the current dialogue state, and is naturally suited to selectively ignoring or attending to dialogue history. We compare the performance of the Transformer Embedding Dialogue (TED) policy to an LSTM and to the REDP, which was specifically designed to overcome this limitation of RNNs. </br></br>

<a href='http://arxiv.org/pdf/1911.09098.pdf'>1911.09098</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.1983баллов, №297</br>
<b>AssemblyNet: A large ensemble of CNNs for 3D Whole Brain MRI\n  Segmentation</b></br>
Authors: , Coup&#xe9;, Pierrick, Mansencal, Boris, Cl&#xe9;ment, Micha&#xeb;l, Giraud, R&#xe9;mi, de Senneville, Baudouin Denis, Ta, Vinh-Thong, Lepetit, Vincent, Manjon, Jos&#xe9; V.</br>
  Whole <font color="#00be00">brain</font> <font color="#be00be">segmentation</font> using deep learning (DL) is a very challenging task since the number of anatomical labels is very high compared to the number of available training images. To address this problem, previous DL methods proposed to use a single convolution neural network (CNN) or few independent CNNs. In this paper, we present a novel ensemble method based on a large number of CNNs processing different overlapping brain areas. Inspired by parliamentary decision-making systems, we propose a framework called AssemblyNet, made of two &quot;assemblies&quot; of U-Nets. Such a parliamentary system is capable of dealing with complex decisions, unseen problem and reaching a consensus quickly. AssemblyNet introduces sharing of knowledge among neighboring U-Nets, an &quot;amendment&quot; procedure made by the second assembly at higher-resolution to refine the decision taken by the first one, and a final decision obtained by majority voting. During our validation, AssemblyNet showed <font color="#960096">competitive</font> performance compared to <font color="red">state-of-the-art</font> methods such as U-Net, Joint label fusion and SLANT. Moreover, we investigated the scan-rescan consistency and the robustness to <font color="blue">diseas</font>e effects of our method. These experiences demonstrated the reliability of AssemblyNet. Finally, we showed the interest of using semi-supervised learning to improve the performance of our method. </br></br>

<a href='http://arxiv.org/pdf/1911.08670.pdf'>1911.08670</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.1982баллов, №298</br>
<b>MMTM: Multimodal Transfer Module for CNN Fusion</b></br>
Authors: , Joze, Hamid Reza Vaezi, Shaban, Amirreza, Iuzzolino, Michael L., Koishida, Kazuhito</br>
  In late fusion, each modality is processed in a separate unimodal Convolutional Neural Network (CNN) stream and the scores of each modality are fused at the end. Due to its simplicity late fusion is still the predominant approach in many <font color="red">state-of-the-art</font> multimodal applications. In this paper, we present a simple neural network module for leveraging the knowledge from multiple modalities in convolutional neural networks. The propose unit, named Multimodal Transfer Module (MMTM), can be added at different levels of the feature hierarchy, enabling slow modality fusion. Using squeeze and excitation operations, MMTM utilizes the knowledge of multiple modalities to recalibrate the channel-wise features in each CNN stream. Despite other intermediate fusion methods, the proposed module could be used for feature modality fusion in convolution layers with different spatial dimensions. Another advantage of the proposed method is that it could be added among unimodal branches with minimum changes in the their network architectures, allowing each branch to be initialized with existing pretrained weights. Experimental results show that our framework improves the recognition accuracy of well-known multimodal networks. We demonstrate state-of-the-art or <font color="#960096">competitive</font> performance on four datasets that span the task domains of dynamic hand gesture recognition, <font color="#be00be">speech enhancement</font>, and action recognition with RGB and body joints. </br></br>

<a href='http://arxiv.org/pdf/1911.07132.pdf'>1911.07132</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.1964баллов, №299</br>
<b>Neural Recurrent Structure Search for Knowledge Graph Embedding</b></br>
Authors: , Zhang, Yongqi, Yao, Quanming, Chen, Lei</br>
  <font color="#960096">Knowledge graph</font> (KG) embedding is a fundamental problem in mining relational patterns. It aims to encode the entities and relations in KG into low dimensional vector space that can be used for subsequent algorithms. Lots of KG embedding models have been proposed to learn the interactions between entities and relations, which contain meaningful semantic information. However, structural information, which encodes local topology among entities, is also important to KG. In this work, we propose S2E to distill structural information and combine it with semantic information for different KGs as a neural <font color="#00be00">architecture search</font> (NAS) problem. First, we analyze the difficulty of using a unified model to solve the distillation problem. Based on it, we define the path distiller to recurrently combine structural and semantic information along relational paths, which are sampled to preserve both local topologies and semantics. Then, inspired by the recent success of NAS, we design a recurrent network-based search space for specific KG tasks and propose a natural gradient (NG) based search algorithm to update architectures. Experimental results demonstrate that the searched models by our proposed S2E <font color="#00be00">outperform</font> human-designed ones, and the NG based search algorithm is efficient compared with other NAS methods. Besides, our work is the first NAS method for RNN that can search architectures with better performance than human-designed models. </br></br>

<a href='http://arxiv.org/pdf/1911.07153.pdf'>1911.07153</a> &nbsp&nbsp (cs:ET) &nbsp&nbsp 0.1925баллов, №300</br>
<b>Stochastic Magnetoelectric Neuron for Temporal Information Encoding</b></br>
Authors: , Yang, Kezhou, Sengupta, Abhronil</br>
  Emulating various facets of computing principles of the <font color="#00be00">brain</font> can potentially lead to the development of neuro-computers that are able to exhibit brain-like cognitive capabilities. In this letter, we propose a magnetoelectronic neuron that utilizes noise as a computing resource and is able to encode information over time through the independent control of external voltage signals. We extensively characterize the device operation and demonstrate its suitability for neuromorphic computing platforms performing temporal information encoding. </br></br>

<a href='http://arxiv.org/pdf/1910.02140.pdf'>1910.02140</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp 0.1924баллов, №301</br>
<b>Discounted Reinforcement Learning is Not an Optimization Problem</b></br>
Authors: , Naik, Abhishek, Shariff, Roshan, Yasui, Niko, Sutton, Richard S.</br>
  Discounted <font color="#00be00">reinforcement learning</font> is fundamentally incompatible with function approximation for control in continuing tasks. It is not an optimization problem in its usual formulation, so when using function approximation there is no optimal policy. We substantiate these claims, then go on to address some misconceptions about discounting and its connection to the average reward formulation. We encourage researchers to adopt rigorous optimization approaches, such as maximizing average reward, for reinforcement learning in continuing tasks. </br></br>

<a href='http://arxiv.org/pdf/1911.05034.pdf'>1911.05034</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp 0.1920баллов, №302</br>
<b>Learning Sparse Sharing Architectures for Multiple Tasks</b></br>
Authors: , Sun, Tianxiang, Shao, Yunfan, Li, Xiaonan, Liu, Pengfei, Yan, Hang, Qiu, Xipeng, Huang, Xuanjing</br>
  Most existing deep multi-task learning models are based on parameter sharing, such as hard sharing, <font color="#00be00">hierarchical</font> sharing, and soft sharing. How choosing a suitable sharing mechanism depends on the relations among the tasks, which is not easy since it is difficult to understand the underlying shared factors among these tasks. In this paper, we propose a novel parameter sharing mechanism, named \\emph{Sparse Sharing}. Given multiple tasks, our approach automatically finds a sparse sharing structure. We start with an over-parameterized base network, from which each task extracts a subnetwork. The subnetworks of multiple tasks are partially overlapped and trained in parallel. We show that both hard sharing and hierarchical sharing can be formulated as particular instances of the sparse sharing framework. We conduct extensive experiments on three sequence labeling tasks. Compared with single-task models and three typical multi-task learning baselines, our proposed approach achieves consistent improvement while requiring fewer parameters. </br></br>

<a href='http://arxiv.org/pdf/1911.07279.pdf'>1911.07279</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.1919баллов, №303</br>
<b>Detecting F-formations &amp; Roles in Crowded Social Scenes with Wearables:\n  Combining Proxemics &amp; Dynamics using LSTMs</b></br>
Authors: , Rosatelli, Alessio, Gedik, Ekin, Hung, Hayley</br>
  In this paper, we investigate the use of proxemics and dynamics for automatically identifying conversing groups, or so-called F-formations. More formally we aim to automatically identify whether wearable sensor data coming from 2 people is indicative of F-formation membership. We also explore the problem of jointly detecting membership and more descriptive information about the pair relating to the role they take in the conversation (i.e. speaker or listener). We jointly model the concepts of proxemics and dynamics using binary proximity and acceleration obtained through a single wearable sensor per person. We test our approaches on the <font color="#00be00">publicly available</font> MatchNMingle dataset which was collected during real-life mingling events. We find out that fusion of these two modalities performs significantly better than them independently, providing an AUC of 0.975 when data from 30-second windows are used. Furthermore, our investigation into roles detection shows that each role pair requires a different time resolution for accurate detection. </br></br>

<a href='http://arxiv.org/pdf/1911.09320.pdf'>1911.09320</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp 0.1907баллов, №304</br>
<b>Minimizing the Bag-of-Ngrams Difference for Non-Autoregressive Neural\n  Machine Translation</b></br>
Authors: , Shao, Chenze, Zhang, Jinchao, Feng, Yang, Meng, Fandong, Zhou, Jie</br>
  Non-Autoregressive Neural Machine Translation (NAT) achieves significant decoding speedup through generating target words independently and simultaneously. However, in the context of non-autoregressive translation, the word-level cross-entropy loss cannot model the target-side sequential dependency properly, leading to its weak correlation with the translation quality. As a result, NAT tends to generate influent translations with over-translation and under-translation errors. In this paper, we propose to train NAT to minimize the Bag-of-Ngrams (BoN) difference between the model output and the reference sentence. The bag-of-ngrams training objective is differentiable and can be efficiently calculated, which encourages NAT to capture the target-side sequential dependency and correlates well with the translation quality. We validate our approach on three translation tasks and show that our approach largely <font color="#00be00">outperform</font>s the NAT baseline by about 5.0 BLEU scores on WMT14 En$\\leftrightarrow$De and about 2.5 BLEU scores on WMT16 En$\\leftrightarrow$Ro. </br></br>

<a href='http://arxiv.org/pdf/1911.07414.pdf'>1911.07414</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.1897баллов, №305</br>
<b>Potential Field: Interpretable and Unified Representation for Trajectory\n  Prediction</b></br>
Authors: , Su, Shan, Peng, Cheng, Shi, Jianbo, Choi, Chiho</br>
  Predicting an agent\'s future trajectory is a challenging task given the complicated stimuli (environmental/inertial/social) of motion. Prior works learn individual stimulus from different modules and fuse the representations in an end-to-end manner, which makes it hard to understand what are actually captured and how they are fused. In this work, we borrow the notion of potential field from physics as an <font color="#be00be">interpret</font>able and unified representation to model all stimuli. This allows us to not only supervise the intermediate learning process, but also have a coherent method to fuse the information of different sources. From the generated potential fields, we further estimate future motion direction and speed, which are modeled as <font color="blue">Gaussi</font>an distributions to account for the multi-modal nature of the problem. The final prediction results are generated by recurrently moving past location based on the estimated motion direction and speed. We show <font color="red">state-of-the-art</font> results on the ETH, UCY, and Stanford Drone datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.09535.pdf'>1911.09535</a> &nbsp&nbsp (cs:AI, cs:ML) &nbsp&nbsp 0.1845баллов, №306</br>
<b>Agent Probing Interaction Policies</b></br>
Authors: , Azeez, Oluwafemi, Ghiya, Siddharth, Miller, Brendan</br>
  <font color="#00be00">Reinforcement learning</font> in a multi agent system is difficult because these systems are inherently non-stationary in nature. In such a case, identifying the type of the opposite agent is crucial and can help us address this non-stationary environment. We have investigated if we can employ some probing policies which help us better identify the type of the other agent in the environment. We\'ve made a simplifying assumption that the other agent has a stationary policy that our probing policy is trying to approximate. Our work extends Environmental Probing Interaction Policy framework to handle multi agent environments. </br></br>

<a href='http://arxiv.org/pdf/1911.07990.pdf'>1911.07990</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.1830баллов, №307</br>
<b>Segmentation Guided Attention Network for Crowd Counting via Curriculum\n  Learning</b></br>
Authors: , Wang, Qian, Breckon, Toby P.</br>
  Crowd counting using deep convolutional neural networks (CNN) has achieved encouraging progress in the last couple of years. Novel network architectures have been designed to handle the scale variance issue in crowd images. For this purpose, the ideas of using multi-column networks with different convolution <font color="blue">kernel</font> sizes and rich feature fusion have been prevalent in literature. Recent works have shown the effectiveness of \\textit{Inception} modules in crowd counting due to its ability to capture multi-scale visual information via the fusion of features from multi-column networks. However, the existing crowd counting networks built with \\textit{Inception} modules usually have a small number of layers and only employ the basic type of \\textit{Inception} modules. In this paper, we investigate the use of pre-trained \\textit{Inception} model for crowd counting. Specifically, we firstly benchmark the baseline \\textit{Inception-v3} models on commonly used crowd counting datasets and show its superiority to other existing models. Subsequently, we present a <font color="#be00be">Segmentation</font> Guided Attention Network (SGANet) with the \\textit{Inception-v3} as the backbone for crowd counting. We also propose a novel <font color="#006400">curriculum learning</font> strategy for more efficient training of crowd counting networks. Finally, we conduct thorough experiments to compare the performance of SGANet and other <font color="red">state-of-the-art</font> models. The experimental results validate the effectiveness of the segmentation guided attention layer and the curriculum learning strategy in crowd counting. </br></br>

<a href='http://arxiv.org/pdf/1911.09322.pdf'>1911.09322</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp 0.1825баллов, №308</br>
<b>Data Proxy Generation for Fast and Efficient Neural Architecture Search</b></br>
Authors: , Park, Minje</br>
  Due to the recent advances on Neural <font color="#00be00">Architecture Search</font> (NAS), it gains popularity in designing best networks for specific tasks. Although it shows promising results on many benchmarks and competitions, NAS still suffers from its demanding computation cost for searching high dimensional architectural design space, and this problem becomes even worse when we want to use a large-scale dataset. If we can make a reliable data proxy for NAS, the efficiency of NAS approaches increase accordingly. Our basic observation for making a data proxy is that each example in a specific dataset has a different impact on NAS process and most of examples are redundant from a relative accuracy ranking perspective, which we should preserve when making a data proxy. We propose a systematic approach to measure the importance of each example from this relative accuracy ranking point of view, and make a reliable data proxy based on the statistics of training and testing examples. Our experiment shows that we can preserve the almost same relative accuracy ranking between all possible network configurations even with 10-20$\\times$ smaller data proxy. </br></br>

<a href='http://arxiv.org/pdf/1911.02257.pdf'>1911.02257</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp 0.1634баллов, №309</br>
<b>Hierarchical Contextualized Representation for Named Entity Recognition</b></br>
Authors: , Luo, Ying, Xiao, Fengshun, Zhao, Hai</br>
  <font color="blue">Named entity</font> recognition (NER) models are typically based on the architecture of Bi-directional LSTM (BiLSTM). The constraints of sequential nature and the modeling of single input prevent the full utilization of global information from larger scope, not only in the entire sentence, but also in the entire document (dataset). In this paper, we address these two deficiencies and propose a model augmented with <font color="#00be00">hierarchical</font> contextualized representation: sentence-level representation and document-level representation. In sentence-level, we take different contributions of words in a single sentence into consideration to enhance the sentence representation learned from an independent BiLSTM via label embedding attention mechanism. In document-level, the key-value memory network is adopted to record the document-aware information for each unique word which is sensitive to similarity of context information. Our two-level hierarchical contextualized representations are fused with each input token embedding and corresponding hidden state of BiLSTM, respectively. The experimental results on three benchmark<font color="#be00be"> NER </font>datasets (CoNLL-2003 and Ontonotes 5.0 English datasets, CoNLL-2002 Spanish dataset) show that we establish new <font color="red">state-of-the-art</font> results. </br></br>

<a href='http://arxiv.org/pdf/1911.07399.pdf'>1911.07399</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 0.1616баллов, №310</br>
<b>NeuronInspect: Detecting Backdoors in Neural Networks via Output\n  Explanations</b></br>
Authors: , Huang, Xijie, Alzantot, Moustafa, Srivastava, Mani</br>
  Deep neural networks have achieved <font color="red">state-of-the-art</font> performance on various tasks. However, lack of <font color="#be00be">interpret</font>ability and transparency makes it easier for malicious attackers to inject trojan backdoor into the neural networks, which will make the model behave abnormally when a backdoor sample with a specific trigger is input. In this paper, we propose NeuronInspect, a framework to detect trojan backdoors in deep neural networks via output explanation techniques. NeuronInspect first identifies the existence of backdoor attack targets by generating the explanation heatmap of the output layer. We observe that generated heatmaps from clean and backdoored models have different characteristics. Therefore we extract features that measure the attributes of explanations from an attacked model namely: sparse, smooth and persistent. We combine these features and use <font color="#be00be">outlier</font> detection to figure out the outliers, which is the set of attack targets. We demonstrate the effectiveness and efficiency of NeuronInspect on MNIST digit recognition dataset and GTSRB traffic sign recognition dataset. We extensively evaluate NeuronInspect on different attack scenarios and prove better robustness and effectiveness over state-of-the-art trojan backdoor detection techniques Neural Cleanse by a great margin. </br></br>

<a href='http://arxiv.org/pdf/1911.08874.pdf'>1911.08874</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.1582баллов, №311</br>
<b>Avoiding Jammers: A Reinforcement Learning Approach</b></br>
Authors: , Ak, Serkan, Bruggenwirth, Stefan</br>
  This paper investigates the anti-jamming performance of a cognitive radar under a partially observable Markov decision process (POMDP) model. First, we obtain an explicit expression for uncertainty of jammer dynamics, which paves the way for illuminating the performance metric of probability of being jammed for the radar beyond a conventional signal-to-noise ratio ($\\mathsf{SNR}$) based analysis. Considering two frequency hopping strategies developed in the framework of <font color="#00be00">reinforcement learning</font> (RL), this performance metric is analyzed with deep Q-network (DQN) and long short term memory (LSTM) networks under various uncertainty values. Finally, the requirement of the target network in the RL algorithm for both network architectures is replaced with a softmax operator. Simulation results show that this operator improves upon the performance of the traditional target network. </br></br>

<a href='http://arxiv.org/pdf/1911.04658.pdf'>1911.04658</a> &nbsp&nbsp (cs:NE) &nbsp&nbsp 0.1562баллов, №312</br>
<b>Multi-Objectivizing Sum-of-the-Parts Combinatorial Optimization Problems\n  by Random Objective Decomposition</b></br>
Authors: , Shi, Jialong, Sun, Jianyong, Zhang, Qingfu</br>
  Multi-objectivization is a term used to describe strategies developed for optimizing single-objective problems by multi-objective algorithms. This paper focuses on the multi-objectivization of the sum-of-the-parts Combinatorial Optimization Problems (COPs), which include the Traveling Salesman Problem (TSP), the Unconstrained Binary Quadratic Programming (UBQP) and other well-known COPs. For a sum-of-the-parts COP, we propose to decompose its original objective into two sub-objectives with controllable correlation. Based on the decomposition method, two new multi-objectivization techniques called Non-Dominance Search (NDS) and Non-Dominance Exploitation (NDE) are developed, respectively. NDS is combined with the Iterated Local Search (ILS) metaheuristic (with fixed neighborhood structure), while NDE is embedded within the Iterated Lin-Kernighan (ILK) metaheuristic (with varied neighborhood structure). The resultant metaheuristics are called ILS+NDS and ILK+NDE, respectively. Empirical studies on some TSP and UBQP instances show that with appropriate correlation between the sub-objectives, there are more chances to escape from local optima when new starting solution is selected from the non-dominated solutions defined by the decomposed sub-objectives. Experimental results also show that ILS+NDS and ILK+NDE both significantly <font color="#00be00">outperform</font> their counterparts on most of the test instances. </br></br>

<a href='http://arxiv.org/pdf/1911.08044.pdf'>1911.08044</a> &nbsp&nbsp (cs:AI, cs:ML) &nbsp&nbsp 0.1505баллов, №313</br>
<b>Adversarial Inverse Reinforcement Learning for Decision Making in\n  Autonomous Driving</b></br>
Authors: , Wang, Pin, Liu, Dapeng, Chen, Jiayu, Chan, Ching-Yao</br>
  Generative Adversarial Imitation Learning (GAIL) is an efficient way to learn sequential control strategies from demonstration. Adversarial Inverse <font color="#00be00">Reinforcement Learning</font> (AIRL) is similar to GAIL but also learns a reward function at the same time and has better training stability. In previous work, however, AIRL has mostly been demonstrated on robotic control in artificial environments. In this paper, we apply AIRL to a practical and challenging problem -- the decision-making in autonomous driving, and also augment AIRL with a semantic reward to improve its performance. We use four metrics to evaluate its learning performance in a simulated driving environment. Results show that the vehicle agent can learn decent decision-making behaviors from scratch, and can reach a level of performance comparable with that of an expert. Additionally, the comparison with GAIL shows that AIRL converges faster, achieves better and more stable performance than GAIL. </br></br>

<a href='http://arxiv.org/pdf/1911.07272.pdf'>1911.07272</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.1437баллов, №314</br>
<b>Unsupervised Visual Representation Learning with Increasing Object Shape\n  Bias</b></br>
Authors: , Wang, Zhibo, Yan, Shen, Zhang, Xiaoyu, Shah, Mubarak, Lobo, Niels</br>
  (Very early draft)Traditional supervised learning keeps pushing convolution neural network(CNN) achieving state-of-art performance. However, lack of large-scale annotation data is always a big problem due to the high cost of it, even ImageNet dataset is over-fitted by complex models now. The success of unsupervised learning method represented by the<font color="#00be00"> Bert </font>model in natural language processing(NLP) field shows its great potential. And it makes that unlimited training samples becomes possible and the great universal generalization ability changes NLP research direction directly. In this article, we purpose a novel unsupervised learning method based on contrastive predictive coding. Under that, we are able to train model with any non-annotation images and improve model\'s performance to reach state-of-art performance at the same level of model complexity. Beside that, since the number of training images could be unlimited amplification, an universal large-scale pre-trained computer vision model is possible in the future. </br></br>

<a href='http://arxiv.org/pdf/1910.02354.pdf'>1910.02354</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.1428баллов, №315</br>
<b>AdvSPADE: Realistic Unrestricted Attacks for Semantic Segmentation</b></br>
Authors: , Shen, Guangyu, Mao, Chengzhi, Yang, Junfeng, Ray, Baishakhi</br>
  Due to the inherent robustness of <font color="#be00be">segmentation</font> models, traditional norm-bounded attack methods show limited effect on such type of models. In this paper, we focus on generating unrestricted adversarial examples for semantic segmentation models. We demonstrate a simple and effective method to generate unrestricted adversarial examples using conditional generative adversarial networks (CGAN) without any hand-crafted metric. The na\\&quot;ive implementation of CGAN, however, yields inferior image quality and low attack success rate. Instead, we leverage the SPADE (Spatially-adaptive denormalization) structure with an additional loss item to generate effective <font color="blue">adversarial att</font>acks in a single step. We validate our approach on the popular Cityscapes and ADE20K datasets, and demonstrate that our synthetic adversarial examples are not only realistic, but also improve the attack success rate by up to 41.0\\% compared with the <font color="red">state of the art</font> adversarial attack methods including PGD. </br></br>

<a href='http://arxiv.org/pdf/1911.08261.pdf'>1911.08261</a> &nbsp&nbsp (cs:NE, cs:CV) &nbsp&nbsp 0.1425баллов, №316</br>
<b>Unsupervised AER Object Recognition Based on Multiscale Spatio-Temporal\n  Features and Spiking Neurons</b></br>
Authors: , Liu, Qianhui, Pan, Gang, Ruan, Haibo, Xing, Dong, Xu, Qi, Tang, Huajin</br>
  This paper proposes an unsupervised address event representation (AER) object recognition approach. The proposed approach consists of a novel multiscale spatio-temporal feature (MuST) representation of input AER events and a spiking neural network (SNN) using spike-timing-dependent plasticity (STDP) for object recognition with MuST. MuST extracts the features contained in both the spatial and temporal information of AER event flow, and meanwhile forms an informative and compact feature spike representation. We show not only how MuST exploits spikes to convey information more effectively, but also how it benefits the recognition using SNN. The recognition process is performed in an unsupervised manner, which does not need to specify the desired status of every single neuron of SNN, and thus can be flexibly applied in <font color="#009600">real-world</font> recognition tasks. The experiments are performed on five AER datasets including a new one named GESTURE-DVS. Extensive experimental results show the effectiveness and advantages of this proposed approach. </br></br>

<a href='http://arxiv.org/pdf/1911.05071.pdf'>1911.05071</a> &nbsp&nbsp (cs:CV, cs:ML, cs:RO) &nbsp&nbsp 0.1402баллов, №317</br>
<b>Experience-Embedded Visual Foresight</b></br>
Authors: , Yen-Chen, Lin, Bauza, Maria, Isola, Phillip</br>
  Visual foresight gives an agent a window into the future, which it can use to anticipate events before they happen and plan strategic behavior. Although impressive results have been achieved on video prediction in constrained settings, these models fail to generalize when confronted with unfamiliar <font color="#009600">real-world</font> objects. In this paper, we tackle the generalization problem via fast adaptation, where we train a prediction model to quickly adapt to the observed visual dynamics of a novel object. Our method, Experience-embedded Visual Foresight (EVF), jointly learns a fast adaptation module, which encodes observed trajectories of the new object into a vector embedding, and a visual prediction model, which conditions on this embedding to generate physically plausible predictions. For evaluation, we compare our method against baselines on video prediction and benchmark its utility on two real-world control tasks. We show that our method is able to quickly adapt to new visual dynamics and achieves lower error than the baselines when manipulating novel objects. </br></br>

<a href='http://arxiv.org/pdf/1911.08566.pdf'>1911.08566</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.1393баллов, №318</br>
<b>Joint Super-Resolution and Alignment of Tiny Faces</b></br>
Authors: , Yin, Yu, Robinson, Joseph P., Zhang, Yulun, Fu, Yun</br>
  <font color="#be00be">Super-resolution</font> (SR) and landmark localization of tiny faces are highly correlated tasks. On the one hand, landmark localization could obtain higher accuracy with faces of high-resolution (HR). On the other hand,<font color="#be00be"> face </font>SR would benefit from prior knowledge of<font color="#be00be"> facial </font>attributes such as landmarks. Thus, we propose a joint alignment and SR network to simultaneously detect facial landmarks and super-resolve tiny faces. More specifically, a shared deep encoder is applied to extract features for both tasks by leveraging complementary information. To exploit the representative power of the <font color="#00be00">hierarchical</font> encoder, intermediate layers of a shared feature extraction module are fused to form efficient feature representations. The fused features are then fed to task-specific modules to detect landmarks and super-resolve face images in parallel. Extensive experiments demonstrate that the proposed model significantly <font color="#00be00">outperform</font>s the <font color="red">state-of-the-art</font> in both landmark localization and SR of faces. We show a large improvement for landmark localization of tiny faces (i.e., 16*16). Furthermore, the proposed framework yields comparable results for landmark localization on low-resolution (LR) faces (i.e., 64*64) to existing methods on HR (i.e., 256*256). As for SR, the proposed method recovers sharper edges and more details from LR face images than other state-of-the-art methods, which we demonstrate qualitatively and quantitatively. </br></br>

<a href='http://arxiv.org/pdf/1911.07840.pdf'>1911.07840</a> &nbsp&nbsp (cs:AI, cs:RO) &nbsp&nbsp 0.1345баллов, №319</br>
<b>Cooperative Pathfinding based on high-scalability Multi-agent RRT*</b></br>
Authors: , Jiang, Jinmingwu, Wu, Kaigui</br>
  Problems that claim several agents to find no-conflicts paths from their start locations to their destinations are named as cooperative pathfinding problems. This problem can be efficiently solved by the Multi-agent RRT*(MA-RRT*) algorithm, which offers better scalability than some traditional algorithms, such as Optimal Anytime(OA), in sparse environments. However, MA-RRT* cannot effectively find solutions in relatively dense environments, cause some random samples in the free space cannot be explored by the rapidly random tree, which hinders the application of MA-RRT* in a more complicated <font color="#009600">real-world</font>. This paper proposes an improved version of MA-RRT *, called Multi-agent RRT* Potential Field (MA-RRT*PF), an anytime algorithm that can efficiently guide the rapidly random tree to the free space in relatively dense environments. It works by incorporating a potential field to the GREEDY function to enhance the ability to avoid the obstacles. The results show that MA-RRT*PF performs much better than MA-RRT* in relatively dense environments in terms of scalability while still maintaining the solution quality. </br></br>

<a href='http://arxiv.org/pdf/1911.07408.pdf'>1911.07408</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp 0.1298баллов, №320</br>
<b>Development of MirrorShape: High Fidelity Large-Scale Shape Rendering\n  Framework for Virtual Reality</b></br>
Authors: , Fedoseev, Aleksey, Chernyadev, Nikita, Tsetserukou, Dzmitry</br>
  Today there is a high variety of haptic devices capable of providing tactile feedback. Although most of existing designs are aimed at realistic simulation of the surface properties, their capabilities are limited in attempts of displaying shape and position of virtual objects. This paper suggests a new concept of distributed haptic display for realistic interaction with virtual object of complex shape by a collaborative robot with shape display end-effector. MirrorShape renders the 3D object in virtual reality (VR) system by contacting the user hands with the robot end-effector at the calculated point in real-time. Our proposed system makes it possible to synchronously merge the position of contact point in VR and end-effector in <font color="#009600">real world</font>. This feature provides presentation of different shapes, and at the same time expands the working area comparing to desktop solutions. The preliminary user study revealed that MirrorShape was effective at reducing positional error in VR interactions. Potentially this approach can be used in the virtual systems for rendering versatile VR objects with wide range of sizes with high fidelity large-scaleshape experience. </br></br>

<a href='http://arxiv.org/pdf/1910.00982.pdf'>1910.00982</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.1234баллов, №321</br>
<b>Adversarially Robust Few-Shot Learning: A Meta-Learning Approach</b></br>
Authors: , Goldblum, Micah, Fowl, Liam, Goldstein, Tom</br>
  Previous work on adversarially robust neural networks for image classification requires large training sets and computationally expensive training procedures. On the other hand, <font color="#00be00">few-shot</font> learning methods are highly vulnerable to adversarial examples. The goal of our work is to produce networks which both perform well at few-shot classification tasks and are simultaneously robust to adversarial examples. We develop an algorithm for producing adversarially robust meta-learners, and we thoroughly investigate factors which contribute to adversarial vulnerability. Moreover, our method achieves far superior robust performance on few-shot image classification tasks, such as Mini-ImageNet and CIFAR-FS, than robust transfer learning. </br></br>

<a href='http://arxiv.org/pdf/1911.09157.pdf'>1911.09157</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 0.1142баллов, №322</br>
<b>A Tale of Two-Timescale Reinforcement Learning with the Tightest\n  Finite-Time Bound</b></br>
Authors: , Dalal, Gal, Szorenyi, Balazs, Thoppe, Gugan</br>
  Policy evaluation in <font color="#00be00">reinforcement learning</font> is often conducted using two-timescale stochastic approximation, which results in various gradient temporal difference methods such as GTD(0), GTD2, and TDC. Here, we provide convergence rate bounds for this suite of algorithms. Algorithms such as these have two iterates, $\\theta_n$ and $w_n,$ which are updated using two distinct stepsize sequences, $\\alpha_n$ and $\\beta_n,$ respectively. Assuming $\\alpha_n = n^{-\\alpha}$ and $\\beta_n = n^{-\\beta}$ with $1 &gt; \\alpha &gt; \\beta &gt; 0,$ we show that, with high probability, the two iterates converge to their respective solutions $\\theta^*$ and $w^*$ at rates given by $\\|\\theta_n - \\theta^*\\| = \\tilde{O}( n^{-\\alpha/2})$ and $\\|w_n - w^*\\| = \\tilde{O}(n^{-\\beta/2});$ here, $\\tilde{O}$ hides logarithmic terms. Via comparable lower bounds, we show that these bounds are, in fact, tight. To the best of our knowledge, ours is the first finite-time analysis which achieves these rates. While it was known that the two timescale components decouple asymptotically, our results depict this phenomenon more explicitly by showing that it in fact happens from some finite time onwards. Lastly, compared to existing works, our result applies to a broader family of stepsizes, including non-square summable ones. </br></br>

<a href='http://arxiv.org/pdf/1911.09137.pdf'>1911.09137</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp 0.1120баллов, №323</br>
<b>Motion control for autonomous heterogeneous multi-agent area search in\n  uncertain conditions</b></br>
Authors: , Ivi&#x107;, Stefan</br>
  Using multiple<font color="#960096"> mobile </font>robots in search missions offers a lot of benefits, but one needs a suitable and competent motion control algorithm which is able to consider sensors characteristics, the uncertainty of target detection and complexity of needed maneuvers in order to make a multi-agent search autonomous. This paper provides a methodology for an autonomous two-dimensional search using multiple unmanned search agents. The proposed methodology relies on an accurate calculation of target occurrence probability distribution based on the initial estimated target distribution and continuous action of spatial variant search agent sensors. The core of the autonomous search process is a high-level motion control for multiple search agents which utilizes the probabilistic model of target occurrence via Heat Equation Driven Area Coverage (HEDAC) method. This centralized motion control algorithm is tailored for handling a group of search agents which are heterogeneous in both motion and sensing characteristics. The motion of agents is directed by the gradient of the potential field which provides near-ergodic exploration of the search space. The proposed method is tested on three realistic search mission simulations and compared with three alternative methods, where HEDAC <font color="#00be00">outperform</font>s all alternatives in all tests. Conventional search strategies need about double the time to achieve proportionate detection rate when compared to HEDAC controlled search. The scalability test showed that increasing the number of HEDAC controlled search agents, although somewhat deteriorating the search efficiency, provides needed speed-up of the search. This study shows the flexibility and competence of the proposed method and gives a strong foundation for possible <font color="#009600">real-world</font> applications. </br></br>

<a href='http://arxiv.org/pdf/1911.01753.pdf'>1911.01753</a> &nbsp&nbsp (cs:RO, cs:NE) &nbsp&nbsp 0.1110баллов, №324</br>
<b>Cognitive and motor compliance in intentional human-robot interaction</b></br>
Authors: , Chame, Hendry Ferreira, Tani, Jun</br>
  Embodiment and subjective experience in human-robot interaction are important aspects to consider when studying both natural cognition and adaptive robotics to human environments. Although several researches have focused on nonverbal communication and collaboration, the study of autonomous physical interaction has obtained less attention. From the perspective of neurorobotics, we investigate the relation between intentionality, motor compliance, cognitive compliance, and behavior emergence. We propose a variational model inspired by the principles of predictive coding and active inference to study intentionality and cognitive compliance, and an intermittent control concept for motor deliberation and compliance based on torque feed-back. Our experiments with the humanoid Torobo portrait interesting perspectives for the bio-inspired study of developmental and social processes. </br></br>

<a href='http://arxiv.org/pdf/1911.06970.pdf'>1911.06970</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp 0.1067баллов, №325</br>
<b>Off-Policy Policy Gradient Algorithms by Constraining the State\n  Distribution Shift</b></br>
Authors: , Islam, Riashat, Teru, Komal K., Sharma, Deepak</br>
  Off-policy deep <font color="#00be00">reinforcement learning</font> (RL) algorithms are incapable of learning solely from batch offline data without online interactions with the environment, due to the phenomenon known as \\textit{extrapolation error}. This is often due to past data available in the replay buffer that may be quite different from the data distribution under the current policy. We argue that most off-policy learning methods fundamentally suffer from a \\textit{state distribution shift} due to the mismatch between the state visitation distribution of the data collected by the behavior and target policies. This data distribution shift between current and past samples can significantly impact the performance of most modern off-policy based policy optimization algorithms. In this work, we first do a systematic analysis of state distribution mismatch in off-policy learning, and then develop a novel off-policy policy optimization method to constraint the state distribution shift. To do this, we first estimate the state distribution based on features of the state, using a density estimator and then develop a novel constrained off-policy gradient objective that minimizes the state distribution shift. Our experimental results on continuous control tasks show that minimizing this distribution mismatch can significantly improve performance in most popular practical off-policy policy gradient algorithms. </br></br>

<a href='http://arxiv.org/pdf/1911.08709.pdf'>1911.08709</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.1006баллов, №326</br>
<b>Graph-Driven Generative Models for Heterogeneous Multi-Task Learning</b></br>
Authors: , Wang, Wenlin, Xu, Hongteng, Gan, Zhe, Li, Bai, Wang, Guoyin, Chen, Liqun, Yang, Qian, Wang, Wenqi, Carin, Lawrence</br>
  We propose a novel graph-driven generative model, that unifies multiple heterogeneous learning tasks into the same framework. The proposed model is based on the fact that heterogeneous learning tasks, which correspond to different generative processes, often rely on data with a shared graph structure. Accordingly, our model combines a graph convolutional network (GCN) with multiple variational autoencoders, thus embedding the nodes of the graph i.e., samples for the tasks) in a uniform manner while specializing their organization and usage to different tasks. With a focus on healthcare applications (tasks), including <font color="blue">clinic</font>al topic modeling, procedure <font color="blue">recommendat</font>ion and admission-type prediction, we demonstrate that our method successfully leverages information across different tasks, boosting performance in all tasks and <font color="#00be00">outperform</font>ing existing <font color="red">state-of-the-art</font> approaches. </br></br>

<a href='http://arxiv.org/pdf/1911.07361.pdf'>1911.07361</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.0990баллов, №327</br>
<b>Rebalancing Learning on Evolving Data Streams</b></br>
Authors: , Bernardo, Alessio, Della Valle, Emanuele, Bifet, Albert</br>
  Nowadays, every device connected to the Internet generates an ever-growing stream of data (formally, unbounded). Machine Learning on unbounded data streams is a grand challenge due to its resource constraints. In fact, standard machine learning techniques are not able to deal with data whose statistics is subject to gradual or sudden changes without any warning. Massive Online Analysis (MOA) is the collective name, as well as a software library, for new learners that are able to manage data streams. In this paper, we present a research study on streaming rebalancing. Indeed, data streams can be imbalanced as static data, but there is not a method to rebalance them incrementally, one element at a time. For this reason we propose a new streaming approach able to rebalance data streams online. Our new methodology is evaluated against some synthetically generated datasets using prequential evaluation in order to demonstrate that it <font color="#00be00">outperform</font>s the existing approaches. </br></br>

<a href='http://arxiv.org/pdf/1911.08225.pdf'>1911.08225</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp 0.0976баллов, №328</br>
<b>Multimedia Search and Temporal Reasoning</b></br>
Authors: , Moreno, Marcio Ferreira, Santos, Rodrigo Costa Mesquita, Santos, Wallas Henrique Sousa dos, Fiorini, Sandro Rama, Silva, Reinaldo Mozart da Gama</br>
  Properly modelling dynamic information that changes over time still is an open issue. Most modern knowledge bases are unable to represent relationships that are valid only during a given time interval. In this work, we revisit a previous extension to the hyperknowledge framework to deal with temporal facts and propose a temporal query language and engine. We validate our proposal by discussing a qualitative analysis of the modelling of a <font color="#009600">real-world</font> use case in the Oil &amp; Gas industry. </br></br>

<a href='http://arxiv.org/pdf/1911.08967.pdf'>1911.08967</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.0973баллов, №329</br>
<b>Transfer Learning Toolkit: Primers and Benchmarks</b></br>
Authors: , Zhuang, Fuzhen, Duan, Keyu, Guo, Tongjia, Zhu, Yongchun, Xi, Dongbo, Qi, Zhiyuan, He, Qing</br>
  The transfer learning toolkit wraps the codes of 17 transfer learning models and provides integrated interfaces, allowing users to use those models by calling a simple function. It is easy for primary researchers to use this toolkit and to choose proper models for <font color="#009600">real-world</font> applications. The toolkit is written in Python and distributed under MIT open source license. In this paper, the current state of this toolkit is described and the necessary environment setting and usage are introduced. </br></br>

<a href='http://arxiv.org/pdf/1911.06987.pdf'>1911.06987</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.0955баллов, №330</br>
<b>Faster AutoAugment: Learning Augmentation Strategies using\n  Backpropagation</b></br>
Authors: , Hataya, Ryuichiro, Zdenek, Jan, Yoshizoe, Kazuki, Nakayama, Hideki</br>
  Data augmentation methods are indispensable heuristics to boost the performance of deep neural networks, especially in image recognition tasks. Recently, several studies have shown that augmentation strategies found by search algorithms <font color="#00be00">outperform</font> hand-made strategies. Such methods employ black-box search algorithms over image transformations with continuous or discrete parameters and require a long time to obtain better strategies. In this paper, we propose a differentiable policy search pipeline for data augmentation, which is much faster than previous methods. We introduce approximate gradients for several transformation operations with discrete parameters as well as the differentiable mechanism for selecting operations. As the objective of training, we minimize the distance between the distributions of augmented data and the original data, which can be differentiated. We show that our method, Faster AutoAugment, achieves significantly faster searching than prior work without a performance drop. </br></br>

<a href='http://arxiv.org/pdf/1911.09101.pdf'>1911.09101</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 0.0934баллов, №331</br>
<b>Safe Policies for Reinforcement Learning via Primal-Dual Methods</b></br>
Authors: , Paternain, Santiago, Calvo-Fullana, Miguel, Chamon, Luiz F. O., Ribeiro, Alejandro</br>
  In this paper, we study the learning of safe policies in the setting of <font color="#00be00">reinforcement learning</font> problems. This is, we aim to control a Markov Decision Process (MDP) of which we do not know the transition probabilities, but we have access to sample trajectories through experience. We define safety as the agent remaining in a desired safe set with high probability during the operation time. We therefore consider a constrained MDP where the constraints are probabilistic. Since there is no straightforward way to optimize the policy with respect to the probabilistic constraint in a reinforcement learning framework, we propose an ergodic relaxation of the problem. The advantages of the proposed relaxation are threefold. (i) The safety guarantees are maintained in the case of episodic tasks and they are kept up to a given time horizon for continuing tasks. (ii) The constrained optimization problem despite its non-convexity has arbitrarily small duality gap if the parametrization of the policy is rich enough. (iii) The gradients of the Lagrangian associated with the safe-learning problem can be easily computed using standard policy gradient results and stochastic approximation tools. Leveraging these advantages, we establish that primal-dual algorithms are able to find policies that are safe and optimal. We test the proposed approach in a navigation task in a continuous domain. The numerical results show that our algorithm is capable of dynamically adapting the policy to the environment and the required safety levels. </br></br>

<a href='http://arxiv.org/pdf/1911.09177.pdf'>1911.09177</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.0881баллов, №332</br>
<b>Feature Extraction in Augmented Reality</b></br>
Authors: , Parmar, Jekishan K., Desai, Ankit</br>
  Augmented Reality (AR) is used for various applications associated with the <font color="#009600">real world</font>. In this paper, first, describe characteristics and essential services of AR. Brief history on Virtual Reality (VR) and AR is also mentioned in the introductory section. Then, AR Technologies along with its workflow is depicted, which includes the complete AR Process consisting of the stages of Image Acquisition, Feature Extraction, Feature Matching, Geometric Verification, and Associated Information Retrieval. Feature extraction is the essence of AR hence its details are furnished in the paper. </br></br>

<a href='http://arxiv.org/pdf/1911.08324.pdf'>1911.08324</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.0848баллов, №333</br>
<b>Single-Stage 6D Object Pose Estimation</b></br>
Authors: , Hu, Yinlin, Fua, Pascal, Wang, Wei, Salzmann, Mathieu</br>
  Most recent 6D pose estimation frameworks first rely on a deep network to establish correspondences between 3D object keypoints and 2D image locations and then use a variant of a RANSAC-based Perspective-n-Point (PnP) algorithm. This two-stage process, however, is suboptimal: First, it is not end-to-end trainable. Second, training the deep network relies on a surrogate loss that does not directly reflect the final 6D pose estimation task.   In this work, we introduce a deep architecture that directly regresses 6D poses from correspondences. It takes as input a group of candidate correspondences for each 3D keypoint and accounts for the fact that the order of the correspondences within each group is irrelevant, while the order of the groups, that is, of the 3D keypoints, is fixed. Our architecture is generic and can thus be exploited in conjunction with existing correspondence-extraction networks so as to yield single-stage 6D pose estimation frameworks. Our experiments demonstrate that these single-stage frameworks consistently <font color="#00be00">outperform</font> their two-stage counterparts in terms of both accuracy and speed. </br></br>

<a href='http://arxiv.org/pdf/1911.08542.pdf'>1911.08542</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp 0.0793баллов, №334</br>
<b>Learning to Control Latent Representations for Few-Shot Learning of\n  Named Entities</b></br>
Authors: , Florez, Omar U., Mueller, Erik</br>
  Humans excel in continuously learning with small data without forgetting how to solve old problems. However, neural networks require large datasets to compute latent representations across different tasks while minimizing a loss function. For example, a natural language understanding (NLU) system will often deal with emerging entities during its deployment as interactions with users in realistic scenarios will generate new and infrequent names, events, and locations. Here, we address this scenario by introducing an RL trainable controller that disentangles the representation learning of a neural encoder from its memory management role.   Our proposed solution is straightforward and simple: we train a controller to execute an optimal sequence of reading and writing operations on an external memory with the goal of leveraging diverse activations from the past and provide accurate predictions. Our approach is named Learning to Control (LTC) and allows <font color="#00be00">few-shot</font> learning with two degrees of memory plasticity. We experimentally show that our system obtains accurate results for few-shot learning of entity recognition in the Stanford Task-Oriented Dialogue dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.07241.pdf'>1911.07241</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.0790баллов, №335</br>
<b>SiamCAR: Siamese Fully Convolutional Classification and Regression for\n  Visual Tracking</b></br>
Authors: , Guo, Dongyan, Wang, Jun, Cui, Ying, Wang, Zhenhua, Chen, Shengyong</br>
  By decomposing the visual <font color="#be00be">tracking</font> task into two subproblems as classification for pixel category and <font color="#be00be">regression</font> for object bounding box at this pixel, we propose a novel fully convolutional Siamese network to solve visual tracking end-to-end in a per-pixel manner. The proposed framework SiamCAR consists of two simple subnetworks: one Siamese subnetwork for feature extraction and one classification-regression subnetwork for bounding box prediction. Our framework takes ResNet-50 as backbone. Different from <font color="red">state-of-the-art</font> <font color="#be00be">tracker</font>s like Siamese-RPN, SiamRPN++ and SPM, which are based on region proposal, the proposed framework is both proposal and anchor free. Consequently, we are able to avoid the tricky hyper-parameter tuning of anchors and reduce human intervention. The proposed framework is simple, neat and effective. Extensive experiments and comparisons with state-of-the-art trackers are conducted on many challenging benchmarks like GOT-10K, LaSOT, UAV123 and OTB-50. Without bells and whistles, our SiamCAR achieves the leading performance with a considerable real-time speed. </br></br>

<a href='http://arxiv.org/pdf/1911.09002.pdf'>1911.09002</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.0737баллов, №336</br>
<b>RadioUNet: Fast Radio Map Estimation with Convolutional Neural Networks</b></br>
Authors: , Levie, Ron, Yapar, &#xc7;a&#x11f;kan, Kutyniok, Gitta, Caire, Giuseppe</br>
  In this paper we propose a highly efficient and very accurate method for estimating the propagation pathloss from a point x to all points y on the 2D plane. Our method, termed RadioUNet, is a deep neural network. For applications such as user-cell site association and device-to-device (D2D) link scheduling, an accurate knowledge of the pathloss function for all pairs of locations is very important. Commonly used statistical models approximate the pathloss as a decaying function of the distance between the points. However, in realistic propagation environments characterized by the presence of buildings, street canyons, and objects at different heights, such radial-symmetric functions yield very misleading results. In this paper we show that properly designed and trained deep neural networks are able to learn how to estimate the pathloss function, given an urban environment, very accurately and extremely quickly. Our proposed method generates pathloss estimations that are very close to estimations given by physical simulation, but much faster. Moreover, experimental results show that our method significantly <font color="#00be00">outperform</font>s previously proposed methods based on radial basis function interpolation and tensor completion. </br></br>

<a href='http://arxiv.org/pdf/1911.07450.pdf'>1911.07450</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.0717баллов, №337</br>
<b>Unsupervised Reinforcement Learning of Transferable Meta-Skills for\n  Embodied Navigation</b></br>
Authors: , Li, Juncheng, Wang, Xin, Tang, Siliang, Shi, Haizhou, Wu, Fei, Zhuang, Yueting, Wang, William Yang</br>
  Visual navigation is a task of training an embodied agent by intelligently navigating to a target object (e.g., television) using only visual observations. A key challenge for current deep <font color="#00be00">reinforcement learning</font> models lies in the requirements for a large amount of training data. It is exceedingly expensive to construct sufficient 3D synthetic environments annotated with the target object information. In this paper, we focus on visual navigation in the <font color="#be00be">low-resource</font> setting, where we have only a few training environments annotated with object information. We propose a novel unsupervised reinforcement learning approach to learn transferable meta-skills (e.g., bypass obstacles, go straight) from unannotated environments without any supervisory signals. The agent can then fast adapt to visual navigation through learning a high-level master policy to combine these meta-skills, when the visual-navigation-specified reward is provided. Evaluation in the AI2-THOR environments shows that our method significantly <font color="#00be00">outperform</font>s the baseline by 53.34% relatively on SPL, and further qualitative analysis demonstrates that our method learns transferable motor primitives for visual navigation. </br></br>

<a href='http://arxiv.org/pdf/1910.02244.pdf'>1910.02244</a> &nbsp&nbsp (cs:ML, cs:CV, cs:NE) &nbsp&nbsp 0.0707баллов, №338</br>
<b>Yet another but more efficient black-box adversarial attack: tiling and\n  evolution strategies</b></br>
Authors: , Meunier, Laurent, Atif, Jamal, Teytaud, Olivier</br>
  We introduce a new <font color="#be00be">black-box attack</font> achieving <font color="red">state of the art</font> performances. Our approach is based on a new objective function, borrowing ideas from $\\ell_\\infty$-white box attacks, and particularly designed to fit derivative-free optimization requirements. It only requires to have access to the logits of the classifier without any other information which is a more realistic scenario. Not only we introduce a new objective function, we extend previous works on black box <font color="blue">adversarial att</font>acks to a larger spectrum of evolution strategies and other derivative-free optimization methods. We also highlight a new intriguing property that deep neural networks are not robust to single shot tiled attacks. Our models achieve, with a budget limited to $10,000$ queries, results up to $99.2\\%$ of success rate against InceptionV3 classifier with $630$ queries to the network on average in the untargeted attacks setting, which is an improvement by $90$ queries of the current state of the art. In the targeted setting, we are able to reach, with a limited budget of $100,000$, $100\\%$ of success rate with a budget of $6,662$ queries on average, i.e. we need $800$ queries less than the current state of the art. </br></br>

<a href='http://arxiv.org/pdf/1911.09103.pdf'>1911.09103</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.0678баллов, №339</br>
<b>Iterative Peptide Modeling With Active Learning And Meta-Learning</b></br>
Authors: , Barrett, Rainier, White, Andrew D.</br>
  Often the development of novel materials is not amenable to high-throughput or purely computational screening methods. Instead, materials must be synthesized one at a time in a process that does not generate significant amounts of data. One way this method can be improved is by ensuring that each experiment provides the best improvement in both material properties and predictive modeling accuracy. In this work, we study the effectiveness of active learning, which optimizes the order of experiments, and meta learning, which transfers knowledge from one context to another, to reduce the number of experiments necessary to build a predictive model. We present a novel multi-task benchmark database of peptides designed to advance active, <font color="#00be00">few-shot</font>, and meta-learning methods for experimental design. Each task is binary classification of peptides represented as a sequence string. We show results of standard active learning and meta-learning methods across these datasets to assess their ability to improve predictive models with the fewest number of experiments. We find the ensemble query by committee active learning method to be effective. The meta-learning method Reptile was found to improve accuracy. The robustness of these conclusions were tested across multiple model choices. </br></br>

<a href='http://arxiv.org/pdf/1911.08696.pdf'>1911.08696</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.0663баллов, №340</br>
<b>Where is the Bottleneck of Adversarial Learning with Unlabeled Data?</b></br>
Authors: , Zhang, Jingfeng, Han, Bo, Niu, Gang, Liu, Tongliang, Sugiyama, Masashi</br>
  Deep neural networks (DNNs) are incredibly brittle due to adversarial examples. To robustify DNNs, adversarial training was proposed, which requires large-scale but well-labeled data. However, it is quite expensive to annotate large-scale data well. To compensate for this shortage, several seminal works are utilizing large-scale unlabeled data. In this paper, we observe that seminal works do not perform well, since the quality of pseudo labels on unlabeled data is quite poor, especially when the amount of unlabeled data is significantly larger than that of labeled data. We believe that the quality of pseudo labels is the bottleneck of adversarial learning with unlabeled data. To tackle this bottleneck, we leverage deep co-training, which trains two deep networks and encourages two networks diverged by exploiting peer\'s adversarial examples. Based on deep co-training, we propose robust co-training (RCT) for adversarial learning with unlabeled data. We conduct comprehensive experiments on CIFAR-10 and SVHN datasets. Empirical results demonstrate that our RCT can significantly <font color="#00be00">outperform</font> baselines (e.g., robust self-training (RST)) in both standard test accuracy and robust test accuracy w.r.t. different datasets, different network structures, and different types of adversarial training. </br></br>

<a href='http://arxiv.org/pdf/1911.08582.pdf'>1911.08582</a> &nbsp&nbsp (cs:RO, cs:CV, cs:ML) &nbsp&nbsp 0.0662баллов, №341</br>
<b>End to end collision avoidance based on optical flow and neural networks</b></br>
Authors: , Blumenkamp, Jan</br>
  Optical flow is believed to play an important role in the agile flight of birds and insects. Even though it is a very simple concept, it is rarely used in computer vision for collision avoidance. This work implements a neural network based collision avoidance which was deployed and evaluated on a solely for this purpose refitted car. </br></br>

<a href='http://arxiv.org/pdf/1911.08414.pdf'>1911.08414</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.0659баллов, №342</br>
<b>Comparison of Deep learning models on time series forecasting : a case\n  study of Dissolved Oxygen Prediction</b></br>
Authors: , Qin, Hongqian</br>
  Deep learning has achieved impressive prediction performance in the field of sequence learning recently. Dissolved oxygen prediction, as a kind of time-series forecasting, is suitable for this technique. Although many researchers have developed hybrid models or variant models based on deep learning techniques, there is no comprehensive and sound comparison among the deep learning models in this field currently. Plus, most previous studies focused on one-step forecasting by using a small data set. As the convenient access to high-frequency data, this paper compares multi-step deep learning forecasting by using walk-forward validation. Specifically, we test Convolutional Neural Network (CNN), Temporal Convolutional Network (TCN), Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), Bidirectional Recurrent Neural Network (BiRNN) based on the real-time data recorded automatically at a fixed observation point in the Yangtze River from 2012 to 2016. By comparing the average accumulated statistical metrics of root mean square error (RMSE), mean absolute error (MAE), and coefficient of determination in each time step, We find for multi-step time series forecasting, the average performance of each time step does not decrease linearly. GRU <font color="#00be00">outperform</font>s other models with significant advantages. </br></br>

<a href='http://arxiv.org/pdf/1911.09344.pdf'>1911.09344</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.0642баллов, №343</br>
<b>Convolutional Mixture Density Recurrent Neural Network for Predicting\n  User Location with WiFi Fingerprints</b></br>
Authors: , Qian, Weizhu, Lauri, Fabrice, Gechter, Franck</br>
  Predicting smartphone users activity using WiFi fingerprints has been a popular approach for indoor positioning in recent years. However, such a high dimensional time-series prediction problem can be very tricky to solve. To address this issue, we propose a novel deep learning model, the convolutional mixture density recurrent neural network (CMDRNN), which combines the strengths of convolutional neural networks, recurrent neural networks and mixture density networks. In our model, the CNN sub-model is employed to detect the feature of the high dimensional input, the RNN sub-model is utilized to capture the time dependency and the MDN sub-model is for predicting the final output. For validation, we conduct the experiments on the <font color="#009600">real-world</font> dataset and the obtained results illustrate the effectiveness of our method. </br></br>

<a href='http://arxiv.org/pdf/1911.01679.pdf'>1911.01679</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.0619баллов, №344</br>
<b>Apprenticeship Learning via Frank-Wolfe</b></br>
Authors: , Zahavy, Tom, Cohen, Alon, Kaplan, Haim, Mansour, Yishay</br>
  We consider the applications of the Frank-Wolfe (FW) algorithm for Apprenticeship Learning (AL). In this setting, we are given a Markov Decision Process (MDP) without an explicit reward function. Instead, we observe an expert that acts according to some policy, and the goal is to find a policy whose feature expectations are closest to those of the expert policy. We formulate this problem as finding the projection of the feature expectations of the expert on the feature expectations polytope -- the convex hull of the feature expectations of all the deterministic policies in the MDP. We show that this formulation is equivalent to the AL objective and that solving this problem using the FW algorithm is equivalent well-known Projection method of Abbeel and Ng (2004). This insight allows us to analyze AL with tools from convex optimization literature and derive tighter convergence bounds on AL. Specifically, we show that a variation of the FW method that is based on taking &quot;away steps&quot; achieves a linear rate of convergence when applied to AL and that a stochastic version of the FW algorithm can be used to avoid precise estimation of feature expectations. We also experimentally show that this version <font color="#00be00">outperform</font>s the FW baseline. To the best of our knowledge, this is the first work that shows linear convergence rates for AL. </br></br>

<a href='http://arxiv.org/pdf/1911.02067.pdf'>1911.02067</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.0598баллов, №345</br>
<b>Robo-advising: Learning Investors\' Risk Preferences via Portfolio\n  Choices</b></br>
Authors: , Alsabah, Humoud, Capponi, Agostino, Lacedelli, Octavio Ruiz, Stern, Matt</br>
  We introduce a <font color="#00be00">reinforcement learning</font> framework for retail robo-advising. The robo-advisor does not know the investor\'s risk preference, but learns it over time by observing her portfolio choices in different <font color="#be00be">market</font> environments. We develop an exploration-exploitation algorithm which trades off costly solicitations of portfolio choices by the investor with autonomous trading decisions based on stale estimates of investor\'s risk aversion. We show that the algorithm\'s value function converges to the optimal value function of an omniscient robo-advisor over a number of periods that is polynomial in the state and action space. By correcting for the investor\'s mistakes, the robo-advisor may <font color="#00be00">outperform</font> a stand-alone investor, regardless of the investor\'s opportunity cost for making portfolio decisions. </br></br>

<a href='http://arxiv.org/pdf/1911.03922.pdf'>1911.03922</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 0.0598баллов, №346</br>
<b>Un systeme de lemmatisation pour les applications de TALN</b></br>
Authors: , Bessou, Sadik, Louail, Mohamed, Refoufi, Allaoua, Kadem, Zehour, Touahria, Mohamed</br>
  This paper presents a method of stemming for the Arabian texts based on the linguistic techniques of the natural language processing. This method leans on the notion of scheme (one of the strong points of the morphology of the Arabian language). The advantage of this approach is that it doesn\'t use a dictionary of inflexions but a smart dynamic recognition of the different words of the language. </br></br>

<a href='http://arxiv.org/pdf/1911.08794.pdf'>1911.08794</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 0.0580баллов, №347</br>
<b>Natural Language Generation Challenges for Explainable AI</b></br>
Authors: , Reiter, Ehud</br>
  Good quality explanations of artificial intelligence (XAI) reasoning must be written (and evaluated) for an explanatory purpose, targeted towards their readers, have a good narrative and causal structure, and highlight where uncertainty and data quality affect the AI output. I discuss these challenges from a Natural Language Generation (NLG) perspective, and highlight four specific NLG for XAI research challenges. </br></br>

<a href='http://arxiv.org/pdf/1911.07156.pdf'>1911.07156</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 0.0568баллов, №348</br>
<b>Mining Unfollow Behavior in Large-Scale Online Social Networks via\n  Spatial-Temporal Interaction</b></br>
Authors: , Wu, Haozhe, Hu, Zhiyuan, Jia, Jia, Bu, Yaohua, He, Xiangnan, Chua, Tat-Seng</br>
  Online Social Networks (OSNs) evolve through two pervasive behaviors: follow and unfollow, which respectively signify relationship creation and relationship dissolution. Researches on social network evolution mainly focus on the follow behavior, while the unfollow behavior has largely been ignored. Mining unfollow behavior is challenging because user\'s decision on unfollow is not only affected by the simple combination of user\'s attributes like informativeness and reciprocity, but also affected by the complex interaction among them. Meanwhile, prior datasets seldom contain sufficient records for inferring such complex interaction. To address these issues, we first construct a large-scale <font color="#009600">real-world</font> Weibo dataset, which records detailed post content and relationship dynamics of 1.8 million <font color="#be00be">Chinese</font> users. Next, we define user\'s attributes as two categories: spatial attributes (e.g., social role of user) and temporal attributes (e.g., post content of user). Leveraging the constructed dataset, we systematically study how the interaction effects between user\'s spatial and temporal attributes contribute to the unfollow behavior. Afterwards, we propose a novel unified model with heterogeneous information (UMHI) for unfollow prediction. Specifically, our UMHI model: 1) captures user\'s spatial attributes through social network structure; 2) infers user\'s temporal attributes through user-posted content and unfollow history; and 3) models the interaction between spatial and temporal attributes by the nonlinear MLP layers. Comprehensive evaluations on the constructed dataset demonstrate that the proposed UMHI model <font color="#00be00">outperform</font>s baseline methods by 16.44% on average in terms of precision. In addition, factor analyses verify that both spatial attributes and temporal attributes are essential for mining unfollow behavior. </br></br>

<a href='http://arxiv.org/pdf/1911.08548.pdf'>1911.08548</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.0549баллов, №349</br>
<b>Cross-Class Relevance Learning for Temporal Concept Localization</b></br>
Authors: , Ma, Junwei, Gorti, Satya Krishna, Volkovs, Maksims, Stanevich, Ilya, Yu, Guangwei</br>
  We present a novel Cross-Class Relevance Learning approach for the task of temporal concept localization. Most localization architectures rely on feature extraction layers followed by a classification layer which outputs class probabilities for each segment. However, in many <font color="#009600">real-world</font> applications classes can exhibit complex relationships that are difficult to model with this architecture. In contrast, we propose to incorporate target class and class-related features as input, and learn a pairwise binary model to predict general segment to class relevance. This facilitates learning of shared information between classes, and allows for arbitrary class-specific feature engineering. We apply this approach to the 3rd YouTube-8M Video Understanding Challenge together with other leading models, and achieve first place out of over 280 teams. In this paper we describe our approach and show some empirical results. </br></br>

<a href='http://arxiv.org/pdf/1911.08743.pdf'>1911.08743</a> &nbsp&nbsp (cs:CL, cs:AI) &nbsp&nbsp 0.0538баллов, №350</br>
<b>SemanticZ at SemEval-2016 Task 3: Ranking Relevant Answers in Community\n  Question Answering Using Semantic Similarity Based on Fine-tuned Word\n  Embeddings</b></br>
Authors: , Mihaylov, Todor, Nakov, Preslav</br>
  We describe our system for finding good answers in a community forum, as defined in SemEval-2016, Task 3 on Community Question Answering. Our approach relies on several semantic similarity features based on fine-tuned word embeddings and topics similarities. In the main Subtask C, our primary submission was ranked third, with a MAP of 51.68 and accuracy of 69.94. In Subtask A, our primary submission was also third, with MAP of 77.58 and accuracy of 73.39. </br></br>

<a href='http://arxiv.org/pdf/1911.05177.pdf'>1911.05177</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp 0.0517баллов, №351</br>
<b>Numerical and experimental realization of analytical SLAM</b></br>
Authors: , Bucko, Jozef, Sandamirskaya, Yulia, Slotine, Jean-Jacques</br>
  Analytical approach to SLAM problem was introduced in the recent years. In our work we investigate the method numerically with the motivation of using the algorithm in a real hardware experiments. We perform a robustness test of the algorithm and apply it to the robotic hardware in two different setups. In one we try to recover a map of the environment using bearing angle measurements and radial distance measurements. The another setup utilizes only bearing angle information. </br></br>

<a href='http://arxiv.org/pdf/1911.07164.pdf'>1911.07164</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.0510баллов, №352</br>
<b>Meta-Reinforced Synthetic Data for One-Shot Fine-Grained Visual\n  Recognition</b></br>
Authors: , Tsutsui, Satoshi, Fu, Yanwei, Crandall, David</br>
  <font color="#009600">One-shot</font> fine-grained visual recognition often suffers from the problem of training data scarcity for new fine-grained classes. To alleviate this problem, an off-the-shelf image generator can be applied to synthesize additional training images, but these synthesized images are often not helpful for actually improving the accuracy of one-shot fine-grained recognition. This paper proposes a meta-learning framework to combine generated images with original images, so that the resulting ``hybrid\'\' training images can improve one-shot learning. Specifically, the generic image generator is updated by a few training instances of novel classes, and a Meta Image Reinforcing Network (MetaIRNet) is proposed to conduct one-shot fine-grained recognition as well as image reinforcement. The model is trained in an end-to-end manner, and our experiments demonstrate consistent improvement over baselines on one-shot fine-grained image classification benchmarks. </br></br>

<a href='http://arxiv.org/pdf/1911.08276.pdf'>1911.08276</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp 0.0504баллов, №353</br>
<b>Generating relevant scenarios for intelligent transportation service</b></br>
Authors: , Addoui, Ismet, Chouaki, Tarek, Colli, Ambrogio Delli</br>
  This paper addresses risk assessment issues while conceiving complex systems. Indeed, project stakeholders have to share the same problems understanding allowing to undertake rational and optimal decisions. We propose an approach based on Natural Language Processing (NLP) techniques to improve systems quality requirements such as consistency and completeness. We assess the relevancy of our approaches through experimentations and highlighted feedbacks from project stakeholders and players. </br></br>

<a href='http://arxiv.org/pdf/1911.07112.pdf'>1911.07112</a> &nbsp&nbsp (cs:NE) &nbsp&nbsp 0.0500баллов, №354</br>
<b>Particle Swarm and EDAs</b></br>
Authors: , Jenkins, Alison, Gupta, Vinika, Myrick, Alexis, Lenoir, Mary</br>
  The Particle Swarm Optimization (PSO) algorithm is developed for solving the Schaffer F6 function in fewer than 4000 function evaluations on a total of 30 runs. Four variations of the Full Model of Particle Swarm Optimization (PSO) algorithms are presented which consist of combinations of Ring and Star topologies with Synchronous and Asynchronous updates. The Full Model with combinations of Ring and Star topologies in combination with Synchronous and Asynchronous Particle Updates is explored. </br></br>

<a href='http://arxiv.org/pdf/1911.07042.pdf'>1911.07042</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.0500баллов, №355</br>
<b>Automatic Annotation of Hip Anatomy in Fluoroscopy for Robust and\n  Efficient 2D/3D Registration</b></br>
Authors: , Grupp, Robert, Unberath, Mathias, Gao, Cong, Hegeman, Rachel, Murphy, Ryan, Alexander, Clayton, Otake, Yoshito, McArthur, Benjamin, Armand, Mehran, Taylor, Russell</br>
  Fluoroscopy is the standard imaging modality used to guide hip surgery and is therefore a natural sensor for computer-assisted navigation. In order to efficiently solve the complex registration problems presented during navigation, human-assisted annotations of the intraoperative image are typically required. This manual initialization interferes with the surgical workflow and diminishes any advantages gained from navigation. We propose a method for fully automatic registration using annotations produced by a neural network. Neural networks are trained to simultaneously segment <font color="blue">anatomy</font> and identify landmarks in fluoroscopy. Training data is obtained using an intraoperatively incompatible 2D/3D registration of hip anatomy. Ground truth 2D labels are established using projected 3D annotations. Intraoperative registration couples an intensity-based strategy with annotations inferred by the network and requires no human assistance. Ground truth labels were obtained in 366 fluoroscopic images across 6 cadaveric specimens. In a leave-one-subject-out experiment, networks obtained mean dice coefficients for left and right hemipelves, left and right femurs of 0.86, 0.87, 0.90, and 0.84. The mean 2D landmark error was 5.0 mm. The pelvis was registered within 1 degree for 86% of the images when using the proposed intraoperative approach with an average runtime of 7 seconds. In comparison, an intensity-only approach without manual initialization, registered the pelvis to 1 degree in 18% of images. We have created the first accurately annotated, non-synthetic, dataset of hip fluoroscopy. By using these annotations as training data for neural networks, <font color="red">state of the art</font> performance in fluoroscopic <font color="#be00be">segmentation</font> and landmark localization was achieved. Integrating these annotations allows for a robust, fully automatic, and efficient intraoperative registration during fluoroscopic navigation of the hip. </br></br>

<a href='http://arxiv.org/pdf/1911.08600.pdf'>1911.08600</a> &nbsp&nbsp (cs:NE) &nbsp&nbsp 0.0475баллов, №356</br>
<b>Steepest ascent can be exponential in bounded treewidth problems</b></br>
Authors: , Cohen, David A., Cooper, Martin C., Kaznatcheev, Artem, Wallace, Mark</br>
  We investigate the complexity of local search based on steepest ascent. We show that even when all variables have domains of size two and the underlying constraint graph (graph of variable interactions) has bounded treewidth (in our construction, treewidth is 7), there are fitness landscapes for which an exponential number of steps may be required to reach a local optimum. This is an improvement on prior recursive constructions of long steepest ascents, which we prove to need constraint graphs of unbounded treewidth. </br></br>

<a href='http://arxiv.org/pdf/1911.08976.pdf'>1911.08976</a> &nbsp&nbsp (cs:CL, cs:AI) &nbsp&nbsp 0.0468баллов, №357</br>
<b>Red Dragon AI at TextGraphs 2019 Shared Task: Language Model Assisted\n  Explanation Generation</b></br>
Authors: , Chia, Yew Ken, Witteveen, Sam, Andrews, Martin</br>
  The TextGraphs-13 Shared Task on Explanation Regeneration asked participants to develop methods to reconstruct gold explanations for elementary science questions. Red Dragon AI\'s entries used the language of the questions and explanation text directly, rather than a constructing a separate graph-like representation. Our leaderboard submission placed us 3rd in the competition, but we present here three methods of increasing sophistication, each of which scored successively higher on the test set after the competition close. </br></br>

<a href='http://arxiv.org/pdf/1911.09373.pdf'>1911.09373</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp 0.0434баллов, №358</br>
<b>Entity Extraction with Knowledge from Web Scale Corpora</b></br>
Authors: , Wen, Zeyi, Huang, Zeyu, Zhang, Rui</br>
  Entity extraction is an important task in text mining and natural language processing. A popular method for entity extraction is by comparing substrings from free text against a dictionary of entities. In this paper, we present several techniques as a post-processing step for improving the effectiveness of the existing entity extraction technique. These techniques utilise models trained with the web-scale corpora which makes our techniques robust and versatile. Experiments show that our techniques bring a notable improvement on efficiency and effectiveness. </br></br>

<a href='http://arxiv.org/pdf/1911.07572.pdf'>1911.07572</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.0430баллов, №359</br>
<b>Bayesian Recurrent Framework for Missing Data Imputation and Prediction\n  with Clinical Time Series</b></br>
Authors: , Guo, Yang, Liu, Zhengyuan, Krishnswamy, Pavitra, Ramasamy, Savitha</br>
  <font color="#009600">Real-world</font> <font color="blue">clinic</font>al time series data sets exhibit a high prevalence of missing values. Hence, there is an increasing interest in missing data imputation. Traditional statistical approaches impose constraints on the data-generating process and decouple imputation from prediction. Recent works propose recurrent neural network based approaches for missing data imputation and prediction with time series data. However, they generate deterministic outputs and neglect the inherent uncertainty. In this work, we introduce a unified <font color="blue">Bayes</font>ian recurrent framework for simultaneous imputation and prediction on time series data sets. We evaluate our approach on two real-world mortality prediction tasks using the MIMIC-III and PhysioNet benchmark datasets. We demonstrate significant performance gains over <font color="red">state-of-the-art</font> methods, and provide strategies to use the resulting probability distributions to better assess reliability of the imputations and predictions. </br></br>

<a href='http://arxiv.org/pdf/1911.07348.pdf'>1911.07348</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp 0.0417баллов, №360</br>
<b>Robotic Sculpting with Collision-free Motion Planning in Voxel Space</b></br>
Authors: , Jain, Abhinav, Hutchinson, Seth, Dellaert, Frank</br>
  In this paper, we explore the task of robot sculpting. We propose a search based planning algorithm to solve the problem of sculpting by material removal with a multi-axis manipulator. We generate collision free trajectories for a manipulator using best-first search in voxel space. We also show significant speedup of our algorithm by using octrees to decompose the voxel space. We demonstrate our algorithm on a multi-axis manipulator in simulation by sculpting Michelangelo\'s Statue of David, evaluate certain metrics of our algorithm and discuss future goals for the project. </br></br>

<a href='http://arxiv.org/pdf/1911.08589.pdf'>1911.08589</a> &nbsp&nbsp (cs:NE) &nbsp&nbsp 0.0401баллов, №361</br>
<b>Machine Learning Classification Informed by a Functional Biophysical\n  System</b></br>
Authors: , Platt, Jason A., Miller, Anna, Abarbanel, Henry D. I.</br>
  We present a novel machine learning architecture for classification suggested by experiments on the insect olfactory system. The network separates odors via a winnerless competition network, then classifies objects by projection into a high dimensional space where a support vector machine provides more precision in classification. We build this network using biophysical models of neurons with our results showing high discrimination among inputs and exceptional robustness to noise. The same circuitry accurately identifies the amplitudes of mixtures of the odors on which it has been trained. </br></br>

<a href='http://arxiv.org/pdf/1911.08460.pdf'>1911.08460</a> &nbsp&nbsp (cs:CL, cs:SD) &nbsp&nbsp 0.0396баллов, №362</br>
<b>End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern\n  Architectures</b></br>
Authors: , Synnaeve, Gabriel, Xu, Qiantong, Kahn, Jacob, Grave, Edouard, Likhomanenko, Tatiana, Pratap, Vineel, Sriram, Anuroop, Liptchinsky, Vitaliy, Collobert, Ronan</br>
  We study ResNet-, Time-Depth Separable ConvNets-, and Transformer-based acoustic models, trained with CTC or Seq2Seq criterions. We perform experiments on the LibriSpeech dataset, with and without LM decoding, optionally with beam rescoring. We reach 5.18% WER with external language models for decoding and rescoring. Additionally, we leverage the unlabeled data from LibriVox by doing semi-supervised training and show that it is possible to reach 5.29% WER on test-other without decoding, and 4.11% WER with decoding and rescoring, with only the standard 960 hours from LibriSpeech as labeled data. </br></br>

<a href='http://arxiv.org/pdf/1911.09478.pdf'>1911.09478</a> &nbsp&nbsp (cs:CL, cs:AI) &nbsp&nbsp 0.0394баллов, №363</br>
<b>What Do You Mean `Why?\': Resolving Sluices in Conversations</b></br>
Authors: , Hansen, Victor Petr&#xe9;n Bach, S&#xf8;gaard, Anders</br>
  In conversation, we often ask one-word questions such as `Why?\' or `Who?\'. Such questions are typically easy for humans to answer, but can be hard for computers, because their resolution requires retrieving both the right semantic frames and the right arguments from context. This paper introduces the novel ellipsis resolution task of resolving such one-word questions, referred to as sluices in linguistics. We present a crowd-sourced dataset containing annotations of sluices from over 4,000 dialogues collected from conversational QA datasets, as well as a series of strong baseline architectures. </br></br>

<a href='http://arxiv.org/pdf/1911.09230.pdf'>1911.09230</a> &nbsp&nbsp (cs:NE) &nbsp&nbsp 0.0378баллов, №364</br>
<b>Predictive Coding as Stimulus Avoidance in Spiking Neural Networks</b></br>
Authors: , Masumori, Atsushi, Sinapayen, Lana, Ikegami, Takashi</br>
  Predictive coding can be regarded as a function which reduces the error between an input signal and a top-down prediction. If reducing the error is equivalent to reducing the influence of stimuli from the environment, predictive coding can be regarded as stimulation avoidance by prediction. Our previous studies showed that action and selection for stimulation avoidance emerge in spiking neural networks through spike-timing dependent plasticity (STDP). In this study, we demonstrate that spiking neural networks with random structure spontaneously learn to predict temporal sequences of stimuli based solely on STDP. </br></br>

<a href='http://arxiv.org/pdf/1911.08528.pdf'>1911.08528</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp 0.0366баллов, №365</br>
<b>Haptic Sketches on the Arm for Manipulation in Virtual Reality</b></br>
Authors: , Sarac, Mine, Okamura, Allison M., Di Luca, Massimiliano</br>
  We propose a haptic system that applies forces or skin deformation to the user\'s arm, rather than at the fingertips, for believable interaction with virtual objects as an alternative to complex thimble devices. Such a haptic system would be able to convey information to the arm instead of the fingertips, even though the user manipulates virtual objects using their hands. We developed a set of haptic sketches to determine which directions of skin deformation are deemed more believable during a grasp and lift task. Subjective reports indicate that normal forces were the most believable feedback to represent this interaction. </br></br>

<a href='http://arxiv.org/pdf/1911.07776.pdf'>1911.07776</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.0353баллов, №366</br>
<b>DeepPFCN: Deep Parallel Feature Consensus Network For Person\n  Re-Identification</b></br>
Authors: , Singh, Shubham Kumar, Miyapuram, Krishna P, Raman, Shanmuganathan</br>
  Person <font color="blue">re-identification</font> aims to associate images of the same person over multiple non-overlapping camera views at different times. Depending on the human operator, manual re-identification in large camera networks is highly time consuming and erroneous. Automated person re-identification is required due to the extensive quantity of visual data produced by rapid inflation of large scale distributed multi-camera systems. The <font color="red">state-of-the-art</font> works focus on learning and factorize person appearance features into latent discriminative factors at multiple semantic levels. We propose Deep Parallel Feature Consensus Network (DeepPFCN), a novel network architecture that learns multi-scale person appearance features using convolutional neural networks. This model factorizes the visual appearance of a person into latent discriminative factors at multiple semantic levels. Finally consensus is built. The feature representations learned by DeepPFCN are more robust for the person re-identification task, as we learn discriminative scale-specific features and maximize multi-scale feature fusion selections in multi-scale image inputs. We further exploit average and max pooling in separate scale for person-specific task to discriminate features globally and locally. We demonstrate the re-identification advantages of the proposed DeepPFCN model over the state-of-the-art re-identification methods on three benchmark datasets: <font color="#be00be">Market</font>1501, DukeMTMCreID, and CUHK03. We have achieved mAP results of 75.8%, 64.3%, and 52.6% respectively on these benchmark datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.08249.pdf'>1911.08249</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 0.0335баллов, №367</br>
<b>An Accuracy-Enhanced Stemming Algorithm for Arabic Information Retrieval</b></br>
Authors: , Bessou, Sadik, Touahria, Mohamed</br>
  This paper provides a method for indexing and retrieving Arabic texts, based on natural language processing. Our approach exploits the notion of template in word stemming and replaces the words by their stems. This technique has proven to be effective since it has returned significant relevant retrieval results by decreasing silence during the retrieval phase. Series of experiments have been conducted to test the performance of the proposed algorithm ESAIR (Enhanced Stemmer for Arabic Information Retrieval). The results obtained indicate that the algorithm extracts the exact root with an accuracy rate up to 96% and hence, improving information retrieval. </br></br>

<a href='http://arxiv.org/pdf/1911.08684.pdf'>1911.08684</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.0311баллов, №368</br>
<b>TITAN: A Spatiotemporal Feature Learning Framework for Traffic Incident\n  Duration Prediction</b></br>
Authors: , Fu, Kaiqun, Ji, Taoran, Zhao, Liang, Lu, Chang-Tien</br>
  Critical incident stages identification and reasonable prediction of traffic incident duration are essential in traffic incident management. In this paper, we propose a traffic incident duration prediction model that simultaneously predicts the impact of the traffic incidents and identifies the critical groups of temporal features via a multi-task learning framework. First, we formulate a sparsity optimization problem that extracts low-level temporal features based on traffic speed readings and then generalizes higher level features as phases of traffic incidents. Second, we propose novel constraints on feature similarity exploiting prior knowledge about the spatial connectivity of the road network to predict the incident duration. The proposed problem is challenging to solve due to the orthogonality constraints, non-convexity objective, and non-smoothness penalties. We develop an algorithm based on the alternating direction method of multipliers (ADMM) framework to solve the proposed formulation. Extensive experiments and comparisons to other models on <font color="#009600">real-world</font> traffic data and traffic incident records justify the efficacy of our model. </br></br>

<a href='http://arxiv.org/pdf/1911.07101.pdf'>1911.07101</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.0305баллов, №369</br>
<b>Glyph: Fast and Accurately Training Deep Neural Networks on Encrypted\n  Data</b></br>
Authors: , Lou, Qian, Feng, Bo, Fox, Geoffrey C., Jiang, Lei</br>
  Big data is one of the cornerstones to enabling and training deep neural networks (DNNs). Because of the lack of expertise, to gain benefits from their data, average users have to rely on and upload their <font color="#be00be">private</font> data to big data companies they may not trust. Due to the compliance, legal, or <font color="#be00be">privacy</font> constraints, most users are willing to contribute only their encrypted data, and lack interests or resources to join the training of DNNs in cloud. To train a DNN on encrypted data in a completely non-interactive way, a recent work proposes a fully homomorphic encryption (FHE)-based technique implementing all activations in the neural network by \\textit{Brakerski-Gentry-Vaikuntanathan (BGV)}-based lookup tables. However, such inefficient lookup-table-based activations significantly prolong the training latency of privacy-preserving DNNs.   In this paper, we propose, Glyph, a FHE-based scheme to fast and accurately train DNNs on encrypted data by switching between TFHE (Fast Fully Homomorphic Encryption over the Torus) and BGV cryptosystems. Glyph uses logic-operation-friendly TFHE to implement nonlinear activations, while adopts vectorial-arithmetic-friendly BGV to perform multiply-accumulation (MAC) operations. Glyph further applies transfer learning on the training of DNNs to improve the test accuracy and reduce the number of MAC operations between ciphertext and ciphertext in convolutional layers. Our experimental results show Glyph obtains the <font color="red">state-of-the-art</font> test accuracy, but reduces the training latency by $99\\%$ over the prior FHE-based technique on various encrypted datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.08618.pdf'>1911.08618</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.0304баллов, №370</br>
<b>Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA</b></br>
Authors: , Patro, Badri N., Anupriy, Namboodiri, Vinay P.</br>
  In this paper, we aim to obtain improved attention for a visual question answering (VQA) task. It is challenging to provide supervision for attention. An observation we make is that visual explanations as obtained through class activation mappings (specifically Grad-CAM) that are meant to explain the performance of various networks could form a means of supervision. However, as the distributions of attention maps and that of Grad-CAMs differ, it would not be suitable to directly use these as a form of supervision. Rather, we propose the use of a discriminator that aims to distinguish samples of visual explanation and attention maps. The use of adversarial training of the attention regions as a two-player game between attention and explanation serves to bring the distributions of attention maps and visual explanations closer. Significantly, we observe that providing such a means of supervision also results in attention maps that are more closely related to human attention resulting in a substantial improvement over baseline stacked attention network (SAN) models. It also results in a good improvement in rank correlation metric on the VQA task. This method can also be combined with recent MCB based methods and results in consistent improvement. We also provide comparisons with other means for learning distributions such as based on Correlation Alignment (Coral), Maximum Mean Discrepancy (MMD) and Mean Square Error (MSE) losses and observe that the adversarial loss <font color="#00be00">outperform</font>s the other forms of learning the attention maps. Visualization of the results also confirms our hypothesis that attention maps improve using this form of supervision. </br></br>

<a href='http://arxiv.org/pdf/1911.08876.pdf'>1911.08876</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp 0.0294баллов, №371</br>
<b>On Using SpecAugment for End-to-End Speech Translation</b></br>
Authors: , Bahar, Parnia, Zeyer, Albert, Schl&#xfc;ter, Ralf, Ney, Hermann</br>
  This work investigates a simple data augmentation technique, SpecAugment, for end-to-end speech translation. SpecAugment is a low-cost implementation method applied directly to the audio input features and it consists of masking blocks of frequency channels, and/or time steps. We apply SpecAugment on end-to-end speech translation tasks and achieve up to +2.2\\% \\BLEU on LibriSpeech Audiobooks En-&gt;Fr and +1.2% on IWSLT TED-talks En-&gt;De by alleviating overfitting to some extent. We also examine the effectiveness of the method in a variety of data scenarios and show that the method also leads to significant improvements in various data conditions irrespective of the amount of training data. </br></br>

<a href='http://arxiv.org/pdf/1911.05734.pdf'>1911.05734</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp 0.0264баллов, №372</br>
<b>Analysis of minima for geodesic and chordal cost for a minimal 2D\n  pose-graph SLAM problem</b></br>
Authors: , Kong, Felix H., Zhao, Jiaheng, Zhao, Liang, Huang, Shoudong</br>
  In this paper, we show that for a minimal pose-graph problem, even in the ideal case of perfect measurements and spherical covariance, using the so-called &quot;wrap function&quot; when comparing angles results in multiple suboptimal local minima. We numerically estimate regions of attraction to these local minima for some numerical examples, and give evidence to show that they are of nonzero measure. In contrast, under the same assumptions, we show that the \\textit{chordal distance} representation of angle error has a unique minimum up to periodicity. For chordal cost, we also search for initial conditions that fail to converge to the global minimum, and find that this occurs with far fewer points than with geodesic cost. </br></br>

<a href='http://arxiv.org/pdf/1911.06854.pdf'>1911.06854</a> &nbsp&nbsp (cs:ML, cs:AI, cs:RO, stat:ML) &nbsp&nbsp 0.0248баллов, №373</br>
<b>Empirical Study of Off-Policy Policy Evaluation for Reinforcement\n  Learning</b></br>
Authors: , Voloshin, Cameron, Le, Hoang M., Jiang, Nan, Yue, Yisong</br>
  Off-policy policy evaluation (OPE) is the problem of estimating the online performance of a policy using only pre-collected historical data generated by another policy. Given the increasing interest in deploying learning-based methods for safety-critical applications, many recent OPE methods have recently been proposed. Due to disparate experimental conditions from recent literature, the relative performance of current OPE methods is not well understood. In this work, we present the first comprehensive empirical analysis of a broad suite of OPE methods. Based on thousands of experiments and detailed empirical analyses, we offer a summarized set of guidelines for effectively using OPE in practice, and suggest directions for future research. </br></br>

<a href='http://arxiv.org/pdf/1911.06947.pdf'>1911.06947</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp 0.0209баллов, №374</br>
<b>Design of the First Insect-scale Spinning-wing Robot</b></br>
Authors: , Bhushan, Palak, Tomlin, Claire</br>
  Here we present the design of an insect-scale microrobot that generates lift by spinning its wings. This is in contrast to most other microrobot designs at this size scale which rely on flapping wings to produce lift. The robot has a wing span of 4 centimeters and weighs 133 milligrams. It spins its wings at 47 revolutions/second generating $&gt;$ 138 milligrams of lift while consuming approximately 60 milliwatts of total power and operating at a low voltage ($&lt;$ 3 V). Of the total power consumed 8.8 milliwatts is mechanical power generated, part of which goes towards spinning the wings, and 51 milliwatts is wasted in resistive Joule heating. With a lift-to-power ratio of 2.3 grams/W, its performance is at par with the best reported flapping wing devices at the insect-scale. </br></br>

<a href='http://arxiv.org/pdf/1911.09247.pdf'>1911.09247</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 0.0207баллов, №375</br>
<b>How to Ask Better Questions? A Large-Scale Multi-Domain Dataset for\n  Rewriting Ill-Formed Questions</b></br>
Authors: , Chu, Zewei, Chen, Mingda, Chen, Jing, Wang, Miaosen, Gimpel, Kevin, Faruqui, Manaal, Si, Xiance</br>
  We present a large-scale dataset for the task of rewriting an ill-formed natural language question to a well-formed one. Our multi-domain question rewriting MQR dataset is constructed from human contributed Stack Exchange question edit histories. The dataset contains 427,719 question pairs which come from 303 domains. We provide human annotations for a subset of the dataset as a quality estimate. When moving from ill-formed to well-formed questions, the question quality improves by an average of 45 points across three aspects. We train sequence-to-sequence neural models on the constructed dataset and obtain an improvement of 13.2% in BLEU-4 over baseline methods built from other data resources. We release the MQR dataset to encourage research on the problem of question rewriting. </br></br>

<a href='http://arxiv.org/pdf/1911.07992.pdf'>1911.07992</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp 0.0207баллов, №376</br>
<b>Long-Term Personalization of an In-Home Socially Assistive Robot for\n  Children With Autism Spectrum Disorders</b></br>
Authors: , Clabaugh, Caitlyn, Mahajan, Kartik, Jain, Shomik, Pakkar, Roxanna, Becerra, David, Shi, Zhonghao, Deng, Eric, Lee, Rhianna, Ragusa, Gisele, Matari&#x107;, Maja</br>
  Socially assistive robots (SAR) have shown great potential to augment the social and educational development of children with autism spectrum <font color="blue">disorder</font>s (ASD). As SAR continues to substantiate itself as an effective enhancement to human intervention, researchers have sought to study its longitudinal impacts in <font color="#009600">real-world</font> environments, including the home. Computational personalization stands out as a central computational challenge as it is necessary to enable SAR systems to adapt to each child\'s unique and changing needs. Toward that end, we formalized personalization as a <font color="#00be00">hierarchical</font> human robot learning framework (hHRL) consisting of five controllers (disclosure, promise, instruction, feedback, and inquiry) mediated by a meta-controller that utilized <font color="#00be00">reinforcement learning</font> to personalize instruction challenge levels and robot feedback based on each user\'s unique learning patterns. We instantiated and evaluated the approach in a study with 17 children with ASD, aged 3 to 7 years old, over month-long interventions in their homes. Our findings demonstrate that the fully autonomous SAR system was able to personalize its instruction and feedback over time to each child\'s proficiency. As a result, every child participant showed improvements in targeted skills and long-term retention of intervention content. Moreover, all child users were engaged for a majority of the intervention, and their families reported the SAR system to be useful and adaptable. In summary, our results show that autonomous, personalized SAR interventions are both feasible and effective in providing long-term in-home developmental support for children with diverse learning needs. </br></br>

<a href='http://arxiv.org/pdf/1911.09645.pdf'>1911.09645</a> &nbsp&nbsp (cs:SD, cs:CL, cs:ML) &nbsp&nbsp 0.0205баллов, №377</br>
<b>Prosody Transfer in Neural Text to Speech Using Global Pitch and\n  Loudness Features</b></br>
Authors: , Gururani, Siddharth, Gupta, Kilol, Shah, Dhaval, Shakeri, Zahra, Pinto, Jervis</br>
  This paper presents a simple yet effective method to achieve prosody transfer from a reference speech signal to synthesized speech. The main idea is to incorporate well-known acoustic correlates of prosody such as pitch and loudness contours of the reference speech into a modern neural text-to-speech (TTS) synthesizer such as Tacotron2 (TC2). More specifically, a small set of acoustic features are extracted from the reference audio and then used to condition a TC2 synthesizer. The trained model is evaluated using subjective listening tests and novel objective evaluations of prosody transfer are proposed. Listening tests show that the synthesized speech is rated as highly natural and that prosody is successfully transferred from the reference speech signal to the synthesized signal. </br></br>

<a href='http://arxiv.org/pdf/1911.07736.pdf'>1911.07736</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp 0.0203баллов, №378</br>
<b>Modeling Gestalt Visual Reasoning on the Raven\'s Progressive Matrices\n  Intelligence Test Using Generative Image Inpainting Techniques</b></br>
Authors: , Hua, Tianyu, Kunda, Maithilee</br>
  Psychologists recognize Raven\'s Progressive Matrices as a very effective test of general human intelligence. While many computational models have been developed by the AI community to investigate different forms of top-down, deliberative reasoning on the test, there has been less research on bottom-up perceptual processes, like Gestalt image completion, that are also critical in human test performance. In this work, we investigate how Gestalt visual reasoning on the Raven\'s test can be modeled using generative image inpainting techniques from computer vision. We demonstrate that a self-supervised inpainting model trained only on photorealistic images of objects achieves a score of 27/36 on the Colored Progressive Matrices, which corresponds to average performance for nine-year-old children. We also show that models trained on other datasets (faces, places, and textures) do not perform as well. Our results illustrate how learning visual regularities in <font color="#009600">real-world</font> images can translate into successful reasoning about artificial test stimuli. On the flip side, our results also highlight the limitations of such transfer, which may explain why intelligence tests like the Raven\'s are often sensitive to people\'s individual sociocultural backgrounds. </br></br>

<a href='http://arxiv.org/pdf/1910.10706.pdf'>1910.10706</a> &nbsp&nbsp (cs:CV, cs:CL) &nbsp&nbsp 0.0179баллов, №379</br>
<b>KnowIT VQA: Answering Knowledge-Based Questions about Videos</b></br>
Authors: , Garcia, Noa, Otani, Mayu, Chu, Chenhui, Nakashima, Yuta</br>
  We propose a novel video understanding task by fusing knowledge-based and video question answering. First, we introduce KnowIT VQA, a video dataset with 24,282 human-generated question-answer pairs about a popular sitcom. The dataset combines visual, textual and temporal coherence reasoning together with knowledge-based questions, which need of the experience obtained from the viewing of the series to be answered. Second, we propose a video understanding model by combining the visual and textual video content with specific knowledge about the show. Our main findings are: (i) the incorporation of knowledge produces outstanding improvements for VQA in video, and (ii) the performance on KnowIT VQA still lags well behind human accuracy, indicating its usefulness for studying current video modelling limitations. </br></br>

<a href='http://arxiv.org/pdf/1911.06859.pdf'>1911.06859</a> &nbsp&nbsp (cs:ML, cs:NE) &nbsp&nbsp 0.0175баллов, №380</br>
<b>NeuMMU: Architectural Support for Efficient Address Translations in\n  Neural Processing Units</b></br>
Authors: , Hyun, Bongjoon, Kwon, Youngeun, Choi, Yujeong, Kim, John, Rhu, Minsoo</br>
  To satisfy the compute and memory demands of deep neural networks, neural processing units (NPUs) are widely being utilized for accelerating deep learning algorithms. Similar to how GPUs have evolved from a slave device into a mainstream processor architecture, it is likely that NPUs will become first class citizens in this fast-evolving heterogeneous architecture space. This paper makes a case for enabling address translation in NPUs to decouple the virtual and physical memory address space. Through a careful data-driven application characterization study, we root-cause several limitations of prior GPU-centric address translation schemes and propose a memory management unit (MMU) that is tailored for NPUs. Compared to an oracular MMU design point, our proposal incurs only an average 0.06% performance overhead. </br></br>

<a href='http://arxiv.org/pdf/1911.07383.pdf'>1911.07383</a> &nbsp&nbsp (cs:CV, cs:ML, cs:RO) &nbsp&nbsp 0.0167баллов, №381</br>
<b>Towards Robust RGB-D Human Mesh Recovery</b></br>
Authors: , Li, Ren, Cai, Changjiang, Georgakis, Georgios, Karanam, Srikrishna, Chen, Terrence, Wu, Ziyan</br>
  We consider the problem of human pose estimation. While much recent work has focused on the RGB domain, these techniques are inherently under-constrained since there can be many 3D configurations that explain the same 2D projection. To this end, we propose a new method that uses RGB-D data to estimate a parametric human mesh model. Our key innovations include (a) the design of a new dynamic data fusion module that facilitates learning with a combination of RGB-only and RGB-D datasets, (b) a new constraint generator module that provides SMPL supervisory signals when explicit SMPL annotations are not available, and (c) the design of a new depth ranking learning objective, all of which enable principled model training with RGB-D data. We conduct extensive experiments on a variety of RGB-D datasets to demonstrate efficacy. </br></br>

<a href='http://arxiv.org/pdf/1911.08117.pdf'>1911.08117</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp 0.0142баллов, №382</br>
<b>A Hybrid Morpheme-Word Representation for Machine Translation of\n  Morphologically Rich Languages</b></br>
Authors: , Luong, Minh-Thang, Nakov, Preslav, Kan, Min-Yen</br>
  We propose a language-independent approach for improving statistical machine translation for morphologically rich languages using a hybrid morpheme-word representation where the basic unit of translation is the morpheme, but word boundaries are respected at all stages of the translation process. Our model extends the classic phrase-based model by means of (1) word boundary-aware morpheme-level phrase extraction, (2) minimum error-rate training for a morpheme-level translation model using word-level BLEU, and (3) joint scoring with morpheme- and word-level language models. Further improvements are achieved by combining our model with the classic one. The evaluation on English to Finnish using Europarl (714K sentence pairs; 15.5M English words) shows statistically significant improvements over the classic model based on BLEU and human judgments. </br></br>

<a href='http://arxiv.org/pdf/1910.01765.pdf'>1910.01765</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp 0.0125баллов, №383</br>
<b>Robust Semi-Supervised Monocular Depth Estimation with Reprojected\n  Distances</b></br>
Authors: , Guizilini, Vitor, Li, Jie, Ambrus, Rares, Pillai, Sudeep, Gaidon, Adrien</br>
  Dense depth estimation from a single image is a key problem in computer vision, with exciting applications in a multitude of robotic tasks. Initially viewed as a direct <font color="#be00be">regression</font> problem, requiring annotated labels as supervision at training time, in the past few years a substantial amount of work has been done in self-supervised depth training based on strong geometric cues, both from stereo cameras and more recently from monocular video sequences. In this paper we investigate how these two approaches (supervised &amp; self-supervised) can be effectively combined, so that a depth model can learn to encode true scale from sparse supervision while achieving high fidelity local accuracy by leveraging geometric cues. To this end, we propose a novel supervised loss term that complements the widely used photometric loss, and show how it can be used to train robust semi-supervised monocular depth estimation models. Furthermore, we evaluate how much supervision is actually necessary to train accurate scale-aware monocular depth models, showing that with our proposed framework, very sparse <font color="blue">LiDAR</font> information, with as few as 4 beams (less than 100 valid depth values per image), is enough to achieve results <font color="#960096">competitive</font> with the current <font color="red">state-of-the-art</font>. </br></br>

<a href='http://arxiv.org/pdf/1911.01515.pdf'>1911.01515</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp 0.0116баллов, №384</br>
<b>Can the Elliptic Billiard Still Surprise Us?</b></br>
Authors: , Reznik, Dan, Garcia, Ronaldo, Koiller, Jair</br>
  Can any secrets still be shed by that much studied, uniquely integrable, Elliptic Billiard? Starting by examining the family of 3-periodic trajectories and the loci of their Triangular Centers, one obtains a beautiful and variegated gallery of curves: ellipses, quartics, sextics, circles, and even a stationary point. Secondly, one notices this family conserves an intriguing ratio: Inradius-to-Circumradius. In turn this implies three invariants as corollaries: (i) the sum of bounce angle cosines, (ii) the product of excentral cosines, and (iii) the ratio of excentral-to-orbit areas. Monge\'s Orthoptic Circle\'s close relation to 4-periodic Billiard trajectories is well-known. Its geometry provided clues with which to generalize 3-periodic invariants to trajectories of an arbitrary number of edges. This was quite unexpected. Indeed, the Elliptic Billiard did surprise us! </br></br>

<a href='http://arxiv.org/pdf/1911.09281.pdf'>1911.09281</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.0106баллов, №385</br>
<b>Event Detection in Noisy Streaming Data with Combination of\n  Corroborative and Probabilistic Sources</b></br>
Authors: , Suprem, Abhijit, Pu, Calton</br>
  Global physical event detection has traditionally relied on dense coverage of physical sensors around the world; while this is an expensive undertaking, there have not been alternatives until recently. The ubiquity of social networks and human sensors in the field provides a tremendous amount of real-time, live data about true physical events from around the world. However, while such human sensor data have been exploited for retrospective large-scale event detection, such as hurricanes or earthquakes, they has been limited to no success in exploiting this rich resource for general physical event detection.   Prior implementation approaches have suffered from the concept drift phenomenon, where <font color="#009600">real-world</font> data exhibits constant, unknown, unbounded changes in its data distribution, making static machine learning models ineffective in the long term. We propose and implement an end-to-end collaborative drift adaptive system that integrates corroborative and probabilistic sources to deliver real-time predictions. Furthermore, out system is adaptive to concept drift and performs automated continuous learning to maintain high performance. We demonstrate our approach in a real-time demo available online for landslide disaster detection, with extensibility to other real-world physical events such as flooding, wildfires, hurricanes, and earthquakes. </br></br>

<a href='http://arxiv.org/pdf/1911.09145.pdf'>1911.09145</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.0099баллов, №386</br>
<b>DPM: A deep learning PDE augmentation method (with application to\n  large-eddy simulation)</b></br>
Authors: , Freund, Jonathan B., MacArt, Jonathan F., Sirignano, Justin</br>
  Machine learning for scientific applications faces the challenge of limited data. We propose a framework that leverages a priori known physics to reduce overfitting when training on relatively small datasets. A deep neural network is embedded in a partial differential equation (PDE) that expresses the known physics and learns to describe the corresponding unknown or unrepresented physics from the data. Crafted as such, the neural network can also provide corrections for erroneously represented physics, such as discretization errors associated with the PDE\'s numerical solution. Once trained, the deep learning PDE model (DPM) can make out-of-sample predictions for new physical parameters, geometries, and boundary conditions.   Our approach optimizes over the functional form of the PDE. Estimating the embedded neural network requires optimizing over the entire PDE, which itself is a function of the neural network. Adjoint partial differential equations are used to efficiently calculate the high-dimensional gradient of the objective function with respect to the neural network parameters. A stochastic adjoint method (SAM), similar in spirit to stochastic gradient descent, further accelerates training.   The approach is demonstrated and evaluated for turbulence predictions using large-eddy simulation (LES), a filtered version of the Navier--Stokes equation containing unclosed sub-filter-scale terms. The DPM <font color="#00be00">outperform</font>s the widely-used constant-coefficient and dynamic Smagorinsky models, even for filter sizes so large that these established models become qualitatively incorrect. It also significantly outperforms a priori trained models, which do not account for the full PDE. A relaxation of the discrete enforcement of the divergence-free constraint is also considered, instead allowing the DPM to approximately enforce incompressibility physics. </br></br>

<a href='http://arxiv.org/pdf/1911.07098.pdf'>1911.07098</a> &nbsp&nbsp (cs:SD) &nbsp&nbsp 0.0092баллов, №387</br>
<b>VOICe: A Sound Event Detection Dataset For Generalizable Domain\n  Adaptation</b></br>
Authors: , Gharib, Shayan, Drossos, Konstantinos, Fagerlund, Eemi, Virtanen, Tuomas</br>
  The performance of sound event detection methods can significantly degrade when they are used in unseen conditions (e.g. recording devices, ambient noise). Domain adaptation is a promising way to tackle this problem. In this paper, we present VOICe, the first dataset for the development and evaluation of domain adaptation methods for sound event detection. VOICe consists of mixtures with three different sound events (&quot;baby crying&quot;, &quot;glass breaking&quot;, and &quot;gunshot&quot;), which are over-imposed over three different categories of acoustic scenes: vehicle, outdoors, and indoors. Moreover, the mixtures are also offered without any background noise. VOICe is freely available online (<font color="#006400">http</font>s://doi.org/10.5281/zenodo.3514950). In addition, using an adversarial-based training method, we evaluate the performance of a domain adaptation method on VOICe. </br></br>

<a href='http://arxiv.org/pdf/1911.07759.pdf'>1911.07759</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp 0.0088баллов, №388</br>
<b>A gamified simulator and physical platform for self-driving algorithm\n  training and validation</b></br>
Authors: , Siegel, Joshua E., Pappas, Georgios, Politopoulos, Konstantinos, Sun, Yongbin</br>
  We identify the need for a gamified self-driving simulator where game mechanics encourage high-quality data capture, and design and apply such a simulator to collecting lane-following training data. The resulting synthetic data enables a Convolutional Neural Network (CNN) to drive an in-game vehicle. We simultaneously develop a physical test platform based on a radio-controlled vehicle and the Robotic Operating System (ROS) and successfully transfer the simulation-trained model to the physical domain without modification. The cross-platform simulator facilitates unsupervised crowdsourcing, helping to collect diverse data emulating complex, dynamic environment data, infrequent events, and edge cases. The physical platform provides a low-cost solution for validating simulation-trained models or enabling rapid transfer learning, thereby improving the safety and resilience of self-driving algorithms. </br></br>

<a href='http://arxiv.org/pdf/1911.08113.pdf'>1911.08113</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 0.0083баллов, №389</br>
<b>Hunting for Troll Comments in News Community Forums</b></br>
Authors: , Mihaylov, Todor, Nakov, Preslav</br>
  There are different definitions of what a troll is. Certainly, a troll can be somebody who teases people to make them angry, or somebody who offends people, or somebody who wants to dominate any single discussion, or somebody who tries to manipulate people\'s opinion (sometimes for money), etc. The last definition is the one that dominates the public discourse in Bulgaria and Eastern Europe, and this is our focus in this paper. In our work, we examine two types of opinion manipulation trolls: paid trolls that have been revealed from leaked reputation management contracts and mentioned trolls that have been called such by several different people. We show that these definitions are sensible: we build two classifiers that can distinguish a post by such a paid troll from one by a non-troll with 81-82% accuracy; the same classifier achieves 81-82% accuracy on so called mentioned troll vs. non-troll posts. </br></br>

<a href='http://arxiv.org/pdf/1911.09304.pdf'>1911.09304</a> &nbsp&nbsp (cs:CL, cs:AI) &nbsp&nbsp 0.0059баллов, №390</br>
<b>Automatic Text-based Personality Recognition on Monologues and\n  Multiparty Dialogues Using Attentive Networks and Contextual Embeddings</b></br>
Authors: , Jiang, Hang, Zhang, Xianzhe, Choi, Jinho D.</br>
  Previous works related to automatic personality recognition focus on using traditional classification models with linguistic features. However, attentive neural networks with contextual embeddings, which have achieved huge success in text classification, are rarely explored for this task. In this project, we have two major contributions. First, we create the first dialogue-based personality dataset, FriendsPersona, by annotating 5 personality traits of speakers from Friends TV Show through crowdsourcing. Second, we present a novel approach to automatic personality recognition using pre-trained contextual embeddings (BERT and RoBERTa) and attentive neural networks. Our models largely improve the state-of-art results on the monologue Essays dataset by 2.49%, and establish a solid benchmark on our FriendsPersona. By comparing results in two datasets, we demonstrate the challenges of modeling personality in multi-party dialogue. </br></br>

<a href='http://arxiv.org/pdf/1911.08706.pdf'>1911.08706</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp 0.0058баллов, №391</br>
<b>Controlling Neural Machine Translation Formality with Synthetic\n  Supervision</b></br>
Authors: , Niu, Xing, Carpuat, Marine</br>
  This work aims to produce translations that convey source language content at a formality level that is appropriate for a particular audience. Framing this problem as a neural sequence-to-sequence task ideally requires training triplets consisting of a bilingual sentence pair labeled with target language formality. However, in practice, available training examples are limited to English sentence pairs of different <font color="#be00be">style</font>s, and bilingual parallel sentences of unknown formality. We introduce a novel training scheme for multi-task models that automatically generates synthetic training triplets by inferring the missing element on the fly, thus enabling end-to-end training. Comprehensive automatic and human assessments show that our best model <font color="#00be00">outperform</font>s existing models by producing translations that better match desired formality levels while preserving the source meaning. </br></br>

<a href='http://arxiv.org/pdf/1911.07989.pdf'>1911.07989</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp 0.0036баллов, №392</br>
<b>WITCHcraft: Efficient PGD attacks with random step size</b></br>
Authors: , Chiang, Ping-Yeh, Geiping, Jonas, Goldblum, Micah, Goldstein, Tom, Ni, Renkun, Reich, Steven, Shafahi, Ali</br>
  <font color="red">State-of-the-art</font> <font color="blue">adversarial att</font>acks on neural networks use expensive iterative methods and numerous random restarts from different initial points. Iterative FGSM-based methods without restarts trade off performance for computational efficiency because they do not adequately explore the image space and are highly sensitive to the choice of step size. We propose a variant of Projected Gradient Descent (PGD) that uses a random step size to improve performance without resorting to expensive random restarts. Our method, Wide Iterative Stochastic crafting (WITCHcraft), achieves results superior to the classical PGD attack on the CIFAR-10 and MNIST data sets but without additional computational cost. This simple modification of PGD makes crafting attacks more <font color="#be00be">economic</font>al, which is important in situations like adversarial training where attacks need to be crafted in real time. </br></br>

<a href='http://arxiv.org/pdf/1911.07896.pdf'>1911.07896</a> &nbsp&nbsp (cs:RO, cs:CV, cs:ML) &nbsp&nbsp 0.0024баллов, №393</br>
<b>A Deep Learning Approach for Robust Corridor Following</b></br>
Authors: , Dorbala, Vishnu Sashank, Hafez, A. H. Abdul, Jawahar, C. V.</br>
  For an autonomous corridor following task where the environment is continuously changing, several forms of environmental noise prevent an automated feature extraction procedure from performing reliably. Moreover, in cases where pre-defined features are absent from the captured data, a well defined control signal for performing the servoing task fails to get produced. In order to overcome these drawbacks, we present in this work, using a convolutional neural network (CNN) to directly estimate the required control signal from an image, encompassing feature extraction and control law computation into one single end-to-end framework. In particular, we study the task of autonomous corridor following using a CNN and present clear advantages in cases where a traditional method used for performing the same task fails to give a reliable outcome. We evaluate the performance of our method on this task on a Wheelchair Platform developed at our institute for this purpose. </br></br>

<a href='http://arxiv.org/pdf/1911.07910.pdf'>1911.07910</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp 0.0014баллов, №394</br>
<b>Comments on the Du-Kakade-Wang-Yang Lower Bounds</b></br>
Authors: , Van Roy, Benjamin, Dong, Shi</br>
  Du, Kakade, Wang, and Yang recently established intriguing lower bounds on sample complexity, which suggest that <font color="#00be00">reinforcement learning</font> with a misspecified representation is intractable. Another line of work, which centers around a statistic called the eluder dimension, establishes tractability of problems similar to those considered in the Du-Kakade-Wang-Yang paper. We compare these results and reconcile <font color="#be00be">interpret</font>ations. </br></br>

<a href='http://arxiv.org/pdf/1911.07509.pdf'>1911.07509</a> &nbsp&nbsp (cs:CV, cs:ML, cs:NE) &nbsp&nbsp 0.0002баллов, №395</br>
<b>AI-based Pilgrim Detection using Convolutional Neural Networks</b></br>
Authors: , Jabra, Marwa Ben, Ammar, Adel, Koubaa, Anis, Cheikhrouhou, Omar, Hamam, Habib</br>
  Pilgrimage represents the most important Islamic religious gathering in the world where millions of pilgrims visit the holy places of Makkah and Madinah to perform their rituals. The safety and security of pilgrims is the highest priority for the authorities. In Makkah, 5000 cameras are spread around the holy for monitoring pilgrims, but it is almost impossible to track all events by humans considering the huge number of images collected every second. To address this issue, we propose to use artificial intelligence technique based on deep learning and convolution neural networks to detect and identify Pilgrims and their features. For this purpose, we built a comprehensive dataset for the detection of pilgrims and their genders. Then, we develop two convolutional neural networks based on YOLOv3 and Faster-RCNN for the detection of Pilgrims. Experiments results show that Faster RCNN with Inception v2 feature extractor provides the best mean average precision over all classes of 51%. </br></br>

<a href='http://arxiv.org/pdf/1910.09357.pdf'>1910.09357</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0046баллов, №396</br>
<b>Task-Based Learning</b></br>
Authors: , Chen, Di, Zhu, Yada, Cui, Xiaodong, Gomes, Carla P.</br>
  This paper talks about task-based learning. </br></br>

<a href='http://arxiv.org/pdf/1911.08074.pdf'>1911.08074</a> &nbsp&nbsp (cs:ML, cs:NE) &nbsp&nbsp -0.0048баллов, №397</br>
<b>Thick-Net: Parallel Network Structure for Sequential Modeling</b></br>
Authors: , Li, Yu-Xuan, Liu, Jin-Yuan, Li, Liang, Guan, Xiang</br>
  Recurrent neural networks have been widely used in sequence learning tasks. In previous studies, the performance of the model has always been improved by either wider or deeper structures. However, the former becomes more prone to overfitting, while the latter is difficult to optimize. In this paper, we propose a simple new model named Thick-Net, by expanding the network from another dimension: thickness. Multiple parallel values are obtained via more sets of parameters in each hidden state, and the maximum value is selected as the final output among parallel intermediate outputs. Notably, Thick-Net can efficiently avoid overfitting, and is easier to optimize than the vanilla structures due to the large dropout affiliated with it. Our model is evaluated on four sequential tasks including adding problem, permuted sequential MNIST, text classification and language modeling. The results of these tasks demonstrate that our model can not only improve accuracy with faster convergence but also facilitate a better generalization ability. </br></br>

<a href='http://arxiv.org/pdf/1911.08112.pdf'>1911.08112</a> &nbsp&nbsp (cs:CL, cs:AI, cs:ML) &nbsp&nbsp -0.0066баллов, №398</br>
<b>Extended Answer and Uncertainty Aware Neural Question Generation</b></br>
Authors: , Zeng, Hongwei, Zhi, Zhuo, Liu, Jun, Wei, Bifan</br>
  In this paper, we study automatic question generation, the task of creating questions from corresponding text passages where some certain spans of the text can serve as the answers. We propose an Extended Answer-aware Network (EAN) which is trained with Word-based Coverage Mechanism (WCM) and decodes with Uncertainty-aware Beam Search (UBS). The EAN represents the target answer by its surrounding sentence with an encoder, and incorporates the information of the extended answer into paragraph representation with gated paragraph-to-answer attention to tackle the problem of the inadequate representation of the target answer. To reduce undesirable repetition, the WCM penalizes repeatedly attending to the same words at different time-steps in the training stage. The UBS aims to seek a better balance between the model confidence in copying words from an input text paragraph and the confidence in generating words from a vocabulary. We conduct experiments on the SQuAD dataset, and the results show our approach achieves significant performance improvement. </br></br>

<a href='http://arxiv.org/pdf/1911.09231.pdf'>1911.09231</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.0075баллов, №399</br>
<b>Camera-to-Robot Pose Estimation from a Single Image</b></br>
Authors: , Lee, Timothy E., Tremblay, Jonathan, To, Thang, Cheng, Jia, Mosier, Terry, Kroemer, Oliver, Fox, Dieter, Birchfield, Stan</br>
  We present an approach for estimating the pose of a camera with respect to a robot from a single image. Our method uses a deep neural network to process an RGB image from the camera to detect 2D keypoints on the robot. The network is trained entirely on simulated data using domain randomization. Perspective-$n$-point (P$n$P) is then used to recover the camera extrinsics, assuming that the joint configuration of the robot manipulator is known. Unlike classic hand-eye calibration systems, our method does not require an off-line calibration step but rather is capable of computing the camera extrinsics from a single frame, thus opening the possibility of on-line calibration. We show experimental results for three different camera sensors, demonstrating that our approach is able to achieve accuracy with a single frame that is better than that of classic off-line hand-eye calibration using multiple frames. With additional frames, accuracy improves even further. Code, datasets, and pretrained models for three widely-used robot manipulators will be made available. </br></br>

<a href='http://arxiv.org/pdf/1911.07629.pdf'>1911.07629</a> &nbsp&nbsp (cs:CL, cs:AI, cs:ML, stat:ML) &nbsp&nbsp -0.0097баллов, №400</br>
<b>Selection-based Question Answering of an MOOC</b></br>
Authors: , Sahay, Atul, Gholkar, Smita, Arya, Kavi</br>
  e-Yantra Robotics Competition (eYRC) is a unique Robotics Competition hosted by IIT Bombay that is actually an Embedded Systems and Robotics MOOC. Registrations have been growing exponentially in each year from 4500 in 2012 to over 34000 in 2019. In this 5-month long competition students learn complex skills under severe time pressure and have access to a discussion forum to post doubts about the learning material. Responding to questions in real-time is a challenge for project staff. Here, we illustrate the advantage of Deep Learning for real-time question answering in the eYRC discussion forum. We illustrate the advantage of Transformer based contextual embedding mechanisms such as Bidirectional Encoder Representation From Transformer (BERT) over word embedding mechanisms such as Word2Vec. We propose a weighted similarity metric as a measure of matching and find it more reliable than Content-Content or Title-Title similarities alone. The automation of replying to questions has brought the turn around response time(TART) down from a minimum of 21 mins to a minimum of 0.3 secs. </br></br>

<a href='http://arxiv.org/pdf/1911.09602.pdf'>1911.09602</a> &nbsp&nbsp (cs:CL, cs:ML, cs:SD) &nbsp&nbsp -0.0121баллов, №401</br>
<b>Learning Hierarchical Discrete Linguistic Units from Visually-Grounded\n  Speech</b></br>
Authors: , Harwath, David, Hsu, Wei-Ning, Glass, James</br>
  In this paper, we present a method for learning discrete linguistic units by incorporating vector quantization layers into neural models of visually grounded speech. We show that our method is capable of capturing both word-level and sub-word units, depending on how it is configured. What differentiates this paper from prior work on speech unit learning is the choice of training objective. Rather than using a reconstruction-based loss, we use a discriminative, multimodal grounding objective which forces the learned units to be useful for semantic image retrieval. We evaluate the sub-word units on the ZeroSpeech 2019 challenge, achieving a 27.3\\% reduction in ABX error rate over the top-performing submission, while keeping the bitrate approximately the same. We also present experiments demonstrating the noise robustness of these units. Finally, we show that a model with multiple quantizers can simultaneously learn phone-like detectors at a lower layer and word-like detectors at a higher layer. We show that these detectors are highly accurate, discovering 279 words with an F1 score of greater than 0.5. </br></br>

<a href='http://arxiv.org/pdf/1911.09241.pdf'>1911.09241</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.0122баллов, №402</br>
<b>Assessing the Benchmarking Capacity of Machine Reading Comprehension\n  Datasets</b></br>
Authors: , Sugawara, Saku, Stenetorp, Pontus, Inui, Kentaro, Aizawa, Akiko</br>
  Existing analysis work in machine reading comprehension (MRC) is largely concerned with evaluating the capabilities of systems. However, the capabilities of datasets are not assessed for benchmarking language understanding precisely. We propose a semi-automated, ablation-based methodology for this challenge; By checking whether questions can be solved even after removing features associated with a skill requisite for language understanding, we evaluate to what degree the questions do not require the skill. Experiments on 10 datasets (e.g., CoQA, SQuAD v2.0, and RACE) with a strong baseline model show that, for example, the relative scores of a baseline model provided with content words only and with shuffled sentence words in the context are on average 89.2% and 78.5% of the original score, respectively. These results suggest that most of the questions already answered correctly by the model do not necessarily require grammatical and complex reasoning. For precise benchmarking, MRC datasets will need to take extra care in their design to ensure that questions can correctly evaluate the intended skills. </br></br>

<a href='http://arxiv.org/pdf/1911.08826.pdf'>1911.08826</a> &nbsp&nbsp (cs:ML, cs:AI) &nbsp&nbsp -0.0144баллов, №403</br>
<b>Hierarchical Average Reward Policy Gradient Algorithms</b></br>
Authors: , Dharmavaram, Akshay, Riemer, Matthew, Bhatnagar, Shalabh</br>
  Option-critic learning is a general-purpose <font color="#00be00">reinforcement learning</font> (RL) framework that aims to address the issue of long term credit assignment by leveraging temporal abstractions. However, when dealing with extended timescales, discounting future rewards can lead to incorrect credit assignments. In this work, we address this issue by extending the <font color="#00be00">hierarchical</font> option-critic policy gradient <font color="blue">theor</font>em for the average reward criterion. Our proposed framework aims to maximize the long-term reward obtained in the steady-state of the Markov chain defined by the agent\'s policy. Furthermore, we use an ordinary differential equation based approach for our convergence analysis and prove that the parameters of the intra-option policies, termination functions, and value functions, converge to their corresponding optimal values, with probability one. Finally, we illustrate the <font color="#960096">competitive</font> advantage of learning options, in the average reward setting, on a grid-world environment with <font color="#00be00">sparse reward</font>s. </br></br>

<a href='http://arxiv.org/pdf/1911.08335.pdf'>1911.08335</a> &nbsp&nbsp (cs:ML, cs:SD) &nbsp&nbsp -0.0148баллов, №404</br>
<b>Generative Audio Synthesis with a Parametric Model</b></br>
Authors: , Subramani, Krishna, D\'Hooge, Alexandre, Rao, Preeti</br>
  Use a parametric representation of audio to train a generative model in the interest of obtaining more flexible control over the generated sound. </br></br>

<a href='http://arxiv.org/pdf/1911.07168.pdf'>1911.07168</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.0151баллов, №405</br>
<b>Optimal Control of a Differentially Flat 2D Spring-Loaded Inverted\n  Pendulum Model</b></br>
Authors: , Chen, Hua, Wensing, Patrick M., Zhang, Wei</br>
  This paper considers the optimal control problem of an extended spring-loaded inverted pendulum (SLIP) model with two additional actuators for active leg length and hip torque modulation. These additional features arise naturally in practice, allowing for consideration of swing leg kinematics during flight and active control over stance dynamics. On the other hand, nonlinearity and the hybrid nature of the overall SLIP dynamics introduce challenges in the analysis and control of the model. In this paper, we first show that the stance dynamics of the considered SLIP model are differentially flat, which has a strong implication regarding controllability of the stance dynamics. Leveraging this powerful property, a tractable optimal control strategy is developed. This strategy enables online solution while also treating the hybrid nature of the SLIP dynamics. Together with the optimal control strategy, the extended SLIP model grants active disturbance rejection capability at any point during the gait. Performance of the proposed control strategy is demonstrated via numerical tests and shows significant advantage over existing methods. </br></br>

<a href='http://arxiv.org/pdf/1911.07030.pdf'>1911.07030</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.0157баллов, №406</br>
<b>Contribution au Niveau de l\'Approche Indirecte \\`a Base de Transfert\n  dans la Traduction Automatique</b></br>
Authors: , Bessou, Sadik</br>
  In this thesis, we address several important issues concerning the morphological analysis of Arabic language applied to textual data and machine translation. First, we provided an overview on machine translation, its history and its development, then we exposed human translation techniques for eventual inspiration in machine translation, and we exposed linguistic approaches and particularly indirect transfer approaches. Finally, we presented our contributions to the resolution of morphosyntactic problems in computer linguistics as multilingual information retrieval and machine translation. As a first contribution, we developed a morphological analyzer for Arabic, and we have exploited it in the bilingual information retrieval such as a computer application of multilingual documentary. Results validation showed a statistically significant performance. In a second contribution, we proposed a list of morphosyntactic transfer rules from English to Arabic for translation in three phases: analysis, transfer, generation. We focused on the transfer phase without semantic distortion for an abstraction of English in a sufficient subset of Arabic. </br></br>

<a href='http://arxiv.org/pdf/1911.06317.pdf'>1911.06317</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0167баллов, №407</br>
<b>Gradientless Descent: High-Dimensional Zeroth-Order Optimization</b></br>
Authors: , Golovin, Daniel, Karro, John, Kochanski, Greg, Lee, Chansoo, Song, Xingyou, Zhang, Qiuyi</br>
  Zeroth-order optimization is the process of minimizing an objective $f(x)$, given oracle access to evaluations at adaptively chosen inputs $x$. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and present a novel analysis that shows convergence within an $\\epsilon$-ball of the optimum in $O(kQ\\log(n)\\log(R/\\epsilon))$ evaluations, for any monotone transform of a smooth and strongly convex objective with latent dimension $k &lt; n$, where the input dimension is $n$, $R$ is the diameter of the input space and $Q$ is the condition number. Our rates are the first of its kind to be both 1) poly-logarithmically dependent on dimensionality and 2) invariant under monotone transformations. We further leverage our geometric perspective to show that our analysis is optimal. Both monotone invariance and its ability to utilize a low latent dimensionality are key to the empirical success of our algorithms, as demonstrated on BBOB and <font color="#006400">MuJoCo</font> benchmarks. </br></br>

<a href='http://arxiv.org/pdf/1911.01054.pdf'>1911.01054</a> &nbsp&nbsp (cs:CV, cs:ML, cs:RO) &nbsp&nbsp -0.0209баллов, №408</br>
<b>SoildNet: Soiling Degradation Detection in Autonomous Driving</b></br>
Authors: , Das, Arindam</br>
  In the field of autonomous driving, camera sensors are extremely prone to soiling because they are located outside of the car and interact with environmental sources of soiling such as rain drops, snow, dust, sand, mud and so on. This can lead to either partial or complete vision degradation. Hence detecting such decay in vision is very important for safety and overall to preserve the functionality of the &quot;autonomous&quot; components in autonomous driving. The contribution of this work involves: 1) Designing a Deep Convolutional Neural Network (DCNN) based baseline network, 2) Exploiting several network remodelling techniques such as employing static and dynamic group convolution, channel reordering to compress the baseline architecture and make it suitable for low power embedded systems with nearly 1 TOPS, 3) Comparing various result metrics of all interim networks dedicated for soiling degradation detection at tile level of size 64 x 64 on input resolution 1280 x 768. The compressed network, is called SoildNet (Sand, snOw, raIn/dIrt, oiL, Dust/muD) that uses only 9.72% trainable parameters of the base network and reduces the model size by more than 7 times with no loss in accuracy </br></br>

<a href='http://arxiv.org/pdf/1911.08717.pdf'>1911.08717</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0221баллов, №409</br>
<b>Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine\n  Translation</b></br>
Authors: , Guo, Junliang, Tan, Xu, Xu, Linli, Qin, Tao, Chen, Enhong, Liu, Tie-Yan</br>
  Non-autoregressive translation (NAT) models remove the dependence on previous target tokens and generate all target tokens in parallel, resulting in significant inference speedup but at the cost of inferior translation accuracy compared to autoregressive translation (AT) models. Considering that AT models have higher accuracy and are easier to train than NAT models, and both of them share the same model configurations, a natural idea to improve the accuracy of NAT models is to transfer a well-trained AT model to an NAT model through fine-tuning. However, since AT and NAT models differ greatly in training strategy, straightforward fine-tuning does not work well. In this work, we introduce <font color="#006400">curriculum learning</font> into fine-tuning for NAT. Specifically, we design a curriculum in the fine-tuning process to progressively switch the training from autoregressive generation to non-autoregressive generation. Experiments on four benchmark translation datasets show that the proposed method achieves good improvement (more than $1$ BLEU score) over previous NAT baselines in terms of translation accuracy, and greatly speed up (more than $10$ times) the inference process over AT baselines. </br></br>

<a href='http://arxiv.org/pdf/1911.06978.pdf'>1911.06978</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.0231баллов, №410</br>
<b>Grounding Human-to-Vehicle Advice for Self-driving Vehicles</b></br>
Authors: , Kim, Jinkyu, Misu, Teruhisa, Chen, Yi-Ting, Tawari, Ashish, Canny, John</br>
  Recent success suggests that deep neural control networks are likely to be a key component of self-driving vehicles. These networks are trained on large datasets to imitate human actions, but they lack semantic understanding of image contents. This makes them brittle and potentially unsafe in situations that do not match training data. Here, we propose to address this issue by augmenting training data with natural language advice from a human. Advice includes guidance about what to do and where to attend. We present the first step toward advice giving, where we train an end-to-end vehicle controller that accepts advice. The controller adapts the way it attends to the scene (visual attention) and the control (steering and speed). Attention mechanisms tie controller behavior to salient objects in the advice. We evaluate our model on a novel advisable driving dataset with manually annotated human-to-vehicle advice called Honda Research Institute-Advice Dataset (HAD). We show that taking advice improves the performance of the end-to-end network, while the network cues on a variety of visual features that are provided by advice. The dataset is available at <font color="#006400">http</font>s://usa.honda-ri.com/HAD. </br></br>

<a href='http://arxiv.org/pdf/1911.09257.pdf'>1911.09257</a> &nbsp&nbsp (cs:NE, cs:CV, cs:ML) &nbsp&nbsp -0.0256баллов, №411</br>
<b>DeepLABNet: End-to-end Learning of Deep Radial Basis Networks with Fully\n  Learnable Basis Functions</b></br>
Authors: , Hryniowski, Andrew, Wong, Alexander</br>
  From fully connected neural networks to convolutional neural networks, the learned parameters within a neural network have been primarily relegated to the linear parameters (e.g., convolutional filters). The non-linear functions (e.g., activation functions) have largely remained, with few exceptions in recent years, parameter-less, static throughout training, and seen limited variation in design. Largely ignored by the deep learning community, radial basis function (RBF) networks provide an interesting mechanism for learning more complex non-linear activation functions in addition to the linear parameters in a network. However, the interest in RBF networks has waned over time due to the difficulty of integrating RBFs into more complex deep neural network architectures in a tractable and stable manner. In this work, we present a novel approach that enables end-to-end learning of deep RBF networks with fully learnable activation basis functions in an automatic and tractable manner. We demonstrate that our approach for enabling the use of learnable activation basis functions in deep neural networks, which we will refer to as DeepLABNet, is an effective tool for automated activation function learning within complex network architectures. </br></br>

<a href='http://arxiv.org/pdf/1911.08031.pdf'>1911.08031</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0271баллов, №412</br>
<b>The Design and Implementation of a Scalable DL Benchmarking Platform</b></br>
Authors: , Li, Cheng, Dakkak, Abdul, Xiong, Jinjun, Hwu, Wen-mei</br>
  The current Deep Learning (DL) landscape is fast-paced and is rife with non-uniform models, hardware/software (HW/SW) stacks, but lacks a DL benchmarking platform to facilitate evaluation and comparison of DL innovations, be it models, frameworks, libraries, or hardware. Due to the lack of a benchmarking platform, the current practice of evaluating the benefits of proposed DL innovations is both arduous and error-prone - stifling the adoption of the innovations.   In this work, we first identify $10$ design features which are desirable within a DL benchmarking platform. These features include: performing the evaluation in a consistent, reproducible, and scalable manner, being framework and hardware agnostic, supporting <font color="#009600">real-world</font> benchmarking workloads, providing in-depth model execution inspection across the HW/SW stack levels, etc. We then propose MLModelScope, a DL benchmarking platform design that realizes the $10$ objectives. MLModelScope proposes a specification to define DL model evaluations and techniques to provision the evaluation workflow using the user-specified HW/SW stack. MLModelScope defines abstractions for frameworks and supports board range of DL models and evaluation scenarios. We implement MLModelScope as an open-source project with support for all major frameworks and hardware architectures. Through MLModelScope\'s evaluation and automated analysis workflows, we performed case-study analyses of $37$ models across $4$ systems and show how model, hardware, and framework selection affects model accuracy and performance under different benchmarking scenarios. We further demonstrated how MLModelScope\'s tracing capability gives a holistic view of model execution and helps pinpoint bottlenecks. </br></br>

<a href='http://arxiv.org/pdf/1911.06897.pdf'>1911.06897</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.0295баллов, №413</br>
<b>Flexoskeleton printing for versatile insect-inspired robots</b></br>
Authors: , Jiang, Mingsong, Zhou, Ziyi, Gravish, Nicholas G.</br>
  One of the many secrets to the success and prevalence of insects is their versatile, robust, and complex exoskeleton morphology. A fundamental challenge in insect-inspired robotics has been the fabrication of robotic exoskeletons that can match the complexity of exoskeleton structural mechanics. Hybrid robots composed of rigid and soft elements have previously required access to expensive multi-material 3D printers, multi-step casting and machining processes, or limited material choice when using consumer-grade fabrication methods. Here we introduce a new design and fabrication process to rapidly construct flexible exoskeleton-inspired robots called flexoskeleton printing. We modify a consumer-grade fused deposition material (FDM) 3D printer to deposit filament directly onto a heated thermoplastic base layer which provides extremely strong bond strength between the deposited material and the inextensible, flexible base layer. This process significantly improves the fatigue resistance of printed components and enables a new class of insect-inspired robot morphologies. We demonstrate these capabilities through design and testing of a wide library of canonical flexoskeleton elements; ultimately leading to the integration of elements into a flexoskeleton walking legged robot. </br></br>

<a href='http://arxiv.org/pdf/1911.07960.pdf'>1911.07960</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.0337баллов, №414</br>
<b>The {\\alpha}{\\mu} Search Algorithm for the Game of Bridge</b></br>
Authors: , Cazenave, Tristan, Ventos, V&#xe9;ronique</br>
  {\\alpha}{\\mu} is an anytime heuristic search algorithm for incomplete information games that assumes perfect information for the opponents. {\\alpha}{\\mu} addresses the strategy fusion and non-locality problems encountered by Perfect Information Monte Carlo sampling. In this paper {\\alpha}{\\mu} is applied to the game of Bridge. </br></br>

<a href='http://arxiv.org/pdf/1911.08518.pdf'>1911.08518</a> &nbsp&nbsp (cs:NE) &nbsp&nbsp -0.0351баллов, №415</br>
<b>Supported-BinaryNet: Bitcell Array-based Weight Supports for Dynamic\n  Accuracy-Latency Trade-offs in SRAM-based Binarized Neural Network</b></br>
Authors: , Nasrin, Shamma, Ramakrishna, Srikanth, Tulabandhula, Theja, Trivedi, Amit Ranjan</br>
  In this work, we introduce bitcell array-based support parameters to improve the prediction accuracy of SRAM-based binarized neural network (SRAM-BNN). Our approach enhances the training weight space of SRAM-BNN while requiring minimal overheads to a typical design. More flexibility of the weight space leads to higher prediction accuracy in our design. We adapt row digital-to-analog (DAC) converter, and computing flow in SRAM-BNN for bitcell array-based weight supports. Using the discussed interventions, our scheme also allows a dynamic trade-off of accuracy against latency to address dynamic latency constraints in typical real-time applications. We specifically discuss results on two training cases: (i) learning of support parameters on a pre-trained BNN and (ii) simultaneous learning of supports and weight binarization. In the former case, our approach reduces classification error in MNIST by 35.71% (error rate decreases from 1.4% to 0.91%). In the latter case, the error is reduced by 27.65% (error rate decreases from 1.4% to 1.13%). To reduce the power overheads, we propose a dynamic drop out a part of the support parameters. Our architecture can drop out 52% of the bitcell array-based support parameters without losing accuracy. We also characterize our design under varying degrees of process variability in the transistors. </br></br>

<a href='http://arxiv.org/pdf/1911.09565.pdf'>1911.09565</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.0361баллов, №416</br>
<b>A Continuous Teleoperation Subspace with Empirical and Algorithmic\n  Mapping Algorithms for Non-Anthropomorphic Hands</b></br>
Authors: , Meeker, Cassie, Haas-Heger, Maximilian, Ciocarlie, Matei</br>
  Teleoperation is a valuable tool for robotic manipulators in highly unstructured environments. However, finding an intuitive mapping between a human hand and a non-anthropomorphic robot hand can be difficult, due to the hands\' dissimilar kinematics. In this paper, we seek to create a mapping between the human hand and a fully actuated, non-anthropomorphic robot hand that is intuitive enough to enable effective real-time teleoperation, even for novice users. To accomplish this, we propose a low-dimensional teleoperation subspace which can be used as an intermediary for mapping between hand pose spaces. We present two different methods to define the teleoperation subspace: an empirical definition, which requires a person to define hand motions in an intuitive, hand-specific way, and an algorithmic definition, which is kinematically independent, and uses objects to define the subspace. We use each of these definitions to create a teleoperation mapping for different hands. We validate both the empirical and algorithmic mappings with teleoperation experiments controlled by novices and performed on two kinematically distinct hands. The experiments show that the proposed subspace is relevant to teleoperation, intuitive enough to enable control by novices, and can generalize to non-anthropomorphic hands with different kinematic configurations. </br></br>

<a href='http://arxiv.org/pdf/1911.09359.pdf'>1911.09359</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.0380баллов, №417</br>
<b>Multi-Scale RCNN Model for Financial Time-series Classification</b></br>
Authors: , Guang, Liu, Xiaojie, Wang, Ruifan, Li</br>
  <font color="#be00be">Financ</font>ial time-series classification (FTC) is extremely valuable for investment management. In past decades, it draws a lot of attention from a wide extent of research areas, especially Artificial Intelligence (AI). Existing researches majorly focused on exploring the effects of the Multi-Scale (MS) property or the Temporal Dependency (TD) within financial time-series. Unfortunately, most previous researches fail to combine these two properties effectively and often fall short of accuracy and profitability. To effectively combine and utilize both properties of financial time-series, we propose a Multi-Scale Temporal Dependent Recurrent Convolutional Neural Network (MSTD-RCNN) for FTC. In the proposed method, the MS features are simultaneously extracted by convolutional units to precisely describe the state of the financial <font color="#be00be">market</font>. Moreover, the TD and complementary across different scales are captured through a Recurrent Neural Network. The proposed method is evaluated on three financial time-series datasets which source from the <font color="#be00be">Chinese</font> stock market. Extensive experimental results indicate that our model achieves the <font color="red">state-of-the-art</font> performance in trend classification and simulated trading, compared with classical and advanced baseline models. </br></br>

<a href='http://arxiv.org/pdf/1911.07817.pdf'>1911.07817</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.0387баллов, №418</br>
<b>Skin Lesion Classification Using Deep Neural Network</b></br>
Authors: , Guissous, Alla Eddine</br>
  This paper reports the methods and techniques we have developed for classify dermoscopic images (task 1) of the ISIC 2019 challenge dataset for skin lesion classification, our approach aims to use ensemble deep neural network with some powerful techniques to deal with unbalance data sets as its the main problem for this challenge in a move to increase the performance of CNNs model. </br></br>

<a href='http://arxiv.org/pdf/1911.07938.pdf'>1911.07938</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.0392баллов, №419</br>
<b>Towards Good Practices for Multi-Person Pose Estimation</b></br>
Authors: , Yu, Dongdong, Su, Kai, Wang, Changhu</br>
  Multi-Person Pose Estimation is an interesting yet challenging task in computer vision. In this paper, we conduct a series of refinements with the MSPN and PoseFix Networks, and empirically evaluate their impact on the final model performance through ablation studies. By taking all the refinements, we achieve 78.7 on the COCO test-dev dataset and 76.3 on the COCO test-challenge dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.09581.pdf'>1911.09581</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.0402баллов, №420</br>
<b>Feedback Motion Planning for Long-Range Autonomous Underwater Vehicles</b></br>
Authors: , Orioke, Opeyemi S., Alam, Tauhidul, Quinn, Joseph, Kaur, Ramneek, Alsabban, Wesam H., Bobadilla, Leonardo, Smith, Ryan N.</br>
  Ocean ecosystems have spatiotemporal variability and dynamic complexity that require a long-term deployment of an autonomous underwater vehicle for data collection. A new long-range autonomous underwater vehicle called Tethys is adapted to study different oceanic phenomena. Additionally, an ocean environment has external forces and moments along with changing water currents which are generally not considered in a vehicle kinematic model. In this scenario, it is not enough to generate a simple trajectory from an initial location to a goal location in an uncertain ocean as the vehicle can deviate from its intended trajectory. As such, we propose to compute a feedback plan that adapts the vehicle trajectory in the presence of any modeled or unmodeled uncertainties. In this work, we present a feedback motion planning method for the Tethys vehicle by combining a predictive ocean model and its kinematic modeling. Given a goal location, the Tethys kinematic model, and the water flow pattern, our method computes a feedback plan for the vehicle in a dynamic ocean environment that reduces its energy consumption. The computed feedback plan provides the optimal action for the Tethys vehicle to take from any location of the environment to reach the goal location considering its orientation. Our results based on actual ocean model prediction data demonstrate the applicability of our method. </br></br>

<a href='http://arxiv.org/pdf/1911.08772.pdf'>1911.08772</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0417баллов, №421</br>
<b>Understanding Top-k Sparsification in Distributed Deep Learning</b></br>
Authors: , Shi, Shaohuai, Chu, Xiaowen, Cheung, Ka Chun, See, Simon</br>
  Distributed stochastic gradient descent (SGD) algorithms are widely deployed in training large-scale deep learning models, while the communication overhead among workers becomes the new system bottleneck. Recently proposed gradient sparsification techniques, especially Top-$k$ sparsification with error compensation (TopK-SGD), can significantly reduce the communication traffic without an obvious impact on the model accuracy. Some <font color="blue">theor</font>etical studies have been carried out to analyze the convergence property of TopK-SGD. However, existing studies do not dive into the details of Top-$k$ operator in gradient sparsification and use relaxed bounds (e.g., exact bound of Random-$k$) for analysis; hence the derived results cannot well describe the real convergence performance of TopK-SGD. To this end, we first study the gradient distributions of TopK-SGD during the training process through extensive experiments. We then theoretically derive a tighter bound for the Top-$k$ operator. Finally, we exploit the property of gradient distribution to propose an approximate top-$k$ selection algorithm, which is computing-efficient for GPUs, to improve the scaling efficiency of TopK-SGD by significantly reducing the computing overhead. Codes are available at: \\url{<font color="#006400">http</font>s://<font color="red">github</font>.com/hclhkbu/<font color="blue">Gaussi</font>anK-SGD}. </br></br>

<a href='http://arxiv.org/pdf/1911.09345.pdf'>1911.09345</a> &nbsp&nbsp (cs:CV, cs:CL, cs:ML) &nbsp&nbsp -0.0448баллов, №422</br>
<b>Empirical Autopsy of Deep Video Captioning Frameworks</b></br>
Authors: , Aafaq, Nayyer, Akhtar, Naveed, Liu, Wei, Mian, Ajmal</br>
  Contemporary deep learning based video captioning follows encoder-decoder framework. In encoder, visual features are extracted with 2D/3D Convolutional Neural Networks (CNNs) and a transformed version of those features is passed to the decoder. The decoder uses word embeddings and a language model to map visual features to natural language captions. Due to its composite nature, the encoder-decoder pipeline provides the freedom of multiple choices for each of its components, e.g the choices of CNNs models, feature transformations, word embeddings, and language models etc. Component selection can have drastic effects on the overall video captioning performance. However, current literature is void of any systematic investigation in this regard. This article fills this gap by providing the first thorough empirical analysis of the role that each major component plays in a contemporary video captioning pipeline. We perform extensive experiments by varying the constituent components of the video captioning framework, and quantify the performance gains that are possible by mere component selection. We use the popular MSVD dataset as the test-bed, and demonstrate that substantial performance gains are possible by careful selection of the constituent components without major changes to the pipeline itself. These results are expected to provide guiding principles for future research in the fast growing direction of video captioning. </br></br>

<a href='http://arxiv.org/pdf/1911.06902.pdf'>1911.06902</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.0457баллов, №423</br>
<b>Label-similarity Curriculum Learning</b></br>
Authors: , Dogan, Urun, Deshmukh, Aniket Anand, Machura, Marcin, Igel, Christian</br>
  <font color="#006400">Curriculum learning</font> can improve neural network training by guiding the optimization to desirable optima. We propose a novel curriculum learning approach for image classification that adapts the loss function by changing the label representation. The idea is to use a probability distribution over classes as target label, where the class probabilities reflect the similarity to the true class. Gradually, this label representation is shifted towards the standard one-hot-encoding. That is, in the beginning minor mistakes are corrected less than large mistakes, resembling a teaching process in which broad concepts are explained first before subtle differences are taught.   The class similarity can be based on prior knowledge. For the special case of the labels being natural words, we propose a generic way to automatically compute the similarities. The natural words are embedded into Euclidean space using a standard word embedding. The probability of each class is then a function of the cosine similarity between the vector representations of the class and the true label.   The proposed label-similarity curriculum learning (LCL) approach was empirically evaluated on several popular deep learning architectures for image classification task applied to three datasets, ImageNet, CIFAR100, and AWA2. In all scenarios, LCL was able to improve the classification accuracy on the test data compared to standard training. </br></br>

<a href='http://arxiv.org/pdf/1911.08872.pdf'>1911.08872</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.0487баллов, №424</br>
<b>Towards Inconsistency Measurement in Business Rule Bases</b></br>
Authors: , Corea, Carl, Thimm, Matthias</br>
  We investigate the application of inconsistency measures to the problem of analysing business rule bases. Due to some intricacies of the domain of business rule bases, a straightforward application is not feasible. We therefore develop some new rationality postulates for this setting as well as adapt and modify existing inconsistency measures. We further adapt the notion of inconsistency values (or culpability measures) for this setting and give a comprehensive feasibility study. </br></br>

<a href='http://arxiv.org/pdf/1911.08587.pdf'>1911.08587</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0495баллов, №425</br>
<b>Solving machine learning optimization problems using quantum computers</b></br>
Authors: , Dasari, Venkat R., Im, Mee Seong, Beshaj, Lubjana</br>
  Classical optimization algorithms in machine learning often take a long time to compute when applied to a multi-dimensional problem and require a huge amount of CPU and GPU resource. Quantum parallelism has a potential to speed up machine learning algorithms. We describe a generic mathematical model to leverage quantum parallelism to speed-up machine learning algorithms. We also apply quantum machine learning and quantum parallelism applied to a $3$-dimensional image that vary with time. </br></br>

<a href='http://arxiv.org/pdf/1911.09488.pdf'>1911.09488</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.0502баллов, №426</br>
<b>Online Fair Division: A Survey</b></br>
Authors: , Aleksandrov, Martin, Walsh, Toby</br>
  We survey a burgeoning and promising new research area that considers the online nature of many practical fair division problems. We identify wide variety of such online fair division problems, as well as discuss new mechanisms and normative properties that apply to this online setting. The online nature of such fair division problems provides both opportunities and challenges such as the possibility to develop new online mechanisms as well as the difficulty of dealing with an uncertain future. </br></br>

<a href='http://arxiv.org/pdf/1911.08361.pdf'>1911.08361</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0531баллов, №427</br>
<b>Predicting overweight and obesity in later life from childhood data: A\n  review of predictive modeling approaches</b></br>
Authors: , Rautiainen, Ilkka, &#xc4;yr&#xe4;m&#xf6;, Sami</br>
  Background: Overweight and obesity are an increasing phenomenon worldwide. Predicting future overweight or obesity early in the childhood reliably could enable a successful intervention by experts. While a lot of research has been done using explanatory modeling methods, capability of machine learning, and predictive modeling, in particular, remain mainly unexplored. In predictive modeling models are validated with previously unseen examples, giving a more accurate estimate of their performance and generalization ability in real-life scenarios.   Objective: To find and review existing overweight or obesity research from the perspective of employing childhood data and predictive modeling methods.   Methods: The initial phase included bibliographic searches using relevant search terms in PubMed, IEEE database and <font color="#00be00">Google</font> Scholar. The second phase consisted of iteratively searching references of potential studies and recent research that cite the potential studies.   Results: Eight research articles and three review articles were identified as relevant for this review.   Conclusions: Prediction models with high performance either have a relatively short time period to predict or/and are based on late childhood data. Logistic <font color="#be00be">regression</font> is currently the most often used method in forming the prediction models. In addition to child\'s own weight and height information, maternal weight status or body mass index was often used as predictors in the models. </br></br>

<a href='http://arxiv.org/pdf/1911.07749.pdf'>1911.07749</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp -0.0539баллов, №428</br>
<b>On the computation of counterfactual explanations -- A survey</b></br>
Authors: , Artelt, Andr&#xe9;, Hammer, Barbara</br>
  Due to the increasing use of machine learning in practice it becomes more and more important to be able to explain the prediction and behavior of machine learning models. An instance of explanations are counterfactual explanations which provide an intuitive and useful explanations of machine learning models. In this survey we review model-specific methods for efficiently computing counterfactual explanations of many different machine learning models and propose methods for models that have not been considered in literature so far. </br></br>

<a href='http://arxiv.org/pdf/1911.08532.pdf'>1911.08532</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0570баллов, №429</br>
<b>Robust Learning of Discrete Distributions from Batches</b></br>
Authors: , Jain, Ayush, Orlitsky, Alon</br>
  Let $d$ be the lowest $L_1$ distance to which a $k$-symbol distribution $p$ can be estimated from $m$ batches of $n$ samples each, when up to $\\beta m$ batches may be adversarial. For $\\beta&lt;1/2$, Qiao and Valiant (2017) showed that $d=\\Omega(\\beta/\\sqrt{n})$ and requires $m=\\Omega(k/\\beta^2)$ batches. For $\\beta&lt;1/900$, they provided a $d$ and $m$ order-optimal algorithm that runs in time exponential in $k$.   For $\\beta&lt;0.5$, we propose an algorithm with comparably optimal $d$ and $m$, but run-time polynomial in $k$ and all other parameters. </br></br>

<a href='http://arxiv.org/pdf/1911.07357.pdf'>1911.07357</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.0572баллов, №430</br>
<b>Random Restrictions of High-Dimensional Distributions and Uniformity\n  Testing with Subcube Conditioning</b></br>
Authors: , Canonne, Cl&#xe9;ment L., Chen, Xi, Kamath, Gautam, Levi, Amit, Waingarten, Erik</br>
  We give a nearly-optimal algorithm for testing uniformity of distributions supported on $\\{-1,1\\}^n$, which makes $\\tilde O (\\sqrt{n}/\\varepsilon^2)$ queries to a subcube conditional sampling oracle (Bhattacharyya and Chakraborty (2018)). The key technical component is a natural notion of random restriction for distributions on $\\{-1,1\\}^n$, and a quantitative analysis of how such a restriction affects the mean vector of the distribution. Along the way, we consider the problem of mean testing with independent samples and provide a nearly-optimal algorithm. </br></br>

<a href='http://arxiv.org/pdf/1911.09219.pdf'>1911.09219</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.0574баллов, №431</br>
<b>Integrating Automated Play in Level Co-Creation</b></br>
Authors: , Hoyt, Andrew, Guzdial, Matthew, Kumar, Yalini, Smith, Gillian, Riedl, Mark O.</br>
  In level co-creation an AI and human work together to create a video game level. One open challenge in level co-creation is how to empower human users to ensure particular qualities of the final level, such as challenge. There has been significant prior research into automated pathing and automated playtesting for video game levels, but not in how to incorporate these into tools. In this demonstration we present an improvement of the Morai Maker mixed-initiative level editor for Super Mario Bros. that includes automated pathing and challenge approximation features. </br></br>

<a href='http://arxiv.org/pdf/1911.07171.pdf'>1911.07171</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.0583баллов, №432</br>
<b>2nd Place Solution in Google AI Open Images Object Detection Track 2019</b></br>
Authors: , Guo, Ruoyu, Cui, Cheng, Du, Yuning, Meng, Xianglong, Wang, Xiaodi, Liu, Jingwei, Zhu, Jianfeng, Feng, Yuan, Han, Shumin</br>
  We present an <font color="#be00be">object detection</font> framework based on PaddlePaddle. We put all the strategies together (multi-scale training, FPN, Cascade, Dcnv2, Non-local, libra loss) based on ResNet200-vd backbone. Our model score on public leaderboard comes to 0.6269 with single scale test. We proposed a new voting method called top-k voting-nms, based on the SoftNMS detection results. The voting method helps us merge all the models\' results more easily and achieve 2nd place in the <font color="#00be00">Google</font> AI Open Images Object Detection Track 2019. </br></br>

<a href='http://arxiv.org/pdf/1911.07926.pdf'>1911.07926</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.0584баллов, №433</br>
<b>VITON-GAN: Virtual Try-on Image Generator Trained with Adversarial Loss</b></br>
Authors: , Honda, Shion</br>
  Generating a virtual try-on image from in-shop clothing images and a model person\'s snapshot is a challenging task because the human body and clothes have high flexibility in their shapes. In this paper, we develop a Virtual Try-on Generative Adversarial Network (VITON-GAN), that generates virtual try-on images using images of in-shop clothing and a model person. This method enhances the quality of the generated image when occlusion is present in a model person\'s image (e.g., arms crossed in front of the clothes) by adding an adversarial mechanism in the training pipeline. </br></br>

<a href='http://arxiv.org/pdf/1911.08942.pdf'>1911.08942</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0585баллов, №434</br>
<b>Adaptive Wind Driven Optimization Trained Artificial Neural Networks</b></br>
Authors: , Bayraktar, Zikri</br>
  This paper presents the application of a newly developed nature-inspired metaheuristic optimization method, namely the Adaptive Wind Driven Optimization (AWDO), to the training of feedforward artificial neural networks (NN) and presents a discussion into the future research of AWDO implementation in Deep Learning (DL). Application example of digit classification with MNIST dataset reveals interesting behavior of the derivative-free AWDO method compared to steepest descent method where results and future work on the implementation of AWDO in deep neural networks are discussed. </br></br>

<a href='http://arxiv.org/pdf/1911.08635.pdf'>1911.08635</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0596баллов, №435</br>
<b>Logic-inspired Deep Neural Networks</b></br>
Authors: , Le, Minh</br>
  Deep neural networks have achieved impressive performance and become de-facto standard in many tasks. However, phenomena such as adversarial examples and fooling examples hint that the generalization they make is flawed. We argue that the problem roots in their distributed and connected nature and propose remedies inspired by propositional logic. Our experiments show that the proposed models are more local and better at resisting fooling and adversarial examples. By means of an ablation analysis, we reveal insights into adversarial examples and suggest a new hypothesis on their origins. </br></br>

<a href='http://arxiv.org/pdf/1910.09694.pdf'>1910.09694</a> &nbsp&nbsp (cs:NE) &nbsp&nbsp -0.0597баллов, №436</br>
<b>A Domain-agnostic, Noise-resistant, Hardware-efficient Evolutionary\n  Variational Quantum Eigensolver</b></br>
Authors: , Rattew, Arthur G., Hu, Shaohan, Pistoia, Marco, Chen, Richard, Wood, Steve</br>
  Variational quantum algorithms have shown promise in numerous fields due to their versatility in solving problems of scientific and commercial interest. However, leading algorithms for Hamiltonian simulation, such as the Variational Quantum Eigensolver (VQE), use fixed preconstructed ansatzes, limiting their general applicability and accuracy. Thus, variational forms---the quantum circuits that implement ansatzes ---are either crafted heuristically or by encoding domain-specific knowledge. In this paper, we present an Evolutionary Variational Quantum Eigensolver (EVQE), a novel variational algorithm that uses evolutionary programming techniques to minimize the expectation value of a given Hamiltonian by dynamically generating and optimizing an ansatz. The algorithm is equally applicable to optimization problems in all domains, obtaining accurate energy evaluations with hardware-efficient ansatzes. In molecular simulations, the variational forms generated by EVQE are up to $18.6\\times$ shallower and use up to $12\\times$ fewer CX gates than those obtained by VQE with a unitary coupled cluster ansatz. EVQE demonstrates significant noise-resistance properties, obtaining results in noisy simulation with at least $3.6\\times$ less error than VQE using any tested ansatz configuration. We successfully evaluated EVQE on a real 5-qubit IBMQ quantum computer. The experimental results, which we obtained both via simulation and on real quantum hardware, demonstrate the effectiveness of EVQE for general-purpose optimization on the quantum computers of the present and near future. </br></br>

<a href='http://arxiv.org/pdf/1911.07532.pdf'>1911.07532</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp -0.0598баллов, №437</br>
<b>Graph Neural Ordinary Differential Equations</b></br>
Authors: , Poli, Michael, Massaroli, Stefano, Park, Junyoung, Yamashita, Atsushi, Asama, Hajime, Park, Jinkyoo</br>
  We extend the framework of graph neural networks (GNN) to continuous time. Graph neural ordinary differential equations (GDEs) are introduced as the counterpart to GNNs where the input--output relationship is determined by a continuum of GNN layers. The GDE framework is shown to be compatible with the majority of commonly used GNN models with minimal modification to the original formulations. We evaluate the effectiveness of GDEs on both static as well as dynamic datasets: results prove their general effectiveness even in cases where the data is not generated by continuous time processes. </br></br>

<a href='http://arxiv.org/pdf/1911.08556.pdf'>1911.08556</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0603баллов, №438</br>
<b>Towards Reducing Bias in Gender Classification</b></br>
Authors: , Teru, Komal K., Chakraborty, Aishik</br>
  Societal bias towards certain communities is a big problem that affects a lot of machine learning systems. This work aims at addressing the racial bias present in many modern gender recognition systems. We learn race invariant representations of human faces with an adversarially trained autoencoder model. We show that such representations help us achieve less biased performance in gender classification. We use variance in classification accuracy across different races as a surrogate for the racial bias of the model and achieve a drop of over 40% in variance with race invariant representations. </br></br>

<a href='http://arxiv.org/pdf/1911.09104.pdf'>1911.09104</a> &nbsp&nbsp (cs:ET) &nbsp&nbsp -0.0603баллов, №439</br>
<b>Reversible Computation in Wireless Communications</b></br>
Authors: , Siljak, Harun</br>
  This chapter presents the pioneering work in applying reversible computation paradigms to wireless communications. These applications range from developing reversible hardware architectures for underwater acoustic communications to novel distributed optimisation procedures in large radio-frequency antenna arrays based on reversing Petri nets. Throughout the chapter, we discuss the rationale for introducing reversible computation in the domain of wireless communications, exploring the inherently reversible properties of communication channels and systems formed by devices in a wireless network. </br></br>

<a href='http://arxiv.org/pdf/1911.08829.pdf'>1911.08829</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.0629баллов, №440</br>
<b>Casting a Wide Net: Robust Extraction of Potentially Idiomatic\n  Expressions</b></br>
Authors: , Haagsma, Hessel, Nissim, Malvina, Bos, Johan</br>
  Idiomatic expressions like `out of the woods\' and `up the ante\' present a range of difficulties for natural language processing applications. We present work on the annotation and extraction of what we term potentially idiomatic expressions (PIEs), a subclass of multiword expressions covering both literal and non-literal uses of idiomatic expressions. Existing corpora of PIEs are small and have limited coverage of different PIE types, which hampers research. To further progress on the extraction and disambiguation of potentially idiomatic expressions, larger corpora of PIEs are required. In addition, larger corpora are a potential source for valuable linguistic insights into idiomatic expressions and their variability. We propose automatic tools to facilitate the building of larger PIE corpora, by investigating the feasibility of using dictionary-based extraction of PIEs as a pre-extraction tool for English. We do this by assessing the reliability and coverage of idiom dictionaries, the annotation of a PIE corpus, and the automatic extraction of PIEs from a large corpus. Results show that combinations of dictionaries are a reliable source of idiomatic expressions, that PIEs can be annotated with a high reliability (0.74-0.91 Fleiss\' Kappa), and that parse-based PIE extraction yields highly accurate performance (88% F1-score). Combining complementary PIE extraction methods increases reliability further, to over 92% F1-score. Moreover, the extraction method presented here could be extended to other types of multiword expressions and to other languages, given that sufficient NLP tools are available. </br></br>

<a href='http://arxiv.org/pdf/1911.03347.pdf'>1911.03347</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0633баллов, №441</br>
<b>Macro F1 and Macro F1</b></br>
Authors: , Opitz, Juri, Burst, Sebastian</br>
  The \'macro F1\' metric is frequently used to evaluate binary, multi-class and multi-label classification problems. Yet, we find that there exist two different formulas to calculate this quantity. In this note, we show that only under rare circumstances, the two computations can be considered equivalent. More specifically, one formula well \'rewards\' classifiers which produce a skewed error type distribution. In fact, the difference in outcome of the two computations can be as high as 0.5. Finally, we show that the two computations may not only diverge in their scalar result but also lead to different classifier rankings. </br></br>

<a href='http://arxiv.org/pdf/1911.08439.pdf'>1911.08439</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.0646баллов, №442</br>
<b>Towards a computer-interpretable actionable formal model to encode data\n  governance rules</b></br>
Authors: , Zhao, Rui, Atkinson, Malcolm</br>
  With the needs of science and business, data sharing and re-use has become an intensive activity for various areas. In many cases, governance imposes rules concerning data use, but there is no existing computational technique to help data-users comply with such rules. We argue that intelligent systems can be used to improve the situation, by recording provenance records during processing, encoding the rules and performing reasoning. We present our initial work, designing formal models for data rules and flow rules and the reasoning system, as the first step towards helping data providers and data users sustain productive relationships. </br></br>

<a href='http://arxiv.org/pdf/1911.09587.pdf'>1911.09587</a> &nbsp&nbsp (cs:ML, cs:AI) &nbsp&nbsp -0.0674баллов, №443</br>
<b>Vouw: Geometric Pattern Mining using the MDL Principle</b></br>
Authors: , Faas, Micky, van Leeuwen, Matthijs</br>
  We introduce geometric pattern mining, the problem of finding recurring local structure in raster-based data. It differs from other pattern mining problems by encoding complex spatial relations between elements, resulting in arbitrarily shaped patterns. After we formalise this new type of pattern mining, we discuss an approach to selecting a set of patterns using the Minimum Description Length principle. We demonstrate the viability of our approach by introducing Vouw, a heuristic algorithm that finds good solutions to a specific class of geometric pattern mining problems. We empirically show that Vouw delivers high-quality results by using a synthetic benchmark. </br></br>

<a href='http://arxiv.org/pdf/1911.09301.pdf'>1911.09301</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.0675баллов, №444</br>
<b>Image Aesthetics Assessment using Multi Channel Convolutional Neural\n  Networks</b></br>
Authors: , Doshi, Nishi, Shikhenawis, Gitam, Mitra, Suman K</br>
  Image Aesthetics Assessment is one of the emerging domains in research. The domain deals with classification of images into categories depending on the basis of how pleasant they are for the users to watch. In this article, the focus is on categorizing the images in high quality and low quality image. Deep convolutional neural networks are used to classify the images. Instead of using just the raw image as input, different crops and saliency maps of the images are also used, as input to the proposed multi channel CNN architecture. The experiments reported on widely used AVA database show improvement in the aesthetic assessment performance over existing approaches. </br></br>

<a href='http://arxiv.org/pdf/1911.07995.pdf'>1911.07995</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.0677баллов, №445</br>
<b>CD2 : Combined Distances of Contrast Distributions for the Assessment of\n  Perceptual Quality of Image Processing</b></br>
Authors: , Xu, Sascha, Bauer, Jan, Axmann, Benjamin</br>
  The quality of visual input is very important for both human and machine perception. Consequently many processing techniques exist that deal with different distortions. Usually image processing is applied freely and lacks redundancy regarding safety. We propose a novel image comparison method called the Combined Distances of Contrast Distributions (CD2) to protect against errors that arise during processing. Based on the distribution of image contrasts a new reduced-reference image quality assessment (IQA) method is introduced. By combining various distance functions excellent performance on IQA benchmarks is achieved with only a small data and computation overhead. </br></br>

<a href='http://arxiv.org/pdf/1911.07983.pdf'>1911.07983</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.0682баллов, №446</br>
<b>Task-Based Hybrid Shared Control for Training Through Forceful\n  Interaction</b></br>
Authors: , Fitzsimons, Kathleen, Kalinowska, Aleksandra, Dewald, Julius P. A., Murphey, Todd</br>
  Despite the fact that robotic platforms can provide both consistent practice and objective assessments of users over the course of their training, there are relatively few instances where physical human robot interaction has been significantly more effective than unassisted practice or human-mediated training. This paper describes a hybrid shared control robot, which enhances task learning through kinesthetic feedback. The assistance assesses user actions using a task-specific evaluation criterion and selectively accepts or rejects them at each time instant. Through two human subject studies (total n=68), we show that this hybrid approach of switching between full transparency and full rejection of user inputs leads to increased skill acquisition and short-term retention compared to unassisted practice. Moreover, we show that the shared control paradigm exhibits features previously shown to promote successful training. It avoids user passivity by only rejecting user actions and allowing failure at the task. It improves performance during assistance, providing meaningful task-specific feedback. It is sensitive to initial skill of the user and behaves as an `assist-as-needed\' control scheme---adapting its engagement in real time based on the performance and needs of the user. Unlike other successful algorithms, it does not require explicit modulation of the level of impedance or error amplification during training and it is permissive to a range of strategies because of its evaluation criterion. We demonstrate that the proposed hybrid shared control paradigm with a task-based minimal intervention criterion significantly enhances task-specific training. </br></br>

<a href='http://arxiv.org/pdf/1911.07702.pdf'>1911.07702</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0690баллов, №447</br>
<b>An explanation method for Siamese neural networks</b></br>
Authors: , Utkin, Lev V., Kovalev, Maxim S., Kasimov, Ernest M.</br>
  A new method for explaining the Siamese neural network is proposed. It uses the following main ideas. First, the explained feature vector is compared with the prototype of the corresponding class computed at the embedding level (the Siamese neural network output). The important features at this level are determined as features which are close to the same features of the prototype. Second, an autoencoder is trained in a special way in order to take into account the embedding level of the Si-amese network, and its decoder part is used for reconstructing input data with the corresponding changes. Numerical experiments with the well-known dataset MNIST illustrate the propose method. </br></br>

<a href='http://arxiv.org/pdf/1911.08030.pdf'>1911.08030</a> &nbsp&nbsp (cs:ML, cs:NE) &nbsp&nbsp -0.0694баллов, №448</br>
<b>Driver Identification Based on Vehicle Telematics Data using\n  LSTM-Recurrent Neural Network</b></br>
Authors: , Girma, Abenezer, Yan, Xuyang, Homaifar, Abdollah</br>
  Despite advancements in vehicle security systems, over the last decade, auto-theft rates have increased, and cyber-security attacks on internet-connected and autonomous vehicles are becoming a new threat. In this paper, a deep learning model is proposed, which can identify drivers from their driving behaviors based on vehicle telematics data. The proposed Long-Short-Term-Memory (LSTM) model predicts the identity of the driver based on the individual\'s unique driving patterns learned from the vehicle telematics data. Given the telematics is time-series data, the problem is formulated as a time series prediction task to exploit the embedded sequential information. The performance of the proposed approach is evaluated on three naturalistic driving datasets, which gives high accuracy prediction results. The robustness of the model on noisy and <font color="#be00be">anomal</font>ous data that is usually caused by sensor defects or environmental factors is also investigated. Results show that the proposed model prediction accuracy remains satisfactory and <font color="#00be00">outperform</font>s the other approaches despite the extent of anomalies and noise-induced in the data. </br></br>

<a href='http://arxiv.org/pdf/1911.07304.pdf'>1911.07304</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0716баллов, №449</br>
<b>Asymptotics of Reinforcement Learning with Neural Networks</b></br>
Authors: , Sirignano, Justin, Spiliopoulos, Konstantinos</br>
  We prove that a single-layer neural network trained with the Q-learning algorithm converges in distribution to a random ordinary differential equation as the size of the model and the number of training steps become large. Analysis of the limit differential equation shows that it has a unique stationary solution which is the solution of the Bellman equation, thus giving the optimal control for the problem. In addition, we study the convergence of the limit differential equation to the stationary solution. As a by-product of our analysis, we obtain the limiting behavior of single-layer neural networks when trained on i.i.d. data with stochastic gradient descent under the widely-used Xavier initialization. </br></br>

<a href='http://arxiv.org/pdf/1911.09576.pdf'>1911.09576</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0717баллов, №450</br>
<b>Volume-preserving Neural Networks: A Solution to the Vanishing Gradient\n  Problem</b></br>
Authors: , MacDonald, Gordon, Godbout, Andrew, Gillcash, Bryn, Cairns, Stephanie</br>
  We propose a novel approach to addressing the vanishing (or exploding) gradient problem in deep neural networks. We construct a new architecture for deep neural networks where all layers (except the output layer) of the network are a combination of rotation, permutation, diagonal, and activation sublayers which are all volume preserving. This control on the volume forces the gradient (on average) to maintain equilibrium and not explode or vanish. Volume-preserving neural networks train reliably, quickly and accurately and the learning rate is consistent across layers in deep volume-preserving neural networks. To demonstrate this we apply our volume-preserving neural network model to two standard datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.09233.pdf'>1911.09233</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.0738баллов, №451</br>
<b>Contextual Reinforcement Learning of Visuo-tactile Multi-fingered\n  Grasping Policies</b></br>
Authors: , Kumar, Visak, Herman, Tucker, Fox, Dieter, Birchfield, Stan, Tremblay, Jonathan</br>
  Using simulation to train robot manipulation policies holds the promise of an almost unlimited amount of training data, generated safely out of harm\'s way. One of the key challenges of using simulation, to date, has been to bridge the reality gap, so that policies trained in simulation can be deployed in the <font color="#009600">real world</font>. We explore the reality gap in the context of learning a contextual policy for multi-fingered robotic grasping. We propose a Grasping Objects Approach for Tactile (GOAT) robotic hands, learning to overcome the reality gap problem. In our approach we use human hand motion demonstration to initialize and reduce the search space for learning. We contextualize our policy with the bounding cuboid dimensions of the object of interest, which allows the policy to work on a more flexible representation than directly using an image or <font color="#be00be">point cloud</font>. Leveraging fingertip touch sensors in the hand allows the policy to overcome the reduction in geometric information introduced by the coarse bounding box, as well as pose estimation uncertainty. We show our learned policy successfully runs on a real robot without any fine tuning, thus bridging the reality gap. </br></br>

<a href='http://arxiv.org/pdf/1911.06983.pdf'>1911.06983</a> &nbsp&nbsp (cs:ET) &nbsp&nbsp -0.0748баллов, №452</br>
<b>Capacitorless Model of a VO2 Oscillator</b></br>
Authors: , Belyaev, M. A., Velichko, A. A.</br>
  We implement a capacitorless model of a VO2 oscillator by introducing into the circuit of a field-effect transistor and a VO2 thermal sensor, which provide negative current feedback with a time delay. We compare the dynamics of current and voltage oscillations on a switch in a circuit with a capacitor and without a capacitor. The oscillation period in the capacitorless model is controlled in a narrow range by changing the distance between the switch and the sensor. The capacitorless model provides the possibility of significant miniaturization of the oscillator circuit, and it is important for the implementation of large arrays of oscillators in oscillatory neural networks to solve the problem of classification and pattern recognition. </br></br>

<a href='http://arxiv.org/pdf/1911.06932.pdf'>1911.06932</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.0751баллов, №453</br>
<b>3D Conditional Generative Adversarial Networks to enable large-scale\n  seismic image enhancement</b></br>
Authors: , Dutta, Praneet, Power, Bruce, Halpert, Adam, Ezequiel, Carlos, Subramanian, Aravind, Chatterjee, Chanchal, Hari, Sindhu, Prindle, Kenton, Vaddina, Vishal, Leach, Andrew, Domala, Raj, Bandura, Laura, Mascaro, Massimo</br>
  We propose GAN-based image enhancement models for frequency enhancement of 2D and 3D seismic images. Seismic imagery is used to understand and characterize the Earth\'s subsurface for energy exploration. Because these images often suffer from resolution limitations and noise contamination, our proposed method performs large-scale seismic volume frequency enhancement and denoising. The enhanced images reduce uncertainty and improve decisions about issues, such as optimal well placement, that often rely on low signal-to-noise ratio (SNR) seismic volumes. We explored the impact of adding lithology class information to the models, resulting in improved performance on PSNR and SSIM metrics over a baseline model with no conditional information. </br></br>

<a href='http://arxiv.org/pdf/1911.07875.pdf'>1911.07875</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0753баллов, №454</br>
<b>Attribute noise robust binary classification</b></br>
Authors: , Petety, Aditya, Tripathi, Sandhya, Hemachandra, N</br>
  We consider the problem of learning linear classifiers when both features and labels are binary. In addition, the features are noisy, i.e., they could be flipped with an unknown probability. In Sy-De attribute noise model, where all features could be noisy together with same probability, we show that $0$-$1$ loss ($l_{0-1}$) need not be robust but a popular surrogate, squared loss ($l_{sq}$) is. In Asy-In attribute noise model, we prove that $l_{0-1}$ is robust for any distribution over 2 dimensional feature space. However, due to computational intractability of $l_{0-1}$, we resort to $l_{sq}$ and observe that it need not be Asy-In noise robust. Our empirical results support Sy-De robustness of squared loss for low to moderate noise rates. </br></br>

<a href='http://arxiv.org/pdf/1911.08833.pdf'>1911.08833</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.0762баллов, №455</br>
<b>A Conditional Perspective for Iterated Belief Contraction</b></br>
Authors: , Sauerwald, Kai, Kern-Isberner, Gabriele, Beierle, Christoph</br>
  According to Boutillier, Darwiche, Pearl and others, principles for iterated revision can be characterised in terms of changing beliefs about conditionals. For iterated contraction a similar formulation is not known. This is especially because for iterated belief change the connection between revision and contraction via the Levi and Harper identity is not straightforward, and therefore, characterisation results do not transfer easily between iterated revision and contraction. In this article, we develop an axiomatisation of iterated contraction in terms of changing conditional beliefs. We prove that the new set of postulates conforms semantically to the class of operators like the ones given by Konieczny and Pino P\\\'erez for iterated contraction. </br></br>

<a href='http://arxiv.org/pdf/1911.06911.pdf'>1911.06911</a> &nbsp&nbsp (stat:ML) &nbsp&nbsp -0.0774баллов, №456</br>
<b>The quadratic Wasserstein metric for inverse data matching</b></br>
Authors: , Engquist, Bj\x7forn, Ren, Kui, Yang, Yunan</br>
  This work characterizes, analytically and numerically, two major effects of the quadratic Wasserstein ($W_2$) distance as the measure of data discrepancy in computational solutions of inverse problems. First, we show, in the infinite-dimensional setup, that the $W_2$ distance has a smoothing effect on the inversion process, making it robust against high-frequency noise in the data but leading to a reduced resolution for the reconstructed objects at a given noise level. Second, we demonstrate that for some finite-dimensional problems, the $W_2$ distance leads to optimization problems that have better convexity than the classical $L^2$ and $\\dot{\\mathcal{H}}^{-1}$ distances, making it a more preferred distance to use when solving such inverse matching problems. </br></br>

<a href='http://arxiv.org/pdf/1911.07675.pdf'>1911.07675</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0776баллов, №457</br>
<b>GraLSP: Graph Neural Networks with Local Structural Patterns</b></br>
Authors: , Jin, Yilun, Song, Guojie, Shi, Chuan</br>
  It is not until recently that graph neural networks (GNNs) are adopted to perform graph representation learning, among which, those based on the aggregation of features within the neighborhood of a node achieved great success. However, despite such achievements, GNNs illustrate defects in identifying some common structural patterns which, unfortunately, play significant roles in various network phenomena. In this paper, we propose GraLSP, a GNN framework which explicitly incorporates local structural patterns into the neighborhood aggregation through random anonymous walks. Specifically, we capture local graph structures via random anonymous walks, powerful and flexible tools that represent structural patterns. The walks are then fed into the feature aggregation, where we design various mechanisms to address the impact of structural features, including adaptive receptive radius, attention and amplification. In addition, we design objectives that capture similarities between structures and are optimized jointly with node proximity objectives. With the adequate leverage of structural patterns, our model is able to <font color="#00be00">outperform</font> <font color="#960096">competitive</font> counterparts in various prediction tasks in multiple datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.08327.pdf'>1911.08327</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0783баллов, №458</br>
<b>Eliminating artefacts in Polarimetric Images using Deep Learning</b></br>
Authors: , Paranjpye, Dhruv, Mahabal, Ashish, Ramaprakash, A. N., Panopoulou, Gina, Cleary, Kieran, Readhead, Anthony, Blinov, Dmitry, Tassis, Kostas</br>
  Polarization measurements done using Imaging Polarimeters such as the Robotic Polarimeter are very sensitive to the presence of artefacts in images. Artefacts can range from internal reflections in a telescope to satellite trails that could contaminate an area of interest in the image. With the advent of wide-field polarimetry surveys, it is imperative to develop methods that automatically flag artefacts in images. In this paper, we implement a Convolutional Neural Network to identify the most dominant artefacts in the images. We find that our model can successfully classify sources with 98\\% true positive and 97\\% true negative rates. Such models, combined with transfer learning, will give us a running start in artefact elimination for near-future surveys like WALOP. </br></br>

<a href='http://arxiv.org/pdf/1911.03404.pdf'>1911.03404</a> &nbsp&nbsp (cs:NE) &nbsp&nbsp -0.0792баллов, №459</br>
<b>An Analysis of an Integrated Mathematical Modeling -- Artificial Neural\n  Network Approach for the Problems with a Limited Learning Dataset</b></br>
Authors: , Buchaniec, Szymon, Gnatowski, Marek, Brus, Grzegorz</br>
  One of the most common and universal problems in science is to investigate a function. The prediction can be made by an Artificial Neural Network (ANN) or a mathematical model. Both approaches have their advantages and disadvantages. Mathematical models were sought as more trustworthy as their prediction is based on the laws of physics expressed in the form of mathematical equations. However, the majority of existing mathematical models include different empirical parameters, and both approaches inherit inevitable experimental errors. At the same time, the approximation of neural networks can reproduce the solution extremely well if fed with a sufficient amount of data. The difference is that an ANN requires big data to build its accurate approximation whereas a typical mathematical model needs just several data points to estimate an empirical constant. Therefore, the common problem that developer meet is the inaccuracy of mathematical models and artificial neural network. An another common challenge is the computational complexity of the mathematical models, or lack of data for a sufficient precision of the Artificial Neural Networks. In the presented paper those problems are addressed using the integration of a mathematical model with an artificial neural network. In the presented analysis, an ANN predicts just a part of the mathematical model and its weights and biases are adjusted based on the output of the mathematical model. The performance of Integrated Mathematical modeling - Artificial Neural Network (IMANN) is compared to a Dense Neural Network (DNN) with the use of the benchmarking functions. The obtained calculation results indicate that such an approach could lead to an increase of precision as well as limiting the data-set required for learning. </br></br>

<a href='http://arxiv.org/pdf/1911.07758.pdf'>1911.07758</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.0798баллов, №460</br>
<b>Inexact Primal-Dual Gradient Projection Methods for Nonlinear\n  Optimization on Convex Set</b></br>
Authors: , Zhang, Fan, Wang, Hao, Wang, Jiashan, Yang, Kai</br>
  In this paper, we propose a novel primal-dual inexact gradient projection method for nonlinear optimization problems with convex-set constraint. This method only needs inexact computation of the projections onto the convex set for each iteration, consequently reducing the computational cost for projections per iteration. This feature is attractive especially for solving problems where the projections are computationally not easy to calculate. Global convergence guarantee and O(1/k) ergodic convergence rate of the optimality residual are provided under loose assumptions. We apply our proposed strategy to l1-ball constrained problems. Numerical results exhibit that our inexact gradient projection methods for solving l1-ball constrained problems are more efficient than the exact methods. </br></br>

<a href='http://arxiv.org/pdf/1911.05332.pdf'>1911.05332</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0804баллов, №461</br>
<b>Finding Social Media Trolls: Dynamic Keyword Selection Methods for\n  Rapidly-Evolving Online Debates</b></br>
Authors: , Liu, Anqi, Srikanth, Maya, Adams-Cohen, Nicholas, Alvarez, R. Michael, Anandkumar, Anima</br>
  Online harassment is a significant social problem. Prevention of online harassment requires rapid detection of harassing, offensive, and negative social media posts. In this paper, we propose the use of word embedding models to identify offensive and harassing social media messages in two aspects: detecting fast-changing topics for more effective data collection and representing word semantics in different domains. We demonstrate with preliminary results that using the GloVe (Global Vectors for Word Representation) model facilitates the discovery of new and relevant keywords to use for data collection and trolling detection. Our paper concludes with a discussion of a research agenda to further develop and test word embedding models for identification of social media harassment and trolling. </br></br>

<a href='http://arxiv.org/pdf/1911.07384.pdf'>1911.07384</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.0807баллов, №462</br>
<b>Distributed Low Precision Training Without Mixed Precision</b></br>
Authors: , Cheng, Zehua, Wang, Weiyang, Pan, Yan, Lukasiewicz, Thomas</br>
  Low precision training is one of the most popular strategies for deploying the deep model on limited hardware resources. Fixed point implementation of DCNs has the potential to alleviate complexities and facilitate potential deployment on embedded hardware. However, most low precision training solution is based on a mixed precision strategy. In this paper, we have presented an ablation study on different low precision training strategy and propose a solution for IEEE FP-16 format throughout the training process. We tested the ResNet50 on 128 GPU cluster on ImageNet-full dataset. We have viewed that it is not essential to use FP32 format to train the deep models. We have viewed that communication cost reduction, model compression, and large-scale distributed training are three coupled problems. </br></br>

<a href='http://arxiv.org/pdf/1911.08286.pdf'>1911.08286</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.0811баллов, №463</br>
<b>Zoea -- Composable Inductive Programming Without Limits</b></br>
Authors: , McDaid, Edward, McDaid, Sarah</br>
  Automatic generation of software from some form of specification has been a long standing goal of computer science research. To date successful results have been reported for the production of relatively small programs. This paper presents Zoea which is a simple programming language that allows software to be generated from a specification format that closely resembles a set of automated functional tests. Zoea incorporates a number of advances that enable it to generate software that is large enough to have commercial value. Zoea also allows programs to be composed to form still larger programs. As a result Zoea can be used to produce software of any size and complexity. An overview of the core Zoea language is provided together with a high level description of the symbolic AI based Zoea compiler. </br></br>

<a href='http://arxiv.org/pdf/1911.04523.pdf'>1911.04523</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.0816баллов, №464</br>
<b>A Simple Differentiable Programming Language</b></br>
Authors: , Abadi, Martin, Plotkin, Gordon D.</br>
  Automatic differentiation plays a prominent role in scientific computing and in modern machine learning, often in the context of powerful programming systems. The relation of the various embodiments of automatic differentiation to the mathematical notion of derivative is not always entirely clear---discrepancies can arise, sometimes inadvertently. In order to study automatic differentiation in such programming contexts, we define a small but expressive programming language that includes a construct for reverse-mode differentiation. We give operational and denotational semantics for this language. The operational semantics employs popular implementation techniques, while the denotational semantics employs notions of differentiation familiar from real analysis. We establish that these semantics coincide. </br></br>

<a href='http://arxiv.org/pdf/1911.01067.pdf'>1911.01067</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0816баллов, №465</br>
<b>Network Revenue Management with Limited Switches: Known and Unknown\n  Demand Distributions</b></br>
Authors: , Simchi-Levi, David, Xu, Yunzong, Zhao, Jinglong</br>
  This work is motivated by a practical concern from our retail partner. While they respect the advantages of dynamic pricing, they must limit the number of price changes to be within some constant. We study the classical price-based network revenue management problem, where a retailer has finite initial inventory of multiple resources to sell over a finite time horizon. We consider both known and unknown distribution settings, and derive policies that have the best-possible asymptotic performance in both settings. Our results suggest an intrinsic difference between the expected revenue associated with how many switches are allowed, which further depends on the number of resources. Our results are also the first to show a separation between the regret bounds associated with different number of resources. </br></br>

<a href='http://arxiv.org/pdf/1911.09537.pdf'>1911.09537</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0819баллов, №466</br>
<b>Neural Network Memorization Dissection</b></br>
Authors: , Gu, Jindong, Tresp, Volker</br>
  Deep neural networks (DNNs) can easily fit a random labeling of the training data with zero training error. What is the difference between DNNs trained with random labels and the ones trained with true labels? Our paper answers this question with two contributions. First, we study the memorization properties of DNNs. Our empirical experiments shed light on how DNNs prioritize the learning of simple input patterns. In the second part, we propose to measure the similarity between what different DNNs have learned and memorized. With the proposed approach, we analyze and compare DNNs trained on data with true labels and random labels. The analysis shows that DNNs have \\textit{One way to Learn} and \\textit{N ways to Memorize}. We also use gradient information to gain an understanding of the analysis results. </br></br>

<a href='http://arxiv.org/pdf/1911.09356.pdf'>1911.09356</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.0821баллов, №467</br>
<b>Schemaless Queries over Document Tables with Dependencies</b></br>
Authors: , Canim, Mustafa, Cornelio, Cristina, Iyengar, Arun, Musa, Ryan, Muro, Mariano Rodrigez</br>
  Unstructured enterprise data such as reports, manuals and guidelines often contain tables. The traditional way of integrating data from these tables is through a two-step process of table detection/extraction and mapping the table layouts to an appropriate schema. This can be an expensive process. In this paper we show that by using semantic technologies (RDF/SPARQL and database dependencies) paired with a simple but powerful way to transform tables with non-relational layouts, it is possible to offer query answering services over these tables with minimal manual work or domain-specific mappings. Our method enables users to exploit data in tables embedded in documents with little effort, not only for simple retrieval queries, but also for structured queries that require joining multiple interrelated tables. </br></br>

<a href='http://arxiv.org/pdf/1911.01678.pdf'>1911.01678</a> &nbsp&nbsp (cs:CL, cs:AI) &nbsp&nbsp -0.0830баллов, №468</br>
<b>A Joint Model for Definition Extraction with Syntactic Connection and\n  Semantic Consistency</b></br>
Authors: , Veyseh, Amir Pouran Ben, Dernoncourt, Franck, Dou, Dejing, Nguyen, Thien Huu</br>
  Definition Extraction (DE) is one of the well-known topics in Information Extraction that aims to identify terms and their corresponding definitions in unstructured texts. This task can be formalized either as a sentence classification task (i.e., containing term-definition pairs or not) or a sequential labeling task (i.e., identifying the boundaries of the terms and definitions). The previous works for DE have only focused on one of the two approaches, failing to model the inter-dependencies between the two tasks. In this work, we propose a novel model for DE that simultaneously performs the two tasks in a single framework to benefit from their inter-dependencies. Our model features deep learning architectures to exploit the global structures of the input sentences as well as the semantic consistencies between the terms and the definitions, thereby improving the quality of the representation vectors for DE. Besides the joint inference between sentence classification and sequential labeling, the proposed model is fundamentally different from the prior work for DE in that the prior work has only employed the local structures of the input sentences (i.e., word-to-word relations), and not yet considered the semantic consistencies between terms and definitions. In order to implement these novel ideas, our model presents a multi-task learning framework that employs graph convolutional neural networks and predicts the dependency paths between the terms and the definitions. We also seek to enforce the consistency between the representations of the terms and definitions both globally (i.e., increasing semantic consistency between the representations of the entire sentences and the terms/definitions) and locally (i.e., promoting the similarity between the representations of the terms and the definitions). </br></br>

<a href='http://arxiv.org/pdf/1911.08065.pdf'>1911.08065</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0835баллов, №469</br>
<b>Adaptive Activation Network and Functional Regularization for Efficient\n  and Flexible Deep Multi-Task Learning</b></br>
Authors: , Liu, Yingru, Yang, Xuewen, Xie, Dongliang, Wang, Xin, Shen, Li, Huang, Haozhi, Balasubramanian, Niranjan</br>
  Multi-task learning (MTL) is a common paradigm that seeks to improve the generalization performance of task learning by training related tasks simultaneously. However, it is still a challenging problem to search the flexible and accurate architecture that can be shared among multiple tasks. In this paper, we propose a novel deep learning model called Task Adaptive Activation Network (TAAN) that can automatically learn the optimal network architecture for MTL. The main principle of TAAN is to derive flexible activation functions for different tasks from the data with other parameters of the network fully shared. We further propose two functional regularization methods that improve the MTL performance of TAAN. The improved performance of both TAAN and the regularization methods is demonstrated by comprehensive experiments. </br></br>

<a href='http://arxiv.org/pdf/1911.07318.pdf'>1911.07318</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.0843баллов, №470</br>
<b>Towards Efficient Anytime Computation and Execution of Decoupled\n  Robustness Envelopes for Temporal Plans</b></br>
Authors: , Cashmore, Michael, Cimatti, Alessandro, Magazzeni, Daniele, Micheli, Andrea, Zehtabi, Parisa</br>
  One of the major limitations for the employment of model-based planning and scheduling in practical applications is the need of costly re-planning when an incongruence between the observed reality and the formal model is encountered during execution. Robustness Envelopes characterize the set of possible contingencies that a plan is able to address without re-planning, but their exact computation is extremely expensive; furthermore, general robustness envelopes are not amenable for efficient execution. In this paper, we present a novel, anytime algorithm to approximate Robustness Envelopes, making them scalable and executable. This is proven by an experimental analysis showing the efficiency of the algorithm, and by a concrete case study where the execution of robustness envelopes significantly reduces the number of re-plannings. </br></br>

<a href='http://arxiv.org/pdf/1911.08606.pdf'>1911.08606</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.0846баллов, №471</br>
<b>CoopNet: Cooperative Convolutional Neural Network for Low-Power MCUs</b></br>
Authors: , Mocerino, Luca, Calimera, Andrea</br>
  Fixed-point quantization and binarization are two reduction methods adopted to deploy Convolutional Neural Networks (CNN) on end-nodes powered by low-power micro-controller units (MCUs). While most of the existing works use them as stand-alone optimizations, this work aims at demonstrating there is margin for a joint cooperation that leads to inferential engines with lower latency and higher accuracy. Called $CoopNet$, the proposed heterogeneous model is conceived, implemented and tested on off-the-shelf MCUs with small on-chip memory and few computational resources. Experimental results conducted on three different CNNs using as test-bench the low-power RISC core of the Cortex-M family by ARM validate the CoopNet proposal by showing substantial improvements w.r.t. designs where quantization and binarization are applied separately. </br></br>

<a href='http://arxiv.org/pdf/1911.06956.pdf'>1911.06956</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.0851баллов, №472</br>
<b>On Space-spectrum Uncertainty Analysis for Coded Aperture Systems</b></br>
Authors: , Saragadam, Vishwanath, Sankaranarayanan, Aswin</br>
  We introduce and analyze the concept of space-spectrum uncertainty for certain commonly-used designs for spectrally programmable cameras. Our key finding states that, it is impossible to simultaneously capture high-resolution spatial images while programming the spectrum at high resolution. This phenomenon arises due to a Fourier relationship between the aperture used for obtaining spectrum and its corresponding diffraction blur in the (spatial) image. We show that the product of spatial and spectral standard deviations is lower bounded by {\\lambda}/4{\\pi}{\ u_0} femto square-meters, where {\ u_0} is the density of groves in the diffraction grating and {\\lambda} is the wavelength of light. Experiments with a lab prototype for simultaneously measuring spectrum and image validate our findings and its implication for spectral filtering. </br></br>

<a href='http://arxiv.org/pdf/1911.07324.pdf'>1911.07324</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0860баллов, №473</br>
<b>Testing Properties of Multiple Distributions with Few Samples</b></br>
Authors: , Aliakbarpour, Maryam, Silwal, Sandeep</br>
  We propose a new setting for testing properties of distributions while receiving samples from several distributions, but few samples per distribution. Given samples from $s$ distributions, $p_1, p_2, \\ldots, p_s$, we design testers for the following problems: (1) Uniformity Testing: Testing whether all the $p_i$\'s are uniform or $\\epsilon$-far from being uniform in $\\ell_1$-distance (2) Identity Testing: Testing whether all the $p_i$\'s are equal to an explicitly given distribution $q$ or $\\epsilon$-far from $q$ in $\\ell_1$-distance, and (3) Closeness Testing: Testing whether all the $p_i$\'s are equal to a distribution $q$ which we have sample access to, or $\\epsilon$-far from $q$ in $\\ell_1$-distance. By assuming an additional natural condition about the source distributions, we provide sample optimal testers for all of these problems. </br></br>

<a href='http://arxiv.org/pdf/1911.06181.pdf'>1911.06181</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.0861баллов, №474</br>
<b>Adversarial Transformations for Semi-Supervised Learning</b></br>
Authors: , Suzuki, Teppei, Sato, Ikuro</br>
  We propose a Regularization framework based on Adversarial Transformations (RAT) for semi-supervised learning. RAT is designed to enhance robustness of the output distribution of class prediction for a given data against input perturbation. RAT is an extension of Virtual Adversarial Training (VAT) in such a way that RAT adversarialy transforms data along the underlying data distribution by a rich set of data transformation functions that leave class label invariant, whereas VAT simply produces adversarial additive noises. In addition, we verified that a technique of gradually increasing of perturbation region further improve the robustness. In experiments, we show that RAT significantly improves classification performance on CIFAR-10 and SVHN compared to existing regularization methods under standard semi-supervised image classification settings. </br></br>

<a href='http://arxiv.org/pdf/1911.06939.pdf'>1911.06939</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.0863баллов, №475</br>
<b>Unsupervised Representation Learning for Gaze Estimation</b></br>
Authors: , Yu, Yu, Odobez, Jean-Marc</br>
  Although automatic gaze estimation is very important to a large variety of application areas, it is difficult to train accurate and robust gaze models, in great part due to the difficulty in collecting large and diverse data (annotating 3D gaze is expensive and existing datasets use different setups). To address this issue, our main contribution in this paper is to propose an effective approach to learn a low dimensional gaze representation without gaze annotations, which to the best of our best knowledge, is the first work to do so. The main idea is to rely on a gaze redirection network and use the gaze representation difference of the input and target images (of the redirection network) as the redirection variable. A redirection loss in image domain allows the joint training of both the redirection network and the gaze representation network. In addition, we propose a warping field regularization which not only provides an explicit physical meaning to the gaze representations but also avoids redirection distortions. Promising results on <font color="#00be00">few-shot</font> gaze estimation (<font color="#960096">competitive</font> results can be achieved with as few as &lt;=100 calibration samples), cross-dataset gaze estimation, gaze network pretraining, and another task (head pose estimation) demonstrate the validity of our framework. </br></br>

<a href='http://arxiv.org/pdf/1911.07683.pdf'>1911.07683</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.0863баллов, №476</br>
<b>Radar Emitter Classification with Attribute-specific Recurrent Neural\n  Networks</b></br>
Authors: , Notaro, Paolo, Paschali, Magdalini, Hopke, Carsten, Wittmann, David, Navab, Nassir</br>
  Radar pulse streams exhibit increasingly complex temporal patterns and can no longer rely on a purely value-based analysis of the pulse attributes for the purpose of emitter classification. In this paper, we employ Recurrent Neural Networks (RNNs) to efficiently model and exploit the temporal dependencies present inside pulse streams. With the purpose of enhancing the network prediction capability, we introduce two novel techniques: a per-sequence normalization, able to mine the useful temporal patterns; and attribute-specific RNN processing, capable of processing the extracted information effectively. The new techniques are evaluated with an ablation study and the proposed solution is compared to previous Deep Learning (DL) approaches. Finally, a comparative study on the robustness of the same approaches is conducted and its results are presented. </br></br>

<a href='http://arxiv.org/pdf/1911.07704.pdf'>1911.07704</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.0863баллов, №477</br>
<b>Affine Self Convolution</b></br>
Authors: , Diaconu, Nichita, Worrall, Daniel E</br>
  Attention mechanisms, and most prominently self-attention, are a powerful building block for processing not only text but also images. These provide a parameter efficient method for aggregating inputs. We focus on self-attention in vision models, and we combine it with convolution, which as far as we know, are the first to do. What emerges is a convolution with data dependent filters. We call this an Affine Self Convolution. While this is applied differently at each spatial location, we show that it is translation equivariant. We also modify the Squeeze and Excitation variant of attention, extending both variants of attention to the roto-translation group. We evaluate these new models on CIFAR10 and CIFAR100 and show an improvement in the number of parameters, while reaching comparable or higher accuracy at test time against self-trained baselines. </br></br>

<a href='http://arxiv.org/pdf/1911.09431.pdf'>1911.09431</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0874баллов, №478</br>
<b>System Identification with Time-Aware Neural Sequence Models</b></br>
Authors: , Demeester, Thomas</br>
  Established recurrent neural networks are well-suited to solve a wide variety of prediction tasks involving discrete sequences. However, they do not perform as well in the task of dynamical system identification, when dealing with observations from continuous variables that are unevenly sampled in time, for example due to missing observations. We show how such neural sequence models can be adapted to deal with variable step sizes in a natural way. In particular, we introduce a time-aware and stationary extension of existing models (including the Gated Recurrent Unit) that allows them to deal with unevenly sampled system observations by adapting to the observation times, while facilitating higher-order temporal behavior. We discuss the properties and demonstrate the validity of the proposed approach, based on samples from two industrial input/output processes. </br></br>

<a href='http://arxiv.org/pdf/1911.08050.pdf'>1911.08050</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0884баллов, №479</br>
<b>Carpe Diem, Seize the Samples Uncertain &quot;At the Moment&quot; for Adaptive\n  Batch Selection</b></br>
Authors: , Song, Hwanjun, Kim, Minseok, Kim, Sundong, Lee, Jae-Gil</br>
  The performance of deep neural networks is significantly affected by how well mini-batches are constructed. In this paper, we propose a novel adaptive batch selection algorithm called Recency Bias that exploits the uncertain samples predicted inconsistently in recent iterations. The historical label predictions of each sample are used to evaluate its predictive uncertainty within a sliding window. By taking advantage of this design, Recency Bias not only accelerates the training step but also achieves a more accurate network. We demonstrate the superiority of Recency Bias by extensive evaluation on two independent tasks. Compared with existing batch selection methods, the results showed that Recency Bias reduced the test error by up to 20.5% in a fixed wall-clock training time. At the same time, it improved the training time by up to 59.3% to reach the same test error. </br></br>

<a href='http://arxiv.org/pdf/1911.07716.pdf'>1911.07716</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp -0.0897баллов, №480</br>
<b>The Effectiveness of Variational Autoencoders for Active Learning</b></br>
Authors: , Pourkamali-Anaraki, Farhad, Wakin, Michael B.</br>
  The high cost of acquiring labels is one of the main challenges in deploying supervised machine learning algorithms. Active learning is a promising approach to control the learning process and address the difficulties of data labeling by selecting labeled training examples from a large pool of unlabeled instances. In this paper, we propose a new data-driven approach to active learning by choosing a small set of labeled data points that are both informative and representative. To this end, we present an efficient geometric technique to select a diverse core-set in a low-dimensional latent space obtained by training a Variational Autoencoder (VAE). Our experiments demonstrate an improvement in accuracy over two related techniques and, more importantly, signify the representation power of generative modeling for developing new active learning methods in high-dimensional data settings. </br></br>

<a href='http://arxiv.org/pdf/1911.09606.pdf'>1911.09606</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.0899баллов, №481</br>
<b>An Introduction to Artificial Intelligence Applied to Multimedia</b></br>
Authors: , Lima, Guilherme, Costa, Rodrigo, Moreno, Marcio Ferreira</br>
  In this chapter, we give an introduction to symbolic artificial intelligence (AI) and discuss its relation and application to multimedia. We begin by defining what symbolic AI is, what distinguishes it from non-symbolic approaches, such as machine learning, and how it can used in the construction of advanced multimedia applications. We then introduce description logic (DL) and use it to discuss symbolic representation and reasoning. DL is the logical underpinning of OWL, the most successful family of ontology languages. After discussing DL, we present OWL and related Semantic Web technologies, such as RDF and SPARQL. We conclude the chapter by discussing a hybrid model for multimedia representation, called Hyperknowledge. Throughout the text, we make references to technologies and extensions specifically designed to solve the kinds of problems that arise in multimedia representation. </br></br>

<a href='http://arxiv.org/pdf/1911.07976.pdf'>1911.07976</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.0903баллов, №482</br>
<b>Estimating Entropy of Distributions in Constant Space</b></br>
Authors: , Acharya, Jayadev, Bhadane, Sourbh, Indyk, Piotr, Sun, Ziteng</br>
  We consider the task of estimating the entropy of $k$-ary distributions from samples in the streaming model, where space is limited. Our main contribution is an algorithm that requires $O\\left(\\frac{k \\log (1/\\varepsilon)^2}{\\varepsilon^3}\\right)$ samples and a constant $O(1)$ memory words of space and outputs a $\\pm\\varepsilon$ estimate of $H(p)$. Without space limitations, the sample complexity has been established as $S(k,\\varepsilon)=\\Theta\\left(\\frac k{\\varepsilon\\log k}+\\frac{\\log^2 k}{\\varepsilon^2}\\right)$, which is sub-linear in the domain size $k$, and the current algorithms that achieve optimal sample complexity also require nearly-linear space in $k$.   Our algorithm partitions $[0,1]$ into intervals and estimates the entropy contribution of probability values in each interval. The intervals are designed to trade off the bias and variance of these estimates. </br></br>

<a href='http://arxiv.org/pdf/1911.03927.pdf'>1911.03927</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.0906баллов, №483</br>
<b>Cooperative Pathfinding based on memory-efficient Multi-agent RRT*</b></br>
Authors: , Jiang, Jinmingwu, Wu, Kaigui</br>
  In cooperative pathfinding problems, no-conflicts paths that bring several agents from their start location to their destination need to be planned. This problem can be efficiently solved by Multi-agent RRT*(MA-RRT*) algorithm, which offers better scalability than the classical algorithms, such as Optimal Anytime(OA), in sparse environments. However, the implementation of this algorithm in systems with limited memory is hindered because the number of nodes in the tree grows indefinitely as the paths get optimized. This paper proposes an improved version of MA-RRT*, called Multi-agent RRT* Fixed Node(MA-RRT*FN), which limits the number of nodes stored in the tree by removing the weak nodes which are not likely on the path reaching the goal. The results show that MA-RRT*FN performs close to MA-RRT* in terms of scalability and solution quality while the memory required is much lower and fixed. </br></br>

<a href='http://arxiv.org/pdf/1911.09030.pdf'>1911.09030</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0906баллов, №484</br>
<b>Local AdaAlter: Communication-Efficient Stochastic Gradient Descent with\n  Adaptive Learning Rates</b></br>
Authors: , Xie, Cong, Koyejo, Oluwasanmi, Gupta, Indranil, Lin, Haibin</br>
  Recent years have witnessed the growth of large-scale distributed machine learning algorithms -- specifically designed to accelerate model training by distributing computation across multiple machines. When scaling distributed training in this way, the communication overhead is often the bottleneck. In this paper, we study the local distributed Stochastic Gradient Descent~(SGD) algorithm, which reduces the communication overhead by decreasing the frequency of synchronization. While SGD with adaptive learning rates is a widely adopted strategy for training neural networks, it remains unknown how to implement adaptive learning rates in local SGD. To this end, we propose a novel SGD variant with reduced communication and adaptive learning rates, with provable convergence. Empirical results show that the proposed algorithm has fast convergence and efficiently reduces the communication overhead. </br></br>

<a href='http://arxiv.org/pdf/1911.08633.pdf'>1911.08633</a> &nbsp&nbsp (cs:ET) &nbsp&nbsp -0.0910баллов, №485</br>
<b>Adaptive Non-Uniform Compressive Sensing using SOT-MRAM Multibit\n  Crossbar Arrays</b></br>
Authors: , Salehi, Soheil, DeMara, Ronald F.</br>
  A Compressive Sensing (CS) approach is applied to utilize intrinsic computation capabilities of Spin-Orbit Torque Magnetic Random Access Memory (SOT-MRAM) devices for IoT applications wherein lifetime energy, device area, and manufacturing costs are highly-constrained while the sensing environment varies rapidly. In this manuscript, we propose the Adaptive Compressed-sampling via Multibit Crossbar Array (ACMCA) approach to intelligently generate the CS measurement matrix using a multibit SOT-MRAM crossbar array. SPICE circuit and MATLAB algorithm simulation results indicate that ACMCA reduces reconstruction Time-Averaged Normalized Mean Squared Error (TNMSE) by 5dB on average while providing up to 160$\\mu$m$^2$ area reduction compared to a similar previous design presented in the literature while incurring a negligible increase in the energy consumption of generating the CS measurement matrix. </br></br>

<a href='http://arxiv.org/pdf/1911.09287.pdf'>1911.09287</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp -0.0910баллов, №486</br>
<b>Band-limited Training and Inference for Convolutional Neural Networks</b></br>
Authors: , Dziedzic, Adam, Paparrizos, John, Krishnan, Sanjay, Elmore, Aaron, Franklin, Michael</br>
  The convolutional layers are core building blocks of neural network architectures. In general, a convolutional filter applies to the entire frequency spectrum of the input data. We explore artificially constraining the frequency spectra of these filters and data, called band-limiting, during training. The frequency domain constraints apply to both the feed-forward and back-propagation steps. Experimentally, we observe that Convolutional Neural Networks (CNNs) are resilient to this compression scheme and results suggest that CNNs learn to leverage lower-frequency components. In particular, we found: (1) band-limited training can effectively control the resource usage (GPU and memory); (2) models trained with band-limited layers retain high prediction accuracy; and (3) requires no modification to existing training algorithms or neural network architectures to use unlike other compression schemes. </br></br>

<a href='http://arxiv.org/pdf/1911.08354.pdf'>1911.08354</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0913баллов, №487</br>
<b>Energy Usage Reports: Environmental awareness as part of algorithmic\n  accountability</b></br>
Authors: , Lottick, Kadan, Susai, Silvia, Friedler, Sorelle A., Wilson, Jonathan P.</br>
  The carbon footprint of algorithms must be measured and transparently reported so computer scientists can take an honest and active role in environmental sustainability. In this paper, we take analyses usually applied at the industrial level and make them accessible for individual computer science researchers with an easy-to-use Python package. Localizing to the energy mixture of the electrical power grid, we make the conversion from energy usage to CO2 emissions, in addition to contextualizing these results with more human-understandable benchmarks such as automobile miles driven. We also include comparisons with energy mixtures employed in electrical grids around the world. We propose including these automatically-generated Energy Usage Reports as part of standard algorithmic accountability practices, and demonstrate the use of these reports as part of model-choice in a machine learning context. </br></br>

<a href='http://arxiv.org/pdf/1911.08922.pdf'>1911.08922</a> &nbsp&nbsp (cs:SD) &nbsp&nbsp -0.0914баллов, №488</br>
<b>Perceptual Loss Function for Neural Modelling of Audio Systems</b></br>
Authors: , Wright, Alec, V&#xe4;lim&#xe4;ki, Vesa</br>
  This work investigates alternate pre-emphasis filters used as part of the loss function during neural network training for nonlinear audio processing. In our previous work, the error-to-signal ratio loss function was used during network training, with a first-order highpass pre-emphasis filter applied to both the target signal and neural network output. This work considers more perceptually relevant pre-emphasis filters, which include lowpass filtering at high frequencies. We conducted listening tests to determine whether they offer an improvement to the quality of a neural network model of a guitar tube amplifier. Listening test results indicate that the use of an A-weighting pre-emphasis filter offers the best improvement among the tested filters. The proposed perceptual loss function improves the sound quality of neural network models in audio processing without affecting the computational cost. </br></br>

<a href='http://arxiv.org/pdf/1910.04872.pdf'>1910.04872</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.0923баллов, №489</br>
<b>Modeling Conceptual Understanding in Image Reference Games</b></br>
Authors: , Corona, Rodolfo, Alaniz, Stephan, Akata, Zeynep</br>
  An agent who interacts with a wide population of other agents needs to be aware that there may be variations in their understanding of the world. Furthermore, the machinery which they use to perceive may be inherently different, as is the case between humans and machines. In this work, we present both an image reference game between a speaker and a population of listeners where reasoning about the concepts other agents can comprehend is necessary and a model formulation with this capability. We focus on reasoning about the conceptual understanding of others, as well as adapting to novel gameplay partners and dealing with differences in perceptual machinery. Our experiments on three benchmark image/attribute datasets suggest that our learner indeed encodes information directly pertaining to the understanding of other agents, and that leveraging this information is crucial for maximizing gameplay performance. </br></br>

<a href='http://arxiv.org/pdf/1911.09010.pdf'>1911.09010</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.0934баллов, №490</br>
<b>Experimental Exploration of Compact Convolutional Neural Network\n  Architectures for Non-temporal Real-time Fire Detection</b></br>
Authors: , A., Ganesh Samarth C., Bhowmik, Neelanjan, Breckon, Toby P.</br>
  In this work we explore different Convolutional Neural Network (CNN) architectures and their variants for non-temporal binary fire detection and localization in video or still imagery. We consider the performance of experimentally defined, reduced complexity deep CNN architectures for this task and evaluate the effects of different optimization and normalization techniques applied to different CNN architectures (spanning the Inception, ResNet and EfficientNet architectural concepts). Contrary to contemporary trends in the field, our work illustrates a maximum overall accuracy of 0.96 for full frame binary fire detection and 0.94 for superpixel localization using an experimentally defined reduced CNN architecture based on the concept of InceptionV4. We notably achieve a lower false positive rate of 0.06 compared to prior work in the field presenting an efficient, robust and real-time solution for fire region detection. </br></br>

<a href='http://arxiv.org/pdf/1911.09292.pdf'>1911.09292</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.0937баллов, №491</br>
<b>Machine Learning based detection of multiple Wi-Fi BSSs for LTE-U CSAT</b></br>
Authors: , Sathya, Vanlin, Dziedzic, Adam, Ghosh, Monisha, Krishnan, Sanjay</br>
  According to the LTE-U Forum specification, a LTE-U base-station (BS) reduces its duty cycle from 50% to 33% when it senses an increase in the number of co-channel Wi-Fi basic service sets (BSSs) from one to two. The detection of the number of Wi-Fi BSSs that are operating on the channel in real-time, without decoding the Wi-Fi packets, still remains a challenge. In this paper, we present a novel machine learning (ML) approach that solves the problem by using energy values observed during LTE-U OFF duration. Observing the energy values (at LTE-U BS OFF time) is a much simpler operation than decoding the entire Wi-Fi packets. In this work, we implement and validate the proposed ML based approach in real-time experiments, and demonstrate that there are two distinct patterns between one and two Wi-Fi APs. This approach delivers an accuracy close to 100% compared to auto-correlation (AC) and energy detection (ED) approaches. </br></br>

<a href='http://arxiv.org/pdf/1911.08344.pdf'>1911.08344</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.0941баллов, №492</br>
<b>FollowMeUp Sports: New Benchmark for 2D Human Keypoint Recognition</b></br>
Authors: , Huang, Ying, Sun, Bin, Kan, Haipeng, Zhuang, Jiankai, Qin, Zengchang</br>
  Human pose estimation has made significant advancement in recent years. However, the existing datasets are limited in their coverage of pose variety. In this paper, we introduce a novel benchmark FollowMeUp Sports that makes an important advance in terms of specific postures, self-occlusion and class balance, a contribution that we feel is required for future development in human body models. This comprehensive dataset was collected using an established taxonomy of over 200 standard workout activities with three different shot angles. The collected videos cover a wider variety of specific workout activities than previous datasets including push-up, squat and body moving near the ground with severe self-occlusion or occluded by some sport equipment and outfits. Given these rich images, we perform a detailed analysis of the leading human pose estimation approaches gaining insights for the success and failures of these methods. </br></br>

<a href='http://arxiv.org/pdf/1911.08411.pdf'>1911.08411</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0957баллов, №493</br>
<b>Mixed-curvature Variational Autoencoders</b></br>
Authors: , Skopek, Ondrej, Ganea, Octavian-Eugen, B&#xe9;cigneul, Gary</br>
  It has been shown that using geometric spaces with non-zero curvature instead of plain Euclidean spaces with zero curvature improves performance on a range of Machine Learning tasks for learning representations. Recent work has leveraged these geometries to learn latent variable models like Variational Autoencoders (VAEs) in spherical and hyperbolic spaces with constant curvature. While these approaches work well on particular kinds of data that they were designed for e.g. tree-like data for a hyperbolic VAE, there exists no generic approach unifying all three models. We develop a Mixed-curvature Variational Autoencoder, an efficient way to train a VAE whose latent space is a product of constant curvature Riemannian manifolds, where the per-component curvature can be learned. This generalizes the Euclidean VAE to curved latent spaces, as the model essentially reduces to the Euclidean VAE if curvatures of all latent space components go to 0. </br></br>

<a href='http://arxiv.org/pdf/1911.09454.pdf'>1911.09454</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.0959баллов, №494</br>
<b>Customized Graph Embedding: Tailoring the Embedding Vector to a Specific\n  Application</b></br>
Authors: , Hou, Bitan, Wang, Yujing, Zeng, Ming, Jiang, Shan, Mengshoel, Ole J., Tong, Yunhai, Bai, Jing</br>
  The graph is a natural representation of data in a variety of <font color="#009600">real-world</font> applications, for example as a <font color="#960096">knowledge graph</font>, a social network, or a biological network. To better leverage the information behind the data, the method of graph embedding is recently proposed and extensively studied. The traditional graph embedding method, while it provides an effective way to understand what is behind the graph data, is unfortunately sub-optimal in many cases. This is because its learning procedure is disconnected from the target application. In this paper, we propose a novel approach, Customized Graph Embedding (CGE), to tackle this problem. The CGE algorithm learns a customized vector representation of the graph by differentiating the varying importance of distinct graph paths. Experiments are carried out on a diverse set of node classification datasets and strong performance is demonstrated. </br></br>

<a href='http://arxiv.org/pdf/1911.07034.pdf'>1911.07034</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.0965баллов, №495</br>
<b>Instance Shadow Detection</b></br>
Authors: , Wang, Tianyu, Hu, Xiaowei, Wang, Qiong, Heng, Pheng-Ann, Fu, Chi-Wing</br>
  Instance shadow detection is a brand new problem, aiming to find shadow instances paired with object instances. To approach it, we first prepare a new dataset called SOBA, named after Shadow-OBject Association, with 3,623 pairs of shadow and object instances in 1,000 photos, each with individual labeled masks. Second, we design LISA, named after Light-guided Instance Shadow-object Association, an end-to-end framework to automatically predict the shadow and object instances, together with the shadow-object associations and light direction. Then, we pair up the predicted shadow and object instances, and match them with the predicted shadow-object associations to generate the final results. In our evaluations, we formulate a new metric named the shadow-object average precision to measure the performance of our results. Further, we conducted various experiments and demonstrate our method\'s applicability on light direction estimation and photo editing. </br></br>

<a href='http://arxiv.org/pdf/1911.06981.pdf'>1911.06981</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0968баллов, №496</br>
<b>Parametric Graph-based Separable Transforms for Video Coding</b></br>
Authors: , Egilmez, Hilmi E., Teke, Oguzhan, Said, Amir, Seregin, Vadim, Karczewicz, Marta</br>
  In many video coding systems, separable transforms (such as two-dimensional DCT-2) have been used to code block residual signals obtained after prediction. This paper proposes a parametric approach to build graph-based separable transforms (GBSTs) for video coding. Specifically, a GBST is derived from a pair of line graphs, whose weights are determined based on two non-negative parameters. As certain choices of those parameters correspond to the discrete sine and cosine transform types used in recent video coding standards (including DCT-2, DST-7 and DCT-8), this paper further optimizes these graph parameters to better capture residual block statistics and improve video coding efficiency. The proposed GBSTs are tested on the Versatile Video Coding (VVC) reference software, and the experimental results show that about 0.4\\% average coding gain is achieved over the existing set of separable transforms constructed based on DCT-2, DST-7 and DCT-8 in VVC. </br></br>

<a href='http://arxiv.org/pdf/1911.07508.pdf'>1911.07508</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0972баллов, №497</br>
<b>Safe squeezing for antisparse coding</b></br>
Authors: , Elvira, Cl&#xe9;ment, Herzet, C&#xe9;dric</br>
  Spreading the information over all coefficients of a representation is a desirable property in many applications such as digital communication or machine learning. This so-called antisparse representation can be obtained by solving a convex program involving an $\\ell_\\infty$-norm penalty combined with a quadratic discrepancy. In this paper, we propose a new methodology, dubbed safe squeezing, to accelerate the computation of antisparse representation. We describe a test that allows to detect saturated entries in the solution of the optimization problem. The contribution of these entries is compacted into a single vector, thus operating a form of dimensionality reduction. We propose two algorithms to solve the resulting lower dimensional problem. Numerical experiments show the effectiveness of the proposed method to detect the saturated components of the solution and illustrates the induced computational gains in the resolution of the antisparse problem. </br></br>

<a href='http://arxiv.org/pdf/1911.05151.pdf'>1911.05151</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.0972баллов, №498</br>
<b>Learning from Data-Rich Problems: A Case Study on Genetic Variant\n  Calling</b></br>
Authors: , Yi, Ren, Chang, Pi-Chuan, Baid, Gunjan, Carroll, Andrew</br>
  Next Generation Sequencing can sample the whole genome (WGS) or the 1-2% of the genome that codes for proteins called the whole exome (WES). Machine learning approaches to variant calling achieve high accuracy in WGS data, but the reduced number of training examples causes training with WES data alone to achieve lower accuracy. We propose and compare three different data augmentation strategies for improving performance on WES data: 1) joint training with WES and WGS data, 2) warmstarting the WES model from a WGS model, and 3) joint training with the sequencing type specified. All three approaches show improved accuracy over a model trained using just WES data, suggesting the ability of models to generalize insights from the greater WGS data while retaining performance on the specialized WES problem. These data augmentation approaches may apply to other problem areas in genomics, where several specialized models would each see only a subset of the genome. </br></br>

<a href='http://arxiv.org/pdf/1911.09599.pdf'>1911.09599</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.0975баллов, №499</br>
<b>Synthesizing Visual Illusions Using Generative Adversarial Networks</b></br>
Authors: , Gomez-Villa, Alexander, Mart&#xed;n, Adrian, Vazquez-Corral, Javier, Malo, Jes&#xfa;s, Bertalm&#xed;o, Marcelo</br>
  Visual illusions are a very useful tool for vision scientists, because they allow them to better probe the limits, thresholds and errors of the visual system. In this work we introduce the first ever framework to generate novel visual illusions with an artificial neural network (ANN). It takes the form of a generative adversarial network, with a generator of visual illusion candidates and two discriminator modules, one for the inducer background and another that decides whether or not the candidate is indeed an illusion. The generality of the model is exemplified by synthesizing illusions of different types, and validated with psychophysical experiments that corroborate that the outputs of our ANN are indeed visual illusions to human observers. Apart from synthesizing new visual illusions, which may help vision researchers, the proposed model has the potential to open new ways to study the similarities and differences between ANN and human visual perception. </br></br>

<a href='http://arxiv.org/pdf/1911.09204.pdf'>1911.09204</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.0981баллов, №500</br>
<b>DR-KFD: A Differentiable Visual Metric for 3D Shape Reconstruction</b></br>
Authors: , Jin, Jiongchao, Patil, Akshay Gadi, Hao, Zhang</br>
  We advocate the use of differential visual shape metrics to train deep neural networks for 3D reconstruction. We introduce such a metric which compares two 3D shapes by measuring visual, image-space differences between multiview images differentiably rendered from the shapes. Furthermore, we develop a differentiable image-space distance based on mean-squared errors defined over Hard- Net features computed from probabilistic keypoint maps of the compared images. Our differential visual shape metric can be easily plugged into various reconstruction networks, replacing the object-space distortion measures, such as Chamfer or Earth Mover distances, so as to optimize the network weights to produce reconstruction results with better structural fidelity and visual quality. We demonstrate this both objectively, using well-known visual shape metrics for retrieval and classification tasks that are independent from our new metric, and subjectively through a perceptual study. </br></br>

<a href='http://arxiv.org/pdf/1911.08197.pdf'>1911.08197</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.0989баллов, №501</br>
<b>Sequential Mode Estimation with Oracle Queries</b></br>
Authors: , Shah, Dhruti, Choudhury, Tuhinangshu, Karamchandani, Nikhil, Gopalan, Aditya</br>
  We consider the problem of adaptively PAC-learning a probability distribution $\\mathcal{P}$\'s mode by querying an oracle for information about a sequence of i.i.d. samples $X_1, X_2, \\ldots$ generated from $\\mathcal{P}$. We consider two different query models: (a) each query is an index $i$ for which the oracle reveals the value of the sample $X_i$, (b) each query is comprised of two indices $i$ and $j$ for which the oracle reveals if the samples $X_i$ and $X_j$ are the same or not. For these query models, we give sequential mode-estimation algorithms which, at each time $t$, either make a query to the corresponding oracle based on past observations, or decide to stop and output an estimate for the distribution\'s mode, required to be correct with a specified confidence. We analyze the query complexity of these algorithms for any underlying distribution $\\mathcal{P}$, and derive corresponding lower bounds on the optimal query complexity under the two querying models. </br></br>

<a href='http://arxiv.org/pdf/1911.08048.pdf'>1911.08048</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1012баллов, №502</br>
<b>Gradient-based Sparse Principal Component Analysis with Extensions to\n  Online Learning</b></br>
Authors: , Qiu, Yixuan, Lei, Jing, Roeder, Kathryn</br>
  Sparse principal component analysis (PCA) is an important technique for dimensionality reduction of high-dimensional data. However, most existing sparse PCA algorithms are based on non-convex optimization, which provide little guarantee on the global convergence. Sparse PCA algorithms based on a convex formulation, for example the Fantope projection and selection (FPS), overcome this difficulty, but are computationally expensive. In this work we study sparse PCA based on the convex FPS formulation, and propose a new algorithm that is computationally efficient and applicable to large and high-dimensional data sets. Nonasymptotic and explicit bounds are derived for both the optimization error and the statistical accuracy, which can be used for testing and inference problems. We also extend our algorithm to online learning problems, where data are obtained in a streaming fashion. The proposed algorithm is applied to high-dimensional gene expression data for the detection of functional gene groups. </br></br>

<a href='http://arxiv.org/pdf/1911.07229.pdf'>1911.07229</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.1012баллов, №503</br>
<b>Learning Query Inseparable ELH Ontologies</b></br>
Authors: , Ozaki, Ana, Persia, Cosimo, Mazzullo, Andrea</br>
  We investigate the complexity of learning query inseparable ELH ontologies in a variant of Angluin\'s exact learning model. Given a fixed data instance A* and a query language Q, we are interested in computing an ontology H that entails the same queries as a target ontology T on A*, that is, H and T are inseparable w.r.t. A* and Q. The learner is allowed to pose two kinds of questions. The first is `Does (T,A)\\models q?\', with A an arbitrary data instance and q and query in Q. An oracle replies this question with `yes\' or `no\'. In the second, the learner asks `Are H and T inseparable w.r.t. A* and Q?\'. If so, the learning process finishes, otherwise, the learner receives (A*,q) with q in Q,   (T,A*)\\models q and (H,A*)\ ot\\models q (or vice-versa). Then, we analyse conditions in which query inseparability is preserved if A* changes. Finally, we consider the PAC learning model and a setting where the algorithms learn from a batch of classified data, limiting interactions with the oracles. </br></br>

<a href='http://arxiv.org/pdf/1911.03787.pdf'>1911.03787</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1016баллов, №504</br>
<b>Learning to Optimize in Swarms</b></br>
Authors: , Cao, Yue, Chen, Tianlong, Wang, Zhangyang, Shen, Yang</br>
  Learning to optimize has emerged as a powerful framework for various optimization and machine learning tasks. Current such &quot;meta-optimizers&quot; often learn in the space of continuous optimization algorithms that are point-based and uncertainty-unaware. To overcome the limitations, we propose a meta-optimizer that learns in the algorithmic space of both point-based and population-based optimization algorithms. The meta-optimizer targets at a meta-loss function consisting of both cumulative regret and entropy. Specifically, we learn and <font color="#be00be">interpret</font> the update formula through a population of LSTMs embedded with sample- and feature-level attentions. Meanwhile, we estimate the posterior directly over the global optimum and use an uncertainty measure to help guide the learning process. Empirical results over non-convex test functions and the protein-docking application demonstrate that this new meta-optimizer <font color="#00be00">outperform</font>s existing competitors. </br></br>

<a href='http://arxiv.org/pdf/1911.08119.pdf'>1911.08119</a> &nbsp&nbsp (cs:ML, cs:CV) &nbsp&nbsp -0.1018баллов, №505</br>
<b>Adaptive Routing Between Capsules</b></br>
Authors: , Ren, Qiang, Shang, Shaohua, He, Lianghua</br>
  Capsule network is the most recent exciting advancement in the deep learning field and represents positional information by stacking features into vectors. The dynamic routing algorithm is used in the capsule network, however, there are some disadvantages such as the inability to stack multiple layers and a large amount of computation. In this paper, we propose an adaptive routing algorithm that can solve the problems mentioned above. First, the low-layer capsules adaptively adjust their direction and length in the routing algorithm and removing the influence of the coupling coefficient on the gradient propagation, so that the network can work when stacked in multiple layers. Then, the iterative process of routing is simplified to reduce the amount of computation and we introduce the gradient coefficient $\\lambda$. Further, we tested the performance of our proposed adaptive routing algorithm on CIFAR10, Fashion-MNIST, SVHN and MNIST, while achieving better results than the dynamic routing algorithm. </br></br>

<a href='http://arxiv.org/pdf/1911.09365.pdf'>1911.09365</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.1027баллов, №506</br>
<b>Generalized Planning with Positive and Negative Examples</b></br>
Authors: , Segovia-Aguas, Javier, Jim&#xe9;nez, Sergio, Jonsson, Anders</br>
  Generalized planning aims at computing an algorithm-like structure (generalized plan) that solves a set of multiple planning instances. In this paper we define negative examples for generalized planning as planning instances that must not be solved by a generalized plan. With this regard the paper extends the notion of validation of a generalized plan as the problem of verifying that a given generalized plan solves the set of input positives instances while it fails to solve a given input set of negative examples. This notion of plan validation allows us to define quantitative metrics to asses the generalization capacity of generalized plans. The paper also shows how to incorporate this new notion of plan validation into a compilation for plan synthesis that takes both positive and negative instances as input. Experiments show that incorporating negative examples can accelerate plan synthesis in several domains and leverage quantitative metrics to evaluate the generalization capacity of the synthesized plans. </br></br>

<a href='http://arxiv.org/pdf/1911.07201.pdf'>1911.07201</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1033баллов, №507</br>
<b>Countering Inconsistent Labelling by Google\'s Vision API for Rotated\n  Images</b></br>
Authors: , Apte, Aman, Bandyopadhyay, Aritra, Shenoy, K Akhilesh, Andrews, Jason Peter, Rathod, Aditya, Agnihotri, Manish, Jajodia, Aditya</br>
  <font color="#00be00">Google</font>\'s Vision API analyses images and provides a variety of output predictions, one such type is context-based labelling. In this paper, it is shown that adversarial examples that cause incorrect label prediction and <font color="#be00be">spoof</font>ing can be generated by rotating the images. Due to the black-boxed nature of the API, a modular context-based pre-processing pipeline is proposed consisting of a Res-Net50 model, that predicts the angle by which the image must be rotated to correct its orientation. The pipeline successfully performs the correction whilst maintaining the image\'s resolution and feeds it to the API which generates labels similar to the original correctly oriented image and using a Percentage Error metric, the performance of the corrected images as compared to its rotated counter-parts is found to be significantly higher. These observations imply that the API can benefit from such a pre-processing pipeline to increase robustness to rotational perturbances. </br></br>

<a href='http://arxiv.org/pdf/1911.09296.pdf'>1911.09296</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1045баллов, №508</br>
<b>xBD: A Dataset for Assessing Building Damage from Satellite Imagery</b></br>
Authors: , Gupta, Ritwik, Hosfelt, Richard, Sajeev, Sandra, Patel, Nirav, Goodman, Bryce, Doshi, Jigar, Heim, Eric, Choset, Howie, Gaston, Matthew</br>
  We present xBD, a new, large-scale dataset for the advancement of change detection and building damage assessment for humanitarian assistance and disaster recovery research. Natural disaster response requires an accurate understanding of damaged buildings in an affected region. Current response strategies require in-person damage assessments within 24-48 hours of a disaster. Massive potential exists for using aerial imagery combined with computer vision algorithms to assess damage and reduce the potential danger to human life. In collaboration with multiple disaster response agencies, xBD provides pre- and post-event satellite imagery across a variety of disaster events with building polygons, ordinal labels of damage level, and corresponding satellite metadata. Furthermore, the dataset contains bounding boxes and labels for environmental factors such as fire, water, and smoke. xBD is the largest building damage assessment dataset to date, containing 850,736 building annotations across 45,362 km\\textsuperscript{2} of imagery. </br></br>

<a href='http://arxiv.org/pdf/1911.06975.pdf'>1911.06975</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1046баллов, №509</br>
<b>Long Range 3D with Quadocular Thermal (LWIR) Camera</b></br>
Authors: , Filippov, Andrey, Dzhimiev, Oleg</br>
  Long Wave Infrared (LWIR) cameras provide images regardles of the ambient illumination, they tolerate fog and are not blinded by the incoming car headlights. These features make LWIR cameras attractive for autonomous navigation, security and military applications. Thermal images can be used similarly to the visible range ones, including 3D scene reconstruction with two or more such cameras mounted on a rigid frame. There are two additional challenges for this spectral range: lower image resolution and lower contrast of the textures.   In this work, we demonstrate quadocular LWIR camera setup, calibration, image capturing and processing that result in long range 3D perception with 0.077 pix disparity error over 90% of the depth map. With low resolution (160 x 120) LWIR sensors we achieved 10% range accuracy at 28 m with 56 degrees horizontal field of view (HFoV) and 150 mm baseline. Scaled to the now-standard 640 x 512 resolution and 200 mm baseline suitable for head-mounted application the result would be 10% accuracy at 130 m. </br></br>

<a href='http://arxiv.org/pdf/1911.08850.pdf'>1911.08850</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.1052баллов, №510</br>
<b>Self-supervised Learning of 3D Objects from Natural Images</b></br>
Authors: , Kato, Hiroharu, Harada, Tatsuya</br>
  We present a method to learn single-view reconstruction of the 3D shape, pose, and texture of objects from categorized natural images in a self-supervised manner. Since this is a severely ill-posed problem, carefully designing a training method and introducing constraints are essential. To avoid the difficulty of training all elements at the same time, we propose training category-specific base shapes with fixed pose distribution and simple textures first, and subsequently training poses and textures using the obtained shapes. Another difficulty is that shapes and backgrounds sometimes become excessively complicated to mistakenly reconstruct textures on object surfaces. To suppress it, we propose using strong regularization and constraints on object surfaces and background images. With these two techniques, we demonstrate that we can use natural image collections such as CIFAR-10 and PASCAL objects for training, which indicates the possibility to realize 3D object reconstruction on diverse object categories beyond synthetic datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.09659.pdf'>1911.09659</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1056баллов, №511</br>
<b>AdaFilter: Adaptive Filter Fine-tuning for Deep Transfer Learning</b></br>
Authors: , Guo, Yunhui, Li, Yandong, Wang, Liqiang, Rosing, Tajana</br>
  There is an increasing number of pre-trained deep neural network models. However, it is still unclear how to effectively use these models for a new task. Transfer learning, which aims to transfer knowledge from source tasks to a target task, is an effective solution to this problem. Fine-tuning is a popular transfer learning technique for deep neural networks where a few rounds of training are applied to the parameters of a pre-trained model to adapt them to a new task. Despite its popularity, in this paper, we show that fine-tuning suffers from several drawbacks. We propose an adaptive fine-tuning approach, called AdaFilter, which selects only a part of the convolutional filters in the pre-trained model to optimize on a per-example basis. We use a recurrent gated network to selectively fine-tune convolutional filters based on the activations of the previous layer. We experiment with 7 public image classification datasets and the results show that AdaFilter can reduce the average classification error of the standard fine-tuning by 2.54%. </br></br>

<a href='http://arxiv.org/pdf/1911.08199.pdf'>1911.08199</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.1058баллов, №512</br>
<b>Weakly-Supervised Video Moment Retrieval via Semantic Completion Network</b></br>
Authors: , Lin, Zhijie, Zhao, Zhou, Zhang, Zhu, Wang, Qi, Liu, Huasheng</br>
  Video moment retrieval is to search the moment that is most relevant to the given natural language query. Existing methods are mostly trained in a fully-supervised setting, which requires the full annotations of temporal boundary for each query. However, manually labeling the annotations is actually time-consuming and expensive. In this paper, we propose a novel weakly-supervised moment retrieval framework requiring only coarse video-level annotations for training. Specifically, we devise a proposal generation module that aggregates the context information to generate and score all candidate proposals in one single pass. We then devise an algorithm that considers both exploitation and exploration to select top-K proposals. Next, we build a semantic completion module to measure the semantic similarity between the selected proposals and query, compute reward and provide feedbacks to the proposal generation module for scoring refinement. Experiments on the ActivityCaptions and Charades-STA demonstrate the effectiveness of our proposed method. </br></br>

<a href='http://arxiv.org/pdf/1911.06882.pdf'>1911.06882</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.1065баллов, №513</br>
<b>Adaptive Leader-Follower Formation Control and Obstacle Avoidance via\n  Deep Reinforcement Learning</b></br>
Authors: , Zhou, Yanlin, Lu, Fan, Pu, George, Ma, Xiyao, Sun, Runhan, Chen, Hsi-Yuan, Li, Xiaolin, Wu, Dapeng</br>
  We propose a deep <font color="#00be00">reinforcement learning</font> (DRL) methodology for the <font color="#be00be">tracking</font>, obstacle avoidance, and formation control of nonholonomic robots. By separating vision-based control into a perception module and a controller module, we can train a DRL agent without sophisticated physics or 3D modeling. In addition, the modular framework averts daunting retrains of an image-to-action end-to-end neural network, and provides flexibility in transferring the controller to different robots. First, we train a convolutional neural network (CNN) to accurately localize in an indoor setting with dynamic foreground/background. Then, we design a new DRL algorithm named Momentum Policy Gradient (MPG) for continuous control tasks and prove its convergence. We also show that MPG is robust at tracking varying leader movements and can naturally be extended to problems of formation control. Leveraging reward shaping, features such as collision and obstacle avoidance can be easily integrated into a DRL controller. </br></br>

<a href='http://arxiv.org/pdf/1911.06858.pdf'>1911.06858</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.1067баллов, №514</br>
<b>Deep Learning with Persistent Homology for Orbital Angular Momentum\n  (OAM) Decoding</b></br>
Authors: , Rostami, Soheil, Saad, Walid, Hong, Choong Seon</br>
  Orbital angular momentum (OAM)-encoding has recently emerged as an effective approach for increasing the channel capacity of free-space optical communications. In this paper, OAM-based decoding is formulated as a supervised classification problem. To maintain lower error rate in presence of severe atmospheric turbulence, a new approach that combines effective machine learning tools from persistent homology and convolutional neural networks (CNNs) is proposed to decode the OAM modes. A <font color="blue">Gaussi</font>an <font color="blue">kernel</font> with learnable parameters is proposed in order to connect persistent homology to CNN, allowing the system to extract and distinguish robust and unique topological features for the OAM modes. Simulation results show that the proposed approach achieves up to 20% gains in classification accuracy rate over <font color="red">state-of-the-art</font> of method based on only CNNs. These results essentially show that geometric and topological features play a pivotal role in the OAM mode classification problem. </br></br>

<a href='http://arxiv.org/pdf/1910.11632.pdf'>1910.11632</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1071баллов, №515</br>
<b>An End-to-End HW/SW Co-Design Methodology to Design Efficient Deep\n  Neural Network Systems using Virtual Models</b></br>
Authors: , Klaiber, Michael J., Vogel, Sebastian, Acosta, Axel, Korn, Robert, Ecco, Leonardo, Back, Kristine, Guntoro, Andre, Feldner, Ingo</br>
  End-to-end performance estimation and measurement of deep neural network (DNN) systems become more important with increasing complexity of DNN systems consisting of hardware and software components. The methodology proposed in this paper aims at a reduced turn-around time for evaluating different design choices of hardware and software components of DNN systems. This reduction is achieved by moving the performance estimation from the implementation phase to the concept phase by employing virtual hardware models instead of gathering measurement results from physical prototypes. Deep learning compilers introduce hardware-specific transformations and are, therefore, considered a part of the design flow of virtual system models to extract end-to-end performance estimations. To validate the run-time accuracy of the proposed methodology, a system processing the DilatedVGG DNN is realized both as virtual system model and as hardware implementation. The results show that up to 92 % accuracy can be reached in predicting the processing time of the DNN inference. </br></br>

<a href='http://arxiv.org/pdf/1911.06866.pdf'>1911.06866</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1078баллов, №516</br>
<b>Multi-attention Networks for Temporal Localization of Video-level Labels</b></br>
Authors: , Zhang, Lijun, Nizampatnam, Srinath, Gangopadhyay, Ahana, Conde, Marcos V.</br>
  Temporal localization remains an important challenge in video understanding. In this work, we present our solution to the 3rd YouTube-8M Video Understanding Challenge organized by <font color="#00be00">Google</font> Research. Participants were required to build a segment-level classifier using a large-scale training data set with noisy video-level labels and a relatively small-scale validation data set with accurate segment-level labels. We formulated the problem as a multiple instance multi-label learning and developed an attention-based mechanism to selectively emphasize the important frames by attention weights. The model performance is further improved by constructing multiple sets of attention networks. We further fine-tuned the model using the segment-level data set. Our final model consists of an ensemble of attention/multi-attention networks, deep bag of frames models, recurrent neural networks and convolutional neural networks. It ranked 13th on the <font color="#be00be">private</font> leader board and stands out for its efficient usage of resources. </br></br>

<a href='http://arxiv.org/pdf/1911.07935.pdf'>1911.07935</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1091баллов, №517</br>
<b>Fitness Done Right: a Real-time Intelligent Personal Trainer for\n  Exercise Correction</b></br>
Authors: , Chen, Yun, Chen, Yiyue, Tu, Zhengzhong</br>
  Keeping fit has been increasingly important for people nowadays. However, people may not get expected exercise results without following professional guidance while hiring personal trainers is expensive. In this paper, an effective real-time system called Fitness Done Right (FDR) is proposed for helping people exercise correctly on their own. The system includes detecting human body parts, recognizing exercise pose and detecting errors for test poses as well as giving correction advice. Generally, two branch multi-stage CNN is used for training data sets in order to learn human body parts and associations. Then, considering two poses, which are plank and squat in our model, we design a detection algorithm, combining Euclidean and angle distances, to determine the pose in the image. Finally, key values for key features of the two poses are computed correspondingly in the pose error detection part, which helps give correction advice. We conduct our system in real-time situation with error rate down to $1.2\\%$, and the screenshots of experimental results are also presented. </br></br>

<a href='http://arxiv.org/pdf/1911.08028.pdf'>1911.08028</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1093баллов, №518</br>
<b>Simultaneous Region Localization and Hash Coding for Fine-grained Image\n  Retrieval</b></br>
Authors: , Zeng, Haien, Lai, Hanjiang, Yin, Jian</br>
  Fine-grained image hashing is a challenging problem due to the difficulties of discriminative region localization and hash code generation. Most existing deep hashing approaches solve the two tasks independently. While these two tasks are correlated and can reinforce each other. In this paper, we propose a deep fine-grained hashing to simultaneously localize the discriminative regions and generate the efficient binary codes. The proposed approach consists of a region localization module and a hash coding module. The region localization module aims to provide informative regions to the hash coding module. The hash coding module aims to generate effective binary codes and give feedback for learning better localizer. Moreover, to better capture subtle differences, multi-scale regions at different layers are learned without the need of bounding-box/part annotations. Extensive experiments are conducted on two public benchmark fine-grained datasets. The results demonstrate significant improvements in the performance of our method relative to other fine-grained hashing algorithms. </br></br>

<a href='http://arxiv.org/pdf/1911.07337.pdf'>1911.07337</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -0.1106баллов, №519</br>
<b>Stochastic Gradient Annealed Importance Sampling for Efficient Online\n  Marginal Likelihood Estimation</b></br>
Authors: , Cameron, Scott A., Eggers, Hans C., Kroon, Steve</br>
  We consider estimating the marginal likelihood in settings with independent and identically distributed (i.i.d.) data. We propose estimating the predictive distributions in a sequential factorization of the marginal likelihood in such settings by using stochastic gradient Markov Chain Monte Carlo techniques. This approach is far more efficient than traditional marginal likelihood estimation techniques such as nested sampling and annealed importance sampling due to its use of mini-batches to approximate the likelihood. Stability of the estimates is provided by an adaptive annealing schedule. The resulting stochastic gradient annealed importance sampling (SGAIS) technique, which is the key contribution of our paper, enables us to estimate the marginal likelihood of a number of models considerably faster than traditional approaches, with no noticeable loss of accuracy. An important benefit of our approach is that the marginal likelihood is calculated in an online fashion as data becomes available, allowing the estimates to be used for applications such as online weighted model combination. </br></br>

<a href='http://arxiv.org/pdf/1911.08177.pdf'>1911.08177</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.1109баллов, №520</br>
<b>Rethinking deep active learning: Using unlabeled data at model training</b></br>
Authors: , Sim&#xe9;oni, Oriane, Budnik, Mateusz, Avrithis, Yannis, Gravier, Guillaume</br>
  Active learning typically focuses on training a model on few labeled examples alone, while unlabeled ones are only used for acquisition. In this work we depart from this setting by using both labeled and unlabeled data during model training across active learning cycles. We do so by using unsupervised feature learning at the beginning of the active learning pipeline and semi-supervised learning at every active learning cycle, on all available data. The former has not been investigated before in active learning, while the study of latter in the context of deep learning is scarce and recent findings are not conclusive with respect to its benefit. Our idea is orthogonal to acquisition strategies by using more data, much like ensemble methods use more models. By systematically evaluating on a number of popular acquisition strategies and datasets, we find that the use of unlabeled data during model training brings a surprising accuracy improvement in image classification, compared to the differences between acquisition strategies. We thus explore smaller label budgets, even one label per class. </br></br>

<a href='http://arxiv.org/pdf/1911.07249.pdf'>1911.07249</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1116баллов, №521</br>
<b>The Proper Care and Feeding of CAMELS: How Limited Training Data Affects\n  Streamflow Prediction</b></br>
Authors: , Gauch, Martin, Mai, Juliane, Lin, Jimmy</br>
  Accurate streamflow prediction largely relies on historical records of both meteorological data and streamflow measurements. For many regions around the world, however, such data are only scarcely or not at all available. To select an appropriate model for a region with a given amount of historical data, it is therefore indispensable to know a model\'s sensitivity to limited training data, both in terms of geographic diversity and different spans of time. In this study, we provide decision support for tree- and LSTM-based models. We feed the models meteorological measurements from the CAMELS dataset, and individually restrict the training period length and the number of basins used in training. Our findings show that tree-based models provide more accurate predictions on small datasets, while LSTMs are superior given sufficient training data. This is perhaps not surprising, as neural networks are known to be data-hungry; however, we are able to characterize each model\'s strengths under different conditions, including the &quot;breakeven point&quot; when LSTMs begin to overtake tree-based models. </br></br>

<a href='http://arxiv.org/pdf/1911.08656.pdf'>1911.08656</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1138баллов, №522</br>
<b>W-Net: Two-stage U-Net with misaligned data for raw-to-RGB mapping</b></br>
Authors: , Uhm, Kwang-Hyun, Kim, Seung-Wook, Ji, Seo-Won, Cho, Sung-Jin, Hong, Jun-Pyo, Ko, Sung-Jea</br>
  Recent research on a learning mapping between raw Bayer images and RGB images has progressed with the development of deep convolutional neural networks. A challenging data set namely the Zurich Raw-to-RGB data set (ZRR) has been released in the AIM 2019 raw-to-RGB mapping challenge. In ZRR, input raw and target RGB images are captured by two different cameras and thus not perfectly aligned. Moreover, camera metadata such as white balance gains and color correction matrix are not provided, which makes the challenge more difficult. In this paper, we explore an effective network structure and a loss function to address these issues. We exploit a two-stage U-Net architecture and also introduce a loss function that is less variant to alignment and more sensitive to color differences. In addition, we show an ensemble of networks trained with different loss functions can bring a significant performance gain. We demonstrate the superiority of our method by achieving the highest score in terms of both the peak signal-to-noise ratio and the structural similarity and obtaining the second-best mean-opinion-score in the challenge. </br></br>

<a href='http://arxiv.org/pdf/1911.09168.pdf'>1911.09168</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.1140баллов, №523</br>
<b>Active Learning for Deep Detection Neural Networks</b></br>
Authors: , Aghdam, Hamed H., Gonzalez-Garcia, Abel, van de Weijer, Joost, L&#xf3;pez, Antonio M.</br>
  The cost of drawing object bounding boxes (i.e. labeling) for millions of images is prohibitively high. For instance, labeling <font color="#be00be">pedestrian</font>s in a regular urban image could take 35 seconds on average. Active learning aims to reduce the cost of labeling by selecting only those images that are informative to improve the detection network accuracy. In this paper, we propose a method to perform active learning of object detectors based on convolutional neural networks. We propose a new image-level scoring process to rank unlabeled images for their automatic selection, which clearly <font color="#00be00">outperform</font>s classical scores. The proposed method can be applied to videos and sets of still images. In the former case, temporal selection rules can complement our scoring process. As a relevant use case, we extensively study the performance of our method on the task of pedestrian detection. Overall, the experiments show that the proposed method performs better than random selection. Our codes are <font color="#00be00">publicly available</font> at www.gitlab.com/haghdam/deep_active_learning. </br></br>

<a href='http://arxiv.org/pdf/1911.09579.pdf'>1911.09579</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1149баллов, №524</br>
<b>Knowledge Graph Transfer Network for Few-Shot Recognition</b></br>
Authors: , Chen, Riquan, Chen, Tianshui, Hui, Xiaolu, Wu, Hefeng, Li, Guanbin, Lin, Liang</br>
  <font color="#00be00">Few-shot</font> learning aims to learn novel categories from very few samples given some base categories with sufficient training samples. The main challenge of this task is the novel categories are prone to dominated by color, texture, shape of the object or background context (namely specificity), which are distinct for the given few training samples but not common for the corresponding categories (see Figure 1). Fortunately, we find that transferring information of the correlated based categories can help learn the novel concepts and thus avoid the novel concept being dominated by the specificity. Besides, incorporating semantic correlations among different categories can effectively regularize this information transfer. In this work, we represent the semantic correlations in the form of structured <font color="#960096">knowledge graph</font> and integrate this graph into deep neural networks to promote few-shot learning by a novel Knowledge Graph Transfer Network (KGTN). Specifically, by initializing each node with the classifier weight of the corresponding category, a propagation mechanism is learned to adaptively propagate node message through the graph to explore node interaction and transfer classifier information of the base categories to those of the novel ones. Extensive experiments on the ImageNet dataset show significant performance improvement compared with current leading competitors. Furthermore, we construct an ImageNet-6K dataset that covers larger scale categories, i.e, 6,000 categories, and experiments on this dataset further demonstrate the effectiveness of our proposed model. </br></br>

<a href='http://arxiv.org/pdf/1911.07100.pdf'>1911.07100</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -0.1156баллов, №525</br>
<b>Defending Against Model Stealing Attacks with Adaptive Misinformation</b></br>
Authors: , Kariyappa, Sanjay, Qureshi, Moinuddin K</br>
  Deep Neural Networks (DNNs) are susceptible to model stealing attacks, which allows a data-limited adversary with no knowledge of the training dataset to clone the functionality of a target model, just by using black-box query access. Such attacks are typically carried out by querying the target model using inputs that are synthetically generated or sampled from a surrogate dataset to construct a labeled dataset. The adversary can use this labeled dataset to train a clone model, which achieves a classification accuracy comparable to that of the target model. We propose &quot;Adaptive Misinformation&quot; to defend against such model stealing attacks. We identify that all existing model stealing attacks invariably query the target model with Out-Of-Distribution (OOD) inputs. By selectively sending incorrect predictions for OOD queries, our defense substantially degrades the accuracy of the attacker\'s clone model (by up to 40%), while minimally impacting the accuracy (&lt;0.5%) for benign users. Compared to existing defenses, our defense has a significantly better security vs accuracy trade-off and incurs minimal computational overhead. </br></br>

<a href='http://arxiv.org/pdf/1911.05774.pdf'>1911.05774</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1165баллов, №526</br>
<b>Factor Group-Sparse Regularization for Efficient Low-Rank Matrix\n  Recovery</b></br>
Authors: , Fan, Jicong, Ding, Lijun, Chen, Yudong, Udell, Madeleine</br>
  This paper develops a new class of nonconvex regularizers for low-rank matrix recovery. Many regularizers are motivated as convex relaxations of the matrix rank function. Our new factor group-sparse regularizers are motivated as a relaxation of the number of nonzero columns in a factorization of the matrix. These nonconvex regularizers are sharper than the nuclear norm; indeed, we show they are related to Schatten-$p$ norms with arbitrarily small $0 &lt; p \\leq 1$. Moreover, these factor group-sparse regularizers can be written in a factored form that enables efficient and effective nonconvex optimization; notably, the method does not use singular value decomposition. We provide generalization error bounds for low-rank matrix completion which show improved upper bounds for Schatten-$p$ norm reglarization as $p$ decreases. Compared to the max norm and the factored formulation of the nuclear norm, factor group-sparse regularizers are more efficient, accurate, and robust to the initial guess of rank. Experiments show promising performance of factor group-sparse regularization for low-rank matrix completion and robust principal component analysis. </br></br>

<a href='http://arxiv.org/pdf/1911.06996.pdf'>1911.06996</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1168баллов, №527</br>
<b>Selective sampling for accelerating training of deep neural networks</b></br>
Authors: , Weinstein, Berry, Fine, Shai, Hel-Or, Yacov</br>
  We present a selective sampling method designed to accelerate the training of deep neural networks. To this end, we introduce a novel measurement, the minimal margin score (MMS), which measures the minimal amount of displacement an input should take until its predicted classification is switched. For multi-class linear classification, the MMS measure is a natural generalization of the margin-based selection criterion, which was thoroughly studied in the binary classification setting. In addition, the MMS measure provides an interesting insight into the progress of the training process and can be useful for designing and monitoring new training regimes. Empirically we demonstrate a substantial acceleration when training commonly used deep neural network architectures for popular image classification tasks. The efficiency of our method is compared against the standard training procedures, and against commonly used selective sampling alternatives: Hard negative mining selection, and Entropy-based selection. Finally, we demonstrate an additional speedup when we adopt a more aggressive learning drop regime while using the MMS selective sampling method. </br></br>

<a href='http://arxiv.org/pdf/1911.09445.pdf'>1911.09445</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1215баллов, №528</br>
<b>Approximated Orthonormal Normalisation in Training Neural Networks</b></br>
Authors: , Zhang, Guoqiang, Niwa, Kenta, Kleijn, W. B.</br>
  Generalisation of a deep neural network (DNN) is one major concern when employing the deep learning approach for solving practical problems. In this paper we propose a new technique, named approximated orthonormal normalisation (AON), to improve the generalisation capacity of a DNN model. Considering a weight matrix W from a particular neural layer in the model, our objective is to design a function h(W) such that its row vectors are approximately orthogonal to each other while allowing the DNN model to fit the training data sufficiently accurate. By doing so, it would avoid co-adaptation among neurons of the same layer to be able to improve network-generalisation capacity. Specifically, at each iteration, we first approximate (WW^T)^(-1/2) using its Taylor expansion before multiplying the matrix W. After that, the matrix product is then normalised by applying the spectral normalisation (SN) technique to obtain h(W). Conceptually speaking, AON is designed to turn orthonormal regularisation into orthonormal normalisation to avoid manual balancing the original and penalty functions. Experimental results show that AON yields promising validation performance compared to orthonormal regularisation. </br></br>

<a href='http://arxiv.org/pdf/1911.04654.pdf'>1911.04654</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.1242баллов, №529</br>
<b>Norm-Explicit Quantization: Improving Vector Quantization for Maximum\n  Inner Product Search</b></br>
Authors: , Dai, Xinyan, Yan, Xiao, Ng, Kelvin K. W., Liu, Jie, Cheng, James</br>
  Vector quantization (VQ) techniques are widely used in similarity search for data compression, fast metric computation and etc. Originally designed for Euclidean distance, existing VQ techniques (e.g., PQ, AQ) explicitly or implicitly minimize the quantization error. In this paper, we present a new angle to analyze the quantization error, which decomposes the quantization error into norm error and direction error. We show that quantization errors in norm have much higher influence on inner products than quantization errors in direction, and small quantization error does not necessarily lead to good performance in maximum inner product search (MIPS). Based on this observation, we propose norm-explicit quantization (NEQ) --- a general paradigm that improves existing VQ techniques for MIPS. NEQ quantizes the norms of items in a dataset explicitly to reduce errors in norm, which is crucial for MIPS. For the direction vectors, NEQ can simply reuse an existing VQ technique to quantize them without modification. We conducted extensive experiments on a variety of datasets and parameter configurations. The experimental results show that NEQ improves the performance of various VQ techniques for MIPS, including PQ, OPQ, RQ and AQ. </br></br>

<a href='http://arxiv.org/pdf/1911.07630.pdf'>1911.07630</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1243баллов, №530</br>
<b>Value-Added Chemical Discovery Using Reinforcement Learning</b></br>
Authors: , Jiang, Peihong, Doan, Hieu, Madireddy, Sandeep, Assary, Rajeev Surendran, Balaprakash, Prasanna</br>
  Computer-assisted synthesis planning aims to help chemists find better reaction pathways faster. Finding viable and short pathways from sugar <font color="#be00be">molecule</font>s to value-added chemicals can be modeled as a retrosynthesis planning problem with a catalyst allowed. This is a crucial step in efficient biomass conversion. The traditional computational chemistry approach to identifying possible reaction pathways involves computing the reaction energies of hundreds of intermediates, which is a critical bottleneck in silico reaction discovery. Deep <font color="#00be00">reinforcement learning</font> has shown in other domains that a well-trained agent with little or no prior human knowledge can surpass human performance. While some effort has been made to adapt machine learning techniques to the retrosynthesis planning problem, value-added chemical discovery presents unique challenges. Specifically, the reaction can occur in several different sites in a molecule, a subtle case that has never been treated in previous works. With a more versatile formulation of the problem as a Markov decision process, we address the problem using deep reinforcement learning techniques and present promising preliminary results. </br></br>

<a href='http://arxiv.org/pdf/1911.08153.pdf'>1911.08153</a> &nbsp&nbsp (cs:ML, cs:SD) &nbsp&nbsp -0.1244баллов, №531</br>
<b>Distributed Microphone Speech Enhancement based on Deep Learning</b></br>
Authors: , Wang, Syu-Siang, Liang, Yu-You, Hung, Jeih-weih, Tsao, Yu, Wang, Hsin-Min, Fang, Shih-Hau</br>
  Speech-related applications deliver inferior performance in complex noise environments. Therefore, this study primarily addresses this problem by introducing speech-enhancement (SE) systems based on deep neural networks (DNNs) applied to a distributed microphone architecture. The first system constructs a DNN model for each microphone to enhance the recorded noisy speech signal, and the second system combines all the noisy recordings into a large feature structure that is then enhanced through a DNN model. As for the third system, a channel-dependent DNN is first used to enhance the corresponding noisy input, and all the channel-wise enhanced outputs are fed into a DNN fusion model to construct a nearly clean signal. All the three DNN SE systems are operated in the acoustic frequency domain of speech signals in a diffuse-noise field environment. Evaluation experiments were conducted on the Taiwan Mandarin Hearing in Noise Test (TMHINT) database, and the results indicate that all the three DNN-based SE systems provide the original noise-corrupted signals with improved speech quality and intelligibility, whereas the third system delivers the highest signal-to-noise ratio (SNR) improvement and optimal speech intelligibility. </br></br>

<a href='http://arxiv.org/pdf/1911.07456.pdf'>1911.07456</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1247баллов, №532</br>
<b>Steady-State Control and Machine Learning of Large-Scale Deformable\n  Mirror Models</b></br>
Authors: , Haber, Aleksandar</br>
  We use Machine Learning (ML) and system identification validation approaches to estimate neural network models of large-scale Deformable Mirrors (DMs) used in Adaptive Optics (AO) systems. To obtain the training, validation, and test data sets, we simulate a realistic large-scale Finite Element (FE) model of a faceplate DM. The estimated models reproduce the input-output behavior of Vector AutoRegressive with eXogenous (VARX) input models and can be used for the design of high-performance AO systems. We address the model order selection and overfitting problems. We also provide an FE based approach for computing steady-state control signals that produce the desired wavefront shape. This approach can be used to predict the steady-state DM correction performance for different actuator spacings and configurations. The presented methods are tested on models with thousands of state variables and hundreds of actuators. The numerical simulations are performed on low-cost high-performance graphic processing units and implemented using the TensorFlow machine learning framework. The used codes are available online. The approaches presented in this paper are useful for the design and optimization of high-performance DMs and AO systems. </br></br>

<a href='http://arxiv.org/pdf/1911.08196.pdf'>1911.08196</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.1253баллов, №533</br>
<b>Defending with Shared Resources on a Network</b></br>
Authors: , Li, Minming, Tran-Thanh, Long, Wu, Xiaowei</br>
  In this paper we consider a defending problem on a network. In the model, the defender holds a total defending resource of R, which can be distributed to the nodes of the network. The defending resource allocated to a node can be shared by its neighbors. There is a weight associated with every edge that represents the efficiency defending resources are shared between neighboring nodes. We consider the setting when each attack can affect not only the target node, but its neighbors as well. Assuming that nodes in the network have different treasures to defend and different defending requirements, the defender aims at allocating the defending resource to the nodes to minimize the loss due to attack. We give polynomial time exact algorithms for two important special cases of the network defending problem. For the case when an attack can only affect the target node, we present an LP-based exact algorithm. For the case when defending resources cannot be shared, we present a max-flow-based exact algorithm. We show that the general problem is NP-hard, and we give a 2-approximation algorithm based on LP-rounding. Moreover, by giving a matching lower bound of 2 on the integrality gap on the LP relaxation, we show that our rounding is tight. </br></br>

<a href='http://arxiv.org/pdf/1911.08233.pdf'>1911.08233</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1257баллов, №534</br>
<b>Dual affine moment invariants</b></br>
Authors: , Hao, You, Mo, Hanlin, Li, Qi, Zhang, He, Li, Hua</br>
  Affine transformation is one of the most common transformations in nature, which is an important issue in the field of computer vision and shape analysis. And affine transformations often occur in both shape and color space simultaneously, which can be termed as Dual-Affine Transformation (DAT). In general, we should derive invariants of different data formats separately, such as 2D color images, 3D color objects, or even higher-dimensional data. To the best of our knowledge, there is no general framework to derive invariants for all of these data formats. In this paper, we propose a general framework to derive moment invariants under DAT for objects in M-dimensional space with N channels, which can be called dual-affine moment invariants (DAMI). Following this framework, we present the generating formula of DAMI under DAT for 3D color objects. Then, we instantiated a complete set of DAMI for 3D color objects with orders and degrees no greater than 4. Finally, we analyze the characteristic of these DAMI and conduct classification experiments to evaluate the stability and discriminability of them. The results prove that DAMI is robust for DAT. Our derivation framework can be applied to data in any dimension with any number of channels. </br></br>

<a href='http://arxiv.org/pdf/1910.09821.pdf'>1910.09821</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.1275баллов, №535</br>
<b>Structure Matters: Towards Generating Transferable Adversarial Images</b></br>
Authors: , Peng, Dan, Zheng, Zizhan, Luo, Linhao, Zhang, Xiaofeng</br>
  Recent works on adversarial examples for image classification focus on directly modifying pixels with minor perturbations. The small perturbation requirement is imposed to ensure the generated adversarial examples being natural and realistic to humans, which, however, puts a curb on the attack space thus limiting the attack ability and transferability especially for systems protected by a defense mechanism. In this paper, we propose the novel concepts of structure patterns and structure-aware perturbations that relax the small perturbation constraint while still keeping images natural. The key idea of our approach is to allow perceptible deviation in adversarial examples while keeping structure patterns that are central to a human classifier. Built upon these concepts, we propose a \\emph{structure-preserving attack (SPA)} for generating natural adversarial examples with extremely high transferability. Empirical results on the MNIST and the CIFAR10 datasets show that SPA exhibits strong attack ability in both the white-box and black-box setting even defenses are applied. Moreover, with the integration of PGD or CW attack, its attack ability escalates sharply under the white-box setting, without losing the outstanding transferability inherited from SPA. </br></br>

<a href='http://arxiv.org/pdf/1911.08040.pdf'>1911.08040</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.1280баллов, №536</br>
<b>Poison as a Cure: Detecting &amp; Neutralizing Variable-Sized Backdoor\n  Attacks in Deep Neural Networks</b></br>
Authors: , Chan, Alvin, Ong, Yew-Soon</br>
  Deep learning models have recently shown to be vulnerable to backdoor poisoning, an insidious attack where the victim model predicts clean images correctly but classifies the same images as the target class when a trigger poison pattern is added. This poison pattern can be embedded in the training dataset by the adversary. Existing defenses are effective under certain conditions such as a small size of the poison pattern, knowledge about the ratio of poisoned training samples or when a validated clean dataset is available. Since a defender may not have such prior knowledge or resources, we propose a defense against backdoor poisoning that is effective even when those prerequisites are not met. It is made up of several parts: one to extract a backdoor poison signal, detect poison target and base classes, and filter out poisoned from clean samples with proven guarantees. The final part of our defense involves retraining the poisoned model on a dataset augmented with the extracted poison signal and corrective relabeling of poisoned samples to neutralize the backdoor. Our approach has shown to be effective in defending against backdoor attacks that use both small and large-sized poison patterns on nine different target-base class pairs from the CIFAR10 dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.07023.pdf'>1911.07023</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.1283баллов, №537</br>
<b>Effectively Unbiased FID and Inception Score and where to find them</b></br>
Authors: , Chong, Min Jin, Forsyth, David</br>
  This paper shows that two commonly used evaluation metrics for generative models, the Fr\\\'echet Inception Distance (FID) and the Inception Score (IS), are biased -- the expected value of the score computed for a finite sample set is not the true value of the score. Worse, the paper shows that the bias term depends on the particular model being evaluated, so model A may get a better score than model B simply because model A\'s bias term is smaller. This effect cannot be fixed by evaluating at a fixed number of samples. This means all comparisons using FID or IS as currently computed are unreliable.   We then show how to extrapolate the score to obtain an effectively bias-free estimate of scores computed with an infinite number of samples, which we term $\\overline{\\textrm{FID}}_\\infty$ and $\\overline{\\textrm{IS}}_\\infty$. In turn, this effectively bias-free estimate requires good estimates of scores with a finite number of samples. We show that using Quasi-Monte Carlo integration notably improves estimates of FID and IS for finite sample sets. Our extrapolated scores are simple, drop-in replacements for the finite sample scores. Additionally, we show that using low discrepancy sequence in GAN training offers small improvements in the resulting generator. </br></br>

<a href='http://arxiv.org/pdf/1911.08009.pdf'>1911.08009</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.1286баллов, №538</br>
<b>Low Complexity Autoencoder based End-to-End Learning of Coded\n  Communications Systems</b></br>
Authors: , Rajapaksha, Nuwanthika, Rajatheva, Nandana, Latva-aho, Matti</br>
  End-to-end learning of a communications system using the deep learning-based autoencoder concept has drawn interest in recent research due to its simplicity, flexibility and its potential of adapting to complex channel models and practical system imperfections. In this paper, we have compared the bit error rate (BER) performance of autoencoder based systems and conventional channel coded systems with convolutional coding, in order to understand the potential of deep learning-based systems as alternatives to conventional systems. From the simulations, autoencoder implementation was observed to have a better BER in 0-5 dB $E_{b}/N_{0}$ range than its equivalent half-rate convolutional coded BPSK with hard decision decoding, and to have only less than 1 dB gap at a BER of $10^{-5}$. Furthermore, we have also proposed a novel low complexity autoencoder architecture to implement end-to-end learning of coded systems in which we have shown better BER performance than the baseline implementation. The newly proposed low complexity autoencoder was capable of achieving a better BER performance than half-rate 16-QAM with hard decision decoding over the full 0-10 dB $E_{b}/N_{0}$ range and a better BER performance than the soft decision decoding in 0-4 dB $E_{b}/N_{0}$ range. </br></br>

<a href='http://arxiv.org/pdf/1911.08053.pdf'>1911.08053</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1287баллов, №539</br>
<b>A novel method for identifying the deep neural network model with the\n  Serial Number</b></br>
Authors: , Xu, XiangRui, Li, YaQin, Yuan, Cao</br>
  Deep neural network (DNN) with the state of art performance has emerged as a viable and lucrative business service. However, those impressive performances require a large number of computational resources, which comes at a high cost for the model creators. The necessity for protecting DNN models from illegal reproducing and distribution appears salient now. Recently, trigger-set watermarking, breaking the white-box restriction, relying on adversarial training pre-defined (incorrect) labels for crafted inputs, and subsequently using them to verify the model authenticity, has been the main topic of DNN ownership verification. While these methods have successfully demonstrated robustness against removal attacks, few are effective against the tampering attacks from competitors forging the fake watermarks and dogging in the manager. In this paper, we put forth a new framework of the trigger-set watermark by embedding a unique Serial Number (relatedness less original labels) to the deep neural network for model ownership identification, which is both robust to model pruning and resist to tampering attacks. Experiment results demonstrate that the DNN Serial Number only incurs slight accuracy degradation of the original performance and is valid for ownership verification. </br></br>

<a href='http://arxiv.org/pdf/1911.07179.pdf'>1911.07179</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.1287баллов, №540</br>
<b>NeckSense: A Multi-Sensor Necklace for Detecting Eating Activities in\n  Free-Living Conditions</b></br>
Authors: , Zhang, Shibo, Zhao, Yuqi, Nguyen, Dzung Tri, Xu, Runsheng, Sen, Sougata, Hester, Josiah, Alshurafa, Nabil</br>
  We present the design, implementation, and evaluation of a multi-sensor low-power necklace \'NeckSense\' for automatically and unobtrusively capturing fine-grained information about an individual\'s eating activity and eating episodes, across an entire waking-day in a naturalistic setting. The NeckSense fuses and classifies the proximity of the necklace from the chin, the ambient light, the Lean Forward Angle, and the energy signals to determine chewing sequences, a building block of the eating activity. It then clusters the identified chewing sequences to determine eating episodes. We tested NeckSense with 11 obese and 9 non-obese participants across two studies, where we collected more than 470 hours of data in naturalistic setting. Our result demonstrates that NeckSense enables reliable eating-detection for an entire waking-day, even in free-living environments. Overall, our system achieves an F1-score of 81.6% in detecting eating episodes in an exploratory study. Moreover, our system can achieve a F1-score of 77.1% for episodes even in an all-day-around free-living setting. With more than 15.8 hours of battery-life NeckSense will allow researchers and dietitians to better understand natural chewing and eating behaviors, and also enable real-time interventions. </br></br>

<a href='http://arxiv.org/pdf/1911.09518.pdf'>1911.09518</a> &nbsp&nbsp (cs:ML, cs:CV) &nbsp&nbsp -0.1313баллов, №541</br>
<b>Video Segment Copy Detection Using Memory Constrained Hierarchical\n  Batch-Normalized LSTM Autoencoder</b></br>
Authors: , Krishna, Arjun, Ibrahim, A S Akil Arif</br>
  In this report, we introduce a video hashing method for scalable video segment copy detection. The objective of video segment copy detection is to find the video (s) present in a large database, one of whose segments (cropped in time) is a (transformed) copy of the given query video. This transformation may be temporal (for example frame dropping, change in frame rate) or spatial (brightness and contrast change, addition of noise etc.) in nature although the primary focus of this report is detecting temporal attacks. The video hashing method proposed by us uses a deep learning neural network to learn variable length binary hash codes for the entire video considering both temporal and spatial features into account. This is in contrast to most existing video hashing methods, as they use conventional image hashing techniques to obtain hash codes for a video after extracting features for every frame or certain key frames, in which case the temporal information present in the video is not exploited. Our hashing method is specifically resilient to time cropping making it extremely useful in video segment copy detection. Experimental results obtained on the large augmented dataset consisting of around 25,000 videos with segment copies demonstrate the efficacy of our proposed video hashing method. </br></br>

<a href='http://arxiv.org/pdf/1911.08815.pdf'>1911.08815</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1315баллов, №542</br>
<b>Object-based multi-temporal and multi-source land cover mapping\n  leveraging hierarchical class relationships</b></br>
Authors: , Gbodjo, Yawogan Jean Eudes, Ienco, Dino, Leroux, Louise, Interdonato, Roberto, Gaetano, Raffaele, Ndao, Babacar, Dupuy, Stephane</br>
  European satellite missions Sentinel-1 (S1) and Sentinel-2 (S2) provide at highspatial resolution and high revisit time, respectively, radar and optical imagesthat support a wide range of Earth surface monitoring tasks such as LandUse/Land Cover mapping. A long-standing challenge in the remote sensingcommunity is about how to efficiently exploit multiple sources of information and leverage their complementary. In this particular case, get the most out ofradar and optical satellite image time series (SITS). Here, we propose to dealwith land cover mapping through a deep learning framework especially tailoredto leverage the multi-source complementarity provided by radar and opticalSITS. The proposed architecture is based on an extension of Recurrent NeuralNetwork (RNN) enriched via a customized attention mechanism capable to fitthe specificity of SITS data. In addition, we propose a new pretraining strategythat exploits domain expert knowledge to guide the model parameter initial-ization. Thorough experimental evaluations involving several machine learningcompetitors, on two contrasted study sites, have demonstrated the suitabilityof our new attention mechanism combined with the extend RNN model as wellas the benefit/limit to inject domain expert knowledge in the neural networktraining process. </br></br>

<a href='http://arxiv.org/pdf/1911.08603.pdf'>1911.08603</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp -0.1317баллов, №543</br>
<b>Forbidden knowledge in machine learning -- Reflections on the limits of\n  research and publication</b></br>
Authors: , Hagendorff, Thilo</br>
  Certain research strands can yield &quot;forbidden knowledge&quot;. This term refers to knowledge that is considered too sensitive, dangerous or taboo to be produced or shared. Discourses about such publication restrictions are already entrenched in scientific fields like IT security, synthetic biology or nuclear physics research. This paper makes the case for transferring this discourse to machine learning research. Some machine learning applications can very easily be misused and unfold harmful consequences, for instance with regard to generative video or text synthesis, personality analysis, behavior manipulation, software vulnerability detection and the like. Up to now, the machine learning research community embraces the idea of open access. However, this is opposed to precautionary efforts to prevent the malicious use of machine learning applications. Information about or from such applications may, if improperly disclosed, cause harm to people, organizations or whole societies. Hence, the goal of this work is to outline norms that can help to decide whether and when the dissemination of such information should be prevented. It proposes review parameters for the machine learning community to establish an ethical framework on how to deal with forbidden knowledge and dual-use applications. </br></br>

<a href='http://arxiv.org/pdf/1911.07566.pdf'>1911.07566</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1333баллов, №544</br>
<b>Automated fetal brain extraction from clinical Ultrasound volumes using\n  3D Convolutional Neural Networks</b></br>
Authors: , Moser, Felipe, Huang, Ruobing, Papageorghiou, Aris T., Papiez, Bartlomiej W., Namburete, Ana I. L.</br>
  To improve the performance of most neuroimiage analysis pipelines, <font color="#00be00">brain</font> extraction is used as a fundamental first step in the image processing. But in the case of fetal brain development, there is a need for a reliable US-specific tool. In this work we propose a fully automated 3D CNN approach to fetal brain extraction from 3D US <font color="blue">clinic</font>al volumes with minimal preprocessing. Our method accurately and reliably extracts the brain regardless of the large data variation inherent in this imaging modality. It also performs consistently throughout a gestational age range between 14 and 31 weeks, regardless of the pose variation of the subject, the scale, and even partial feature-obstruction in the image, <font color="#00be00">outperform</font>ing all current alternatives. </br></br>

<a href='http://arxiv.org/pdf/1911.09071.pdf'>1911.09071</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.1335баллов, №545</br>
<b>Exploring the Origins and Prevalence of Texture Bias in Convolutional\n  Neural Networks</b></br>
Authors: , Hermann, Katherine L., Kornblith, Simon</br>
  Recent work has indicated that, unlike humans, ImageNet-trained CNNs tend to classify images by texture rather than shape. How pervasive is this bias, and where does it come from? We find that, when trained on datasets of images with conflicting shape and texture, the inductive bias of CNNs often favors shape; in general, models learn shape at least as easily as texture. Moreover, although ImageNet training leads to classifier weights that classify ambiguous images according to texture, shape is decodable from the hidden representations of ImageNet networks. Turning to the question of the origin of texture bias, we identify consistent effects of task, architecture, preprocessing, and hyperparameters. Different self-supervised training objectives and different architectures have significant and largely independent effects on the shape bias of the learned representations. Among modern ImageNet architectures, we find that shape bias is positively correlated with ImageNet accuracy. Random-crop data augmentation encourages reliance on texture: Models trained without crops have lower accuracy but higher shape bias. Finally, hyperparameter combinations that yield similar accuracy are associated with vastly different levels of shape bias. Our results suggest general strategies to reduce texture bias in neural networks. </br></br>

<a href='http://arxiv.org/pdf/1911.09032.pdf'>1911.09032</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp -0.1354баллов, №546</br>
<b>Outside the Box: Abstraction-Based Monitoring of Neural Networks</b></br>
Authors: , Henzinger, Thomas A., Lukina, Anna, Schilling, Christian</br>
  Neural networks have demonstrated unmatched performance in a range of classification tasks. Despite numerous efforts of the research community, novelty detection remains one of the significant limitations of neural networks. The ability to identify previously unseen inputs as novel is crucial for our understanding of the decisions made by neural networks. At runtime, inputs not falling into any of the categories learned during training cannot be classified correctly by the neural network. Existing approaches treat the neural network as a black box and try to detect novel inputs based on the confidence of the output predictions. However, neural networks are not trained to reduce their confidence for novel inputs, which limits the effectiveness of these approaches. We propose a framework to monitor a neural network by observing the hidden layers. We employ a common abstraction from program analysis - boxes - to identify novel behaviors in the monitored layers, i.e., inputs that cause behaviors outside the box. For each neuron, the boxes range over the values seen in training. The framework is efficient and flexible to achieve a desired trade-off between raising false warnings and detecting novel inputs. We illustrate the performance and the robustness to variability in the unknown classes on popular image-classification benchmarks. </br></br>

<a href='http://arxiv.org/pdf/1911.07970.pdf'>1911.07970</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1354баллов, №547</br>
<b>Revealing Perceptible Backdoors, without the Training Set, via the\n  Maximum Achievable Misclassification Fraction Statistic</b></br>
Authors: , Xiang, Zhen, Miller, David J., Kesidis, George</br>
  Recently, a special type of data poisoning (DP) attack, known as a backdoor, was proposed. These attacks aimto have a classifier learn to classify to a target class whenever the backdoor pattern is present in a test sample. In thispaper, we address post-training detection of perceptible backdoor patterns in DNN image classifiers, wherein thedefender does not have access to the poisoned training set, but only to the trained classifier itself, as well as to clean(unpoisoned) examples from the classification domain. This problem is challenging since a perceptible backdoorpattern could be any seemingly innocuous object in a scene, and, without the poisoned training set, we have nohint about the actual backdoor pattern used during training. We identify two important properties of perceptiblebackdoor patterns, based upon which we propose a novel detector using the maximum achievable misclassificationfraction (MAMF) statistic. We detect whether the trained DNN has been backdoor-attacked and infer the sourceand target classes used for devising the attack. Our detector, with an easily chosen threshold, is evaluated on fivedatasets, five DNN structures and nine backdoor patterns, and shows strong detection capability. Coupled with animperceptible backdoor detector, our approach helps achieve detection for all evasive backdoors of interest. </br></br>

<a href='http://arxiv.org/pdf/1911.07954.pdf'>1911.07954</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.1359баллов, №548</br>
<b>ISP4ML: Understanding the Role of Image Signal Processing in Efficient\n  Deep Learning Vision Systems</b></br>
Authors: , Hansen, Patrick, Vilkin, Alexey, Khrustalev, Yury, Hanwell, David, Mattina, Matthew, Whatmough, Paul N.</br>
  Convolutional neural networks (CNNs) are now predominant components in a variety of computer vision (CV) systems. These systems typically include an image signal processor (ISP), even though the ISP is traditionally designed to produce images that look appealing to humans. In CV systems, it is not clear what the role of the ISP is, or if it is even required at all for accurate prediction. In this work, we investigate the efficacy of the ISP in CNN classification tasks, and outline the system-level trade-offs between prediction accuracy and computational cost. To do so, we build software models of a configurable ISP and an imaging sensor in order to train CNNs on ImageNet with a range of different ISP settings and functionality. Results on ImageNet show that an ISP improves accuracy by 4.6%-12.2% on MobileNet architectures of different widths. Results using ResNets demonstrate that these trends also generalize to deeper networks. An ablation study of the various processing stages in a typical ISP reveals that the tone mapper is the most significant stage when operating on high dynamic range (HDR) images, by providing 5.8% average accuracy improvement alone. Overall, the ISP benefits system efficiency because the memory and computational costs of the ISP is minimal compared to the cost of using a larger CNN to achieve the same accuracy. </br></br>

<a href='http://arxiv.org/pdf/1911.08731.pdf'>1911.08731</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1367баллов, №549</br>
<b>Distributionally Robust Neural Networks for Group Shifts: On the\n  Importance of Regularization for Worst-Case Generalization</b></br>
Authors: , Sagawa, Shiori, Koh, Pang Wei, Hashimoto, Tatsunori B., Liang, Percy</br>
  Overparameterized neural networks can be highly accurate on average on an i.i.d. test set yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that hold on average but not in such groups). Distributionally robust optimization (DRO) allows us to learn models that instead minimize the worst-case training loss over a set of pre-defined groups. However, we find that naively applying group DRO to overparameterized neural networks fails: these models can perfectly fit the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, their poor worst-case performance arises from poor generalization on some groups. By coupling group DRO models with increased regularization---stronger-than-typical $\\ell_2$ regularization or early stopping---we achieve substantially higher worst-group accuracies, with 10-40 percentage point improvements on a natural language inference task and two image tasks, while maintaining high average accuracies. Our results suggest that regularization is critical for worst-group generalization in the overparameterized regime, even if it is not needed for average generalization. Finally, we introduce and give convergence guarantees for a stochastic optimizer for the group DRO setting, underpinning the empirical study above. </br></br>

<a href='http://arxiv.org/pdf/1911.09435.pdf'>1911.09435</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1370баллов, №550</br>
<b>TEINet: Towards an Efficient Architecture for Video Recognition</b></br>
Authors: , Liu, Zhaoyang, Luo, Donghao, Wang, Yabiao, Wang, Limin, Tai, Ying, Wang, Chengjie, Li, Jilin, Huang, Feiyue, Lu, Tong</br>
  Efficiency is an important issue in designing video architectures for action recognition. 3D CNNs have witnessed remarkable progress in action recognition from videos. However, compared with their 2D counterparts, 3D convolutions often introduce a large amount of parameters and cause high computational cost. To relieve this problem, we propose an efficient temporal module, termed as Temporal Enhancement-and-Interaction (TEI Module), which could be plugged into the existing 2D CNNs (denoted by TEINet). The TEI module presents a different paradigm to learn temporal features by decoupling the modeling of channel correlation and temporal interaction. First, it contains a Motion Enhanced Module (MEM) which is to enhance the motion-related features while suppress irrelevant information (e.g., background). Then, it introduces a Temporal Interaction Module (TIM) which supplements the temporal contextual information in a channel-wise manner. This two-stage modeling scheme is not only able to capture temporal structure flexibly and effectively, but also efficient for model inference. We conduct extensive experiments to verify the effectiveness of TEINet on several benchmarks (e.g., Something-Something V1&amp;V2, Kinetics, UCF101 and HMDB51). Our proposed TEINet can achieve a good recognition accuracy on these datasets but still preserve a high efficiency. </br></br>

<a href='http://arxiv.org/pdf/1911.07738.pdf'>1911.07738</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1376баллов, №551</br>
<b>Profile-based Resource Allocation for Virtualized Network Functions</b></br>
Authors: , Van Rossem, Steven, Tavernier, Wouter, Colle, Didier, Pickavet, Mario, Demeester, Piet</br>
  The virtualization of compute and network resources enables an unseen flexibility for deploying network services. A wide spectrum of emerging technologies allows an ever-growing range of orchestration possibilities in cloud-based environments. But in this context it remains challenging to rhyme dynamic cloud configurations with deterministic performance. The service operator must somehow map the performance specification in the Service Level Agreement (SLA) to an adequate resource allocation in the virtualized infrastructure. We propose the use of a VNF profile to alleviate this process. This is illustrated by profiling the performance of four example network functions (a virtual router, switch, firewall and cache server) under varying workloads and resource configurations. We then compare several methods to derive a model from the profiled datasets. We select the most accurate method to further train a model which predicts the services\' performance, in function of incoming workload and allocated resources. Our presented method can offer the service operator a recommended resource allocation for the targeted service, in function of the targeted performance and maximum workload specified in the SLA. This helps to deploy the softwarized service with an optimal amount of resources to meet the SLA requirements, thereby avoiding unnecessary scaling steps. </br></br>

<a href='http://arxiv.org/pdf/1911.06837.pdf'>1911.06837</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1385баллов, №552</br>
<b>Dynamic Modeling and Equilibria in Fair Decision Making</b></br>
Authors: , Williams, Joshua, Kolter, J. Zico</br>
  Recent studies on fairness in automated decision making systems have both investigated the potential future impact of these decisions on the population at large, and emphasized that imposing \'\'typical\'\' fairness constraints such as demographic parity or equality of opportunity does not guarantee a benefit to disadvantaged groups. However, these previous studies have focused on either simple one-step cost/benefit criteria, or on discrete underlying state spaces. In this work, we first propose a natural continuous representation of population state, governed by the Beta distribution, using a loan granting setting as a running example. Next, we apply a model of population dynamics under lending decisions, and show that when conditional payback probabilities are estimated correctly 1) ``optimal\'\' behavior by lenders can lead to \'\'Matthew Effect\'\' bifurcations (i.e., \'\'the rich get richer and the poor get poorer\'\'), but that 2) many common fairness constraints on the allowable policies cause groups to converge to the same equilibrium point. Last, we contrast our results in the case of misspecified conditional probability estimates with prior work, and show that for this model, different levels of group misestimation guarantees that even fair policies lead to bifurcations. We illustrate some of the modeling conclusions on real data from credit scoring. </br></br>

<a href='http://arxiv.org/pdf/1911.09188.pdf'>1911.09188</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1397баллов, №553</br>
<b>Localized Compression: Applying Convolutional Neural Networks to\n  Compressed Images</b></br>
Authors: , George, Christopher A., West, Bradley M.</br>
  We address the challenge of applying existing convolutional neural network (CNN) architectures to compressed images. Existing CNN architectures represent images as a matrix of pixel intensities with a specified dimension; this desired dimension is achieved by downgrading or cropping. Downgrading and cropping are attractive in that the result is also an image; however, an algorithm producing an alternative &quot;compressed&quot; representation could yield better classification performance. This compression algorithm need not be reversible, but must be compatible with the CNN\'s operations. This problem is thus the counterpart of the well-studied problem of applying compressed CNNs to uncompressed images, which has attracted great interest as CNNs are deployed to size-, weight-, and power- (SWaP)-limited devices. We introduce Localized Compression, a generalization of downgrading in which the original image is divided into blocks and each block is compressed to a smaller size using either sampling- or random-matrix-based techniques. By aligning the size of the compressed blocks with the size of the CNN\'s convolutional region, localized compression can be made compatible with any CNN architecture. Our experimental results show that Localized Compression results in classification accuracy approximately 1-2% higher than is achieved by downgrading to the equivalent resolution. </br></br>

<a href='http://arxiv.org/pdf/1911.08160.pdf'>1911.08160</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.1401баллов, №554</br>
<b>Deep interval prediction model with gradient descend optimization method\n  for short-term wind power prediction</b></br>
Authors: , Li, Chaoshun, Tang, Geng, Xue, Xiaoming, Chen, Xinbiao, Wang, Ruoheng, Zhang, Chu</br>
  The application of wind power interval prediction for power systems attempts to give more comprehensive support to dispatchers and operators of the grid. Lower upper bound estimation (LUBE) method is widely applied in interval prediction. However, the existing LUBE approaches are trained by meta-heuristic optimization, which is either time-consuming or show poor effect when the LUBE model is complex. In this paper, a deep interval prediction method is designed in the framework of LUBE and an efficient gradient descend (GD) training approach is proposed to train the LUBE model. In this method, the long short-term memory is selected as a representative to show the modelling approach. The architecture of the proposed model consists of three parts, namely the long short-term memory module, the fully connected layers and the rank ordered module. Two loss functions are specially designed for implementing the GD training method based on the root mean square back propagation algorithm. To verify the performance of the proposed model, conventional LUBE models, as well as popular statistic interval prediction models are compared in numerical experiments. The results show that the proposed approach performs best in terms of effectiveness and efficiency with average 45% promotion in quality of prediction interval and 66% reduction of time consumptions compared to traditional LUBE models. </br></br>

<a href='http://arxiv.org/pdf/1910.02390.pdf'>1910.02390</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1418баллов, №555</br>
<b>Migration through Machine Learning Lens -- Predicting Sexual and\n  Reproductive Health Vulnerability of Young Migrants</b></br>
Authors: , Nigam, Amber, Jaiswal, Pragati, Girkar, Uma, Arora, Teertha, Celi, Leo A.</br>
  In this paper, we have discussed initial findings and results of our experiment to predict sexual and reproductive health vulnerabilities of migrants in a data-constrained environment. Notwithstanding the limited research and data about migrants and migration cities, we propose a solution that simultaneously focuses on data gathering from migrants, augmenting awareness of the migrants to reduce mishaps, and setting up a mechanism to present insights to the key stakeholders in migration to act upon. We have designed a webapp for the stakeholders involved in migration: migrants, who would participate in data gathering process and can also use the app for getting to know safety and awareness tips based on analysis of the data received; public health workers, who would have an access to the database of migrants on the app; policy makers, who would have a greater understanding of the ground reality, and of the patterns of migration through machine-learned analysis. Finally, we have experimented with different machine learning models on an artificially curated dataset. We have shown, through experiments, how machine learning can assist in predicting the migrants at risk and can also help in identifying the critical factors that make migration dangerous for migrants. The results for identifying vulnerable migrants through machine learning algorithms are statistically significant at an alpha of 0.05. </br></br>

<a href='http://arxiv.org/pdf/1911.04013.pdf'>1911.04013</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1422баллов, №556</br>
<b>Customized video filtering on YouTube</b></br>
Authors: , Anand, Vishal, Shukla, Ravi, Gupta, Ashwani, Kumar, Abhishek</br>
  Inappropriate and profane content on social media is exponentially increasing and big corporations are becoming more aware of the type of content on which they are advertising and how it may affect their brand reputation. But with a huge surge in content being posted online it becomes seemingly difficult to filter out related videos on which they can run their ads without compromising brand name. Advertising on youtube videos generates a huge amount of revenue for corporations. It becomes increasingly important for such corporations to advertise on only the videos that don\'t hurt the feelings, community or harmony of the audience at large. In this paper, we propose a system to identify inappropriate content on YouTube and leverage it to perform a first of its kind, large scale, quantitative characterization that reveals some of the risks of YouTube ads consumption on inappropriate videos. Customization of the architecture have also been included to serve different requirements of corporations. Our analysis reveals that YouTube is still plagued by such disturbing videos and its currently deployed countermeasures are ineffective in terms of detecting them in a timely manner. Our framework tries to fill this gap by providing a handy, add on solution to filter the videos and help corporations and companies to push ads on the platform without worrying about the content on which the ads are displayed. </br></br>

<a href='http://arxiv.org/pdf/1911.07086.pdf'>1911.07086</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.1438баллов, №557</br>
<b>Signed Input Regularization</b></br>
Authors: , Taghanaki, Saeid Asgari, Abhishek, Kumar, Hamarneh, Ghassan</br>
  Over-parameterized deep models usually over-fit to a given training distribution, which makes them sensitive to small changes and out-of-distribution samples at inference time, leading to low generalization performance. To this end, several model-based and randomized data-dependent regularization methods are applied, such as data augmentation, which prevent a model from memorizing the training distribution. Instead of the random transformation of the input images, we propose SIGN, a new regularization method, which modifies the input variables using a linear transformation by estimating each variable\'s contribution to the final prediction. Our proposed technique maps the input data to a new manifold where the less important variables are de-emphasized. To test the effectiveness of the proposed idea and compare it with other competing methods, we design several test scenarios, such as classification performance, uncertainty, out-of-distribution, and robustness analyses. We compare the methods using three different datasets and four models. We find that SIGN encourages more compact class representations, which results in the model\'s robustness to random corruptions and out-of-distribution samples while also simultaneously achieving superior performance on normal data compared to other competing methods. Our experiments also demonstrate the successful transferability of the SIGN samples from one model to another. </br></br>

<a href='http://arxiv.org/pdf/1911.07308.pdf'>1911.07308</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1458баллов, №558</br>
<b>Counterfactual Vision-and-Language Navigation via Adversarial Path\n  Sampling</b></br>
Authors: , Fu, Tsu-Jui, Wang, Xin, Peterson, Matthew, Grafton, Scott, Eckstein, Miguel, Wang, William Yang</br>
  Vision-and-Language Navigation (VLN) is a task where agents must decide how to move through a 3D environment to reach a goal by grounding natural language instructions to the visual surroundings. One of the problems of the VLN task is data scarcity since it is difficult to collect enough navigation paths with human-annotated instructions for interactive environments. In this paper, we explore the use of counterfactual thinking as a human-inspired data augmentation method that results in robust models. Counterfactual thinking is a concept that describes the human propensity to create possible alternatives to life events that have already occurred. We propose an adversarial-driven counterfactual reasoning model that can consider effective conditions instead of low-quality augmented data. In particular, we present a model-agnostic adversarial path sampler (APS) that learns to sample challenging paths that force the navigator to improve based on the navigation performance. APS also serves to do pre-exploration of unseen environments to strengthen the model\'s ability to generalize. We evaluate the influence of APS on the performance of different VLN baseline models using the room-to-room dataset (R2R). The results show that the adversarial training process with our proposed APS benefits VLN models under both seen and unseen environments. And the pre-exploration process can further gain additional improvements under unseen environments. </br></br>

<a href='http://arxiv.org/pdf/1911.07681.pdf'>1911.07681</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1486баллов, №559</br>
<b>GLMNet: Graph Learning-Matching Networks for Feature Matching</b></br>
Authors: , Jiang, Bo, Sun, Pengfei, Tang, Jin, Luo, Bin</br>
  Recently, graph convolutional networks (GCNs) have shown great potential for the task of graph matching. It can integrate graph node feature embedding, node-wise affinity learning and matching optimization together in a unified end-to-end model. One important aspect of graph matching is the construction of two matching graphs. However, the matching graphs we feed to existing graph convolutional matching networks are generally fixed and independent of graph matching, which thus are not guaranteed to be optimal for the graph matching task. Also, existing GCN matching method employs several general smoothing-based graph convolutional layers to generate graph node embeddings, in which extensive smoothing convolution operation may dilute the desired discriminatory information of graph nodes. To overcome these issues, we propose a novel Graph Learning-Matching Network (GLMNet) for graph matching problem. GLMNet has three main aspects. (1) It integrates graph learning into graph matching which thus adaptively learn a pair of optimal graphs that best serve graph matching task. (2) It further employs a Laplacian sharpening convolutional module to generate more discriminative node embeddings for graph matching. (3) A new constraint regularized loss is designed for GLMNet training which can encode the desired one-to-one matching constraints in matching optimization. Experiments on two benchmarks demonstrate the effectiveness of GLMNet and advantages of its main modules. </br></br>

<a href='http://arxiv.org/pdf/1911.09325.pdf'>1911.09325</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1493баллов, №560</br>
<b>Simultaneous Implementation Features Extraction and Recognition Using\n  C3D Network for WiFi-based Human Activity Recognition</b></br>
Authors: , Yafeng, Liu, Tian, Chen, Zhongyu, Liu, Lei, Zhang, Yanjun, Hu, Enjie, Ding</br>
  Human actions recognition has attracted more and more people\'s attention. Many technology have been developed to express human action\'s features, such as image, skeleton-based, and channel state information(CSI). Among them, on account of CSI\'s easy to be equipped and undemanding for light, and it has gained more and more attention in some special scene. However, the relationship between CSI signal and human actions is very complex, and some preliminary work must be done to make CSI features easy to understand for computer. Nowadays, many work departed CSI-based features\' action dealing into two parts. One part is for features extraction and dimension reduce, and the other part is for time series problems. Some of them even omitted one of the two part work. Therefore, the accuracies of current recognition systems are far from satisfactory. In this paper, we propose a new deep learning based approach, i.e. C3D network and C3D network with attention mechanism, for human actions recognition using CSI signals. This kind of network can make feature extraction from spatial convolution and temporal convolution simultaneously, and through this network the two part of CSI-based human actions recognition mentioned above can be realized at the same time. The entire algorithm structure is simplified. The experimental results show that our proposed C3D network is able to achieve the best recognition performance for all activities when compared with some benchmark approaches. </br></br>

<a href='http://arxiv.org/pdf/1911.08292.pdf'>1911.08292</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.1513баллов, №561</br>
<b>Fairness through Equality of Effort</b></br>
Authors: , Huang, Wen, Wu, Yongkai, Zhang, Lu, Wu, Xintao</br>
  Fair machine learning is receiving an increasing attention in machine learning fields. Researchers in fair learning have developed correlation or association-based measures such as demographic disparity, mistreatment disparity, calibration, causal-based measures such as total effect, direct and indirect discrimination, and counterfactual fairness, and fairness notions such as equality of opportunity and equal odds that consider both decisions in the training data and decisions made by predictive models. In this paper, we develop a new causal-based fairness notation, called equality of effort. Different from existing fairness notions which mainly focus on discovering the disparity of decisions between two groups of individuals, the proposed equality of effort notation helps answer questions like to what extend a legitimate variable should change to make a particular individual achieve a certain outcome level and addresses the concerns whether the efforts made to achieve the same outcome level for individuals from the protected group and that from the unprotected group are different. We develop algorithms for determining whether an individual or a group of individuals is discriminated in terms of equality of effort. We also develop an optimization-based method for removing discriminatory effects from the data if discrimination is detected. We conduct empirical evaluations to compare the equality of effort and existing fairness notion and show the effectiveness of our proposed algorithms. </br></br>

<a href='http://arxiv.org/pdf/1911.08538.pdf'>1911.08538</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1520баллов, №562</br>
<b>Heterogeneous Deep Graph Infomax</b></br>
Authors: , Ren, Yuxiang, Liu, Bo, Huang, Chao, Dai, Peng, Bo, Liefeng, Zhang, Jiawei</br>
  Graph representation learning is to learn universal node representations that preserve both node attributes and structural information. The derived node representations can be used to serve various downstream tasks, such as node classification and node <font color="#be00be">clustering</font>. When a graph is heterogeneous, the problem becomes more challenging than the homogeneous graph node learning problem. Inspired by the emerging information <font color="blue">theor</font>etic-based learning algorithm, in this paper we propose an unsupervised graph neural network Heterogeneous Deep Graph Infomax (HDGI) for heterogeneous graph representation learning. We use the meta-path structure to analyze the connections involving semantics in heterogeneous graphs and utilize graph convolution module and semantic-level attention mechanism to capture local representations. By maximizing local-global mutual information, HDGI effectively learns high-level node representations that can be utilized in downstream graph-related tasks. Experiment results show that HDGI remarkably <font color="#00be00">outperform</font>s <font color="red">state-of-the-art</font> unsupervised graph representation learning methods on both classification and clustering tasks. By feeding the learned representations into a parametric model, such as logistic <font color="#be00be">regression</font>, we even achieve comparable performance in node classification tasks when comparing with state-of-the-art supervised end-to-end GNN models. </br></br>

<a href='http://arxiv.org/pdf/1910.03230.pdf'>1910.03230</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.1537баллов, №563</br>
<b>Meta Module Network for Compositional Visual Reasoning</b></br>
Authors: , Chen, Wenhu, Gan, Zhe, Li, Linjie, Cheng, Yu, Wang, William, Liu, Jingjing</br>
  There are two main lines of research on visual reasoning: neural module network (NMN) with explicit multi-hop reasoning through handcrafted neural modules, and monolithic network with implicit reasoning in the latent feature space. The former excels in <font color="#be00be">interpret</font>ability and compositionality, while the latter usually achieves better performance due to model flexibility and parameter efficiency. In order to bridge the gap between the two and leverage the merits of both, we present Meta Module Network (MMN), a novel hybrid approach that can utilize a Meta Module to perform versatile functionalities, while preserving compositionality and interpretability through modularized design. The proposed model first parses an input question into a functional program through a Program Generator. Instead of handcrafting a task-specific network to represent each function similar to traditional NMN, we propose a Meta Module, which can read a recipe (function specifications) to dynamically instantiate the task-specific Instance Modules for compositional reasoning. To endow different instance modules with designated functionalities, we design a symbolic teacher which can execute against provided scene graphs to generate guidelines for the instantiated modules (student) to follow during training. Experiments conducted on the GQA benchmark demonstrates that MMN <font color="#00be00">outperform</font>s both NMN and monolithic network baselines, with good generalization ability to handle unseen functions. </br></br>

<a href='http://arxiv.org/pdf/1911.07349.pdf'>1911.07349</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.1565баллов, №564</br>
<b>Putting visual object recognition in context</b></br>
Authors: , Zhang, Mengmi, Tseng, Claire, Kreiman, Gabriel</br>
  Context plays an important role in visual recognition. Recent studies have shown that visual recognition networks can be fooled by placing objects in inconsistent contexts (e.g. a cow in the ocean). To understand and model the role of contextual information in visual recognition, we systematically and quantitatively investigated ten critical properties of where, when, and how context modulates recognition including amount of context, context and object resolution, geometrical structure of context, context congruence, time required to incorporate contextual information, and temporal dynamics of contextual modulation. The tasks involve recognizing a target object surrounded with context in a natural image. As an essential benchmark, we first describe a series of psychophysics experiments, where we alter one aspect of context at a time, and quantify human recognition accuracy. To computationally assess performance on the same tasks, we propose a biologically inspired context aware object recognition model consisting of a two-stream architecture. The model processes visual information at the fovea and periphery in parallel, dynamically incorporates both object and contextual information, and sequentially reasons about the class label for the target object. Across a wide range of behavioral tasks, the model approximates human level performance without retraining for each task, captures the dependence of context enhancement on image properties, and provides initial steps towards integrating scene and object information for visual recognition. </br></br>

<a href='http://arxiv.org/pdf/1911.08360.pdf'>1911.08360</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.1571баллов, №565</br>
<b>All-Pay Bidding Games on Graphs</b></br>
Authors: , Avni, Guy, Ibsen-Jensen, Rasmus, Tkadlec, Josef</br>
  In this paper we introduce and study {\\em all-pay bidding games}, a class of two player, zero-sum games on graphs. The game proceeds as follows. We place a token on some vertex in the graph and assign budgets to the two players. Each turn, each player submits a sealed legal bid (non-negative and below their remaining budget), which is deducted from their budget and the highest bidder moves the token onto an adjacent vertex. The game ends once a sink is reached, and \\PO pays \\PT the outcome that is associated with the sink. The players attempt to maximize their expected outcome. Our games model settings where effort (of no inherent value) needs to be invested in an ongoing and stateful manner. On the negative side, we show that even in simple games on DAGs, optimal strategies may require a distribution over bids with infinite support. A central quantity in bidding games is the {\\em ratio} of the players budgets. On the positive side, we show a simple FPTAS for DAGs, that, for each budget ratio, outputs an approximation for the optimal strategy for that ratio. We also implement it, show that it performs well, and suggests interesting properties of these games. Then, given an outcome $c$, we show an algorithm for finding the necessary and sufficient initial ratio for guaranteeing outcome $c$ with probability~$1$ and a strategy ensuring such. Finally, while the general case has not previously been studied, solving the specific game in which \\PO wins iff he wins the first two auctions, has been long stated as an open question, which we solve. </br></br>

<a href='http://arxiv.org/pdf/1911.09649.pdf'>1911.09649</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1575баллов, №566</br>
<b>Learning to Localize Sound Sources in Visual Scenes: Analysis and\n  Applications</b></br>
Authors: , Senocak, Arda, Oh, Tae-Hyun, Kim, Junsik, Yang, Ming-Hsuan, Kweon, In So</br>
  Visual events are usually accompanied by sounds in our daily lives. However, can the machines learn to correlate the visual scene and sound, as well as localize the sound source only by observing them like humans? To investigate its empirical learnability, in this work we first present a novel unsupervised algorithm to address the problem of localizing sound sources in visual scenes. In order to achieve this goal, a two-stream network structure which handles each modality with attention mechanism is developed for sound source localization. The network naturally reveals the localized response in the scene without human annotation. In addition, a new sound source dataset is developed for performance evaluation. Nevertheless, our empirical evaluation shows that the unsupervised method generates false conclusions in some cases. Thereby, we show that this false conclusion cannot be fixed without human prior knowledge due to the well-known correlation and causality mismatch misconception. To fix this issue, we extend our network to the supervised and semi-supervised network settings via a simple modification due to the general architecture of our two-stream network. We show that the false conclusions can be effectively corrected even with a small amount of supervision, i.e., semi-supervised setup. Furthermore, we present the versatility of the learned audio and visual embeddings on the cross-modal content alignment and we extend this proposed algorithm to a new application, sound saliency based automatic camera view panning in 360-degree{\\deg} videos. </br></br>

<a href='http://arxiv.org/pdf/1911.08936.pdf'>1911.08936</a> &nbsp&nbsp (cs:CL, cs:AI, cs:ML) &nbsp&nbsp -0.1682баллов, №567</br>
<b>Knowledge Graph Alignment Network with Gated Multi-hop Neighborhood\n  Aggregation</b></br>
Authors: , Sun, Zequn, Wang, Chengming, Hu, Wei, Chen, Muhao, Dai, Jian, Zhang, Wei, Qu, Yuzhong</br>
  Graph neural networks (GNNs) have emerged as a powerful paradigm for embedding-based entity alignment due to their capability of identifying isomorphic subgraphs. However, in real <font color="#960096">knowledge graph</font>s (KGs), the counterpart entities usually have non-isomorphic neighborhood structures, which easily causes GNNs to yield different representations for them. To tackle this problem, we propose a new KG alignment network, namely AliNet, aiming at mitigating the non-isomorphism of neighborhood structures in an end-to-end manner. As the direct neighbors of counterpart entities are usually dissimilar due to the schema heterogeneity, AliNet introduces distant neighbors to expand the overlap between their neighborhood structures. It employs an attention mechanism to highlight helpful distant neighbors and reduce noises. Then, it controls the aggregation of both direct and distant neighborhood information using a gating mechanism. We further propose a relation loss to refine entity representations. We perform thorough experiments with detailed ablation studies and analyses on five entity alignment datasets, demonstrating the effectiveness of AliNet. </br></br>

<a href='http://arxiv.org/pdf/1911.09512.pdf'>1911.09512</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1691баллов, №568</br>
<b>A Comparative Analysis of Forecasting Financial Time Series Using ARIMA,\n  LSTM, and BiLSTM</b></br>
Authors: , Siami-Namini, Sima, Tavakoli, Neda, Namin, Akbar Siami</br>
  Machine and deep learning-based algorithms are the emerging approaches in addressing prediction problems in time series. These techniques have been shown to produce more accurate results than conventional <font color="#be00be">regression</font>-based modeling. It has been reported that artificial Recurrent Neural Networks (RNN) with memory, such as Long Short-Term Memory (LSTM), are superior compared to Autoregressive Integrated Moving Average (ARIMA) with a large margin. The LSTM-based models incorporate additional &quot;gates&quot; for the purpose of memorizing longer sequences of input data. The major question is that whether the gates incorporated in the LSTM architecture already offers a good prediction and whether additional training of data would be necessary to further improve the prediction.   Bidirectional LSTMs (BiLSTMs) enable additional training by traversing the input data twice (i.e., 1) left-to-right, and 2) right-to-left). The research question of interest is then whether BiLSTM, with additional training capability, <font color="#00be00">outperform</font>s regular unidirectional LSTM. This paper reports a behavioral analysis and comparison of BiLSTM and LSTM models. The objective is to explore to what extend additional layers of training of data would be beneficial to tune the involved parameters. The results show that additional training of data and thus BiLSTM-based modeling offers better predictions than regular LSTM-based models. More specifically, it was observed that BiLSTM models provide better predictions compared to ARIMA and LSTM models. It was also observed that BiLSTM models reach the equilibrium much slower than LSTM-based models. </br></br>

<a href='http://arxiv.org/pdf/1911.09531.pdf'>1911.09531</a> &nbsp&nbsp (cs:ML, cs:AI) &nbsp&nbsp -0.1708баллов, №569</br>
<b>Towards FAIR protocols and workflows: The OpenPREDICT case study</b></br>
Authors: , Celebi, Remzi, Moreira, Joao Rebelo, Hassan, Ahmed A., Ayyar, Sandeep, Ridder, Lars, Kuhn, Tobias, Dumontier, Michel</br>
  It is essential for the advancement of science that scientists and researchers share, reuse and reproduce workflows and protocols used by others. The FAIR principles are a set of guidelines that aim to maximize the value and usefulness of research data, and emphasize a number of important points regarding the means by which digital objects are found and reused by others. The question of how to apply these principles not just to the static input and output data but also to the dynamic workflows and protocols that consume and produce them is still under debate and poses a number of challenges. In this paper we describe our inclusive and overarching approach to apply the FAIR principles to workflows and protocols and demonstrate its benefits. We apply and evaluate our approach on a case study that consists of making the PREDICT workflow, a highly cited drug repurposing workflow, open and FAIR. This includes FAIRification of the involved datasets, as well as applying semantic technologies to represent and store data about the detailed versions of the general protocol, of the concrete workflow instructions, and of their execution traces. A semantic model was proposed to better address these specific requirements and were evaluated by answering competency questions. This semantic model consists of classes and relations from a number of existing ontologies, including Workflow4ever, PROV, EDAM, and BPMN. This allowed us then to formulate and answer new kinds of competency questions. Our evaluation shows the high degree to which our FAIRified OpenPREDICT workflow now adheres to the FAIR principles and the practicality and usefulness of being able to answer our new competency questions. </br></br>

<a href='http://arxiv.org/pdf/1911.07656.pdf'>1911.07656</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1714баллов, №570</br>
<b>The Similarity-Consensus Regularized Multi-view Learning for Dimension\n  Reduction</b></br>
Authors: , Meng, Xiangzhu, Wang, Huibing, Feng, Lin</br>
  During the last decades, learning a low-dimensional space with discriminative information for dimension reduction (DR) has gained a surge of interest. However, it\'s not accessible for these DR methods to achieve satisfactory performance when facing the features from multiple views. In multi-view learning problems, one instance can be represented by multiple heterogeneous features, which are highly related but sometimes look different from each other. In addition, correlations between features from multiple views always vary greatly, which challenges the capability of multi-view learning methods. Consequently, constructing a multi-view learning framework with generalization and scalability, which could take advantage of multi-view information as much as possible, is extremely necessary but challenging. To implement the above target, this paper proposes a novel multi-view learning framework based on similarity consensus, which makes full use of correlations among multi-view features while considering the scalability and robustness of the framework. It aims to straightforwardly extend those existing DR methods into multi-view learning domain by preserving the similarity between different views to capture the low-dimensional embedding. Two schemes based on pairwise-consensus and centroid-consensus are separately proposed to force multiple views to learn from each other and then an iterative alternating strategy is developed to obtain the optimal solution. The proposed method is evaluated on 5 benchmark datasets and comprehensive experiments show that our proposed multi-view framework can yield comparable and promising performance with previous approaches proposed in recent literatures. </br></br>

<a href='http://arxiv.org/pdf/1911.06922.pdf'>1911.06922</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1722баллов, №571</br>
<b>Benanza: Automatic $\\mu$Benchmark Generation to Compute &quot;Lower-bound&quot;\n  Latency and Inform Optimizations of Deep Learning Models on GPUs</b></br>
Authors: , Li, Cheng, Dakkak, Abdul, Xiong, Jinjun, Hwu, Wen-mei</br>
  As Deep Learning (DL) models have been increasingly used in latency-sensitive applications, there has been a growing interest in improving their response time. An important venue for such improvement is to profile the execution of these models and characterize their performance to identify possible optimization opportunities. However, the current profiling tools lack the highly desired abilities to characterize ideal performance, identify sources of inefficiency, and quantify the benefits of potential optimizations. Such deficiencies have led to slow characterization/optimization cycles that cannot keep up with the fast pace at which new DL models are introduced.   We propose Benanza, a sustainable and extensible benchmarking and analysis design that speeds up the characterization/optimization cycle of DL models on GPUs. Benanza consists of four major components: a model processor that parses models into an internal representation, a configurable benchmark generator that automatically generates micro-benchmarks given a set of models, a database of benchmark results, and an analyzer that computes the &quot;lower-bound&quot; latency of DL models using the benchmark data and informs optimizations of model execution. The &quot;lower-bound&quot; latency metric estimates the ideal model execution on a GPU system and serves as the basis for identifying optimization opportunities in frameworks or system libraries. We used Benanza to evaluate 30 ONNX models in MXNet, ONNX Runtime, and PyTorch on 7 GPUs ranging from Kepler to the latest Turing, and identified optimizations in parallel layer execution, cuDNN convolution algorithm selection, framework inefficiency, layer fusion, and using Tensor Cores. </br></br>

<a href='http://arxiv.org/pdf/1911.09647.pdf'>1911.09647</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1730баллов, №572</br>
<b>Uniform error estimates for artificial neural network approximations for\n  heat equations</b></br>
Authors: , Gonon, Lukas, Grohs, Philipp, Jentzen, Arnulf, Kofler, David, &#x160;i&#x161;ka, David</br>
  Recently, artificial neural networks (ANNs) in conjunction with stochastic gradient descent optimization methods have been employed to approximately compute solutions of possibly rather high-dimensional partial differential equations (PDEs). Very recently, there have also been a number of rigorous mathematical results in the scientific literature which examine the approximation capabilities of such deep learning based approximation algorithms for PDEs. These mathematical results from the scientific literature prove in part that algorithms based on ANNs are capable of overcoming the curse of dimensionality in the numerical approximation of high-dimensional PDEs. In these mathematical results from the scientific literature usually the error between the solution of the PDE and the approximating ANN is measured in the $L^p$-sense with respect to some $p \\in [1,\\infty)$ and some probability measure. In many applications it is, however, also important to control the error in a uniform $L^\\infty$-sense. The key contribution of the main result of this article is to develop the techniques to obtain error estimates between solutions of PDEs and approximating ANNs in the uniform $L^\\infty$-sense. In particular, we prove that the number of parameters of an ANN to uniformly approximate the classical solution of the heat equation in a region $ [a,b]^d $ for a fixed time point $ T \\in (0,\\infty) $ grows at most polynomially in the dimension $ d \\in \\mathbb{N} $ and the reciprocal of the approximation precision $ \\varepsilon &gt; 0 $. This shows that ANNs can overcome the curse of dimensionality in the numerical approximation of the heat equation when the error is measured in the uniform $L^\\infty$-norm. </br></br>

<a href='http://arxiv.org/pdf/1911.08727.pdf'>1911.08727</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1742баллов, №573</br>
<b>Layer-wise Adaptive Gradient Sparsification for Distributed Deep\n  Learning with Convergence Guarantees</b></br>
Authors: , Shi, Shaohuai, Tang, Zhenheng, Wang, Qiang, Zhao, Kaiyong, Chu, Xiaowen</br>
  To reduce the long training time of large deep neural network (DNN) models, distributed synchronous stochastic gradient descent (S-SGD) is commonly used on a cluster of workers. However, the speedup brought by multiple workers is limited by the communication overhead. Two approaches, namely pipelining and gradient sparsification, have been separately proposed to alleviate the impact of communication overheads. Yet, the gradient sparsification methods can only initiate the communication after the backpropagation, and hence miss the pipelining opportunity. In this paper, we propose a new distributed optimization method named LAGS-SGD, which combines S-SGD with a novel layer-wise adaptive gradient sparsification (LAGS) scheme. In LAGS-SGD, every worker selects a small set of &quot;significant&quot; gradients from each layer independently whose size can be adaptive to the communication-to-computation ratio of that layer. The layer-wise nature of LAGS-SGD opens the opportunity of overlapping communications with computations, while the adaptive nature of LAGS-SGD makes it flexible to control the communication time. We prove that LAGS-SGD has convergence guarantees and it has the same order of convergence rate as vanilla S-SGD under a weak analytical assumption. Extensive experiments are conducted to verify the analytical assumption and the convergence performance of LAGS-SGD. Experimental results show that LAGS-SGD achieves from around 40\\% to 95\\% of the maximum benefit of pipelining on a 16-node GPU cluster. Combining the benefit of pipelining and sparsification, the speedup of LAGS-SGD over S-SGD ranges from 2.86$\\times$ to 8.52$\\times$ on our tested CNN and LSTM models, without losing obvious model accuracy. </br></br>

<a href='http://arxiv.org/pdf/1911.07302.pdf'>1911.07302</a> &nbsp&nbsp (cs:NE) &nbsp&nbsp -0.1767баллов, №574</br>
<b>Haploid-Diploid Evolution: Nature\'s Memetic Algorithm</b></br>
Authors: , Tsompanas, Michail-Antisthenis, Bull, Larry, Adamatzky, Andrew, Balaz, Igor</br>
  This paper uses a recent explanation for the fundamental haploid-diploid lifecycle of eukaryotic organisms to present a new memetic algorithm that differs from all previous known work using diploid representations. A form of the Baldwin effect has been identified as inherent to the evolutionary mechanisms of eukaryotes and a simplified version is presented here which maintains such behaviour. Using a well-known abstract tuneable model, it is shown that varying fitness landscape ruggedness varies the benefit of haploid-diploid algorithms. Moreover, the methodology is applied to optimise the targeted delivery of a therapeutic compound utilizing nano-particles to <font color="#be00be">cancer</font>ous tumour cells with the multicellular simulator PhysiCell. </br></br>

<a href='http://arxiv.org/pdf/1911.09275.pdf'>1911.09275</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.1798баллов, №575</br>
<b>Machine Learning-enhanced Realistic Framework for Real-time Seismic\n  Monitoring -- The Winning Solution of the 2017 International Aftershock\n  Detection Contest</b></br>
Authors: , Shen, Dazhong, Zhang, Qi, Xu, Tong, Zhu, Hengshu, Zhao, Wenjia, Yin, Zikai, Zhou, Peilun, Fang, Lihua, Chen, Enhong, Xiong, Hui</br>
  Identifying the arrival times of seismic P-phases plays a significant role in real-time seismic monitoring, which provides critical guidance for emergency response activities. While considerable research has been conducted on this topic, efficiently capturing the arrival times of seismic P-phases hidden within intensively distributed and noisy seismic waves, such as those generated by the aftershocks of destructive earthquakes, remains a real challenge since existing methods rely on laborious expert supervision. To this end, in this paper, we present a machine learning-enhanced framework, ML-Picker, for the automatic identification of seismic P-phase arrivals on continuous and massive waveforms. More specifically, ML-Picker consists of three modules, namely, Trigger, Classifier, and Refiner, and an ensemble learning strategy is exploited to integrate several machine learning classifiers. An evaluation of the aftershocks following the $M8.0$ Wenchuan earthquake demonstrates that ML-Picker can not only achieve the best identification performance but also identify 120% more seismic P-phase arrivals as complementary data. Meanwhile, experimental results also reveal both the applicability of different machine learning models for waveforms collected from different seismic stations and the regularities of seismic P-phase arrivals that might be neglected during manual inspection. These findings clearly validate the effectiveness, efficiency, flexibility and stability of ML-Picker. In particular, with the preliminary version of ML-Picker, we won the championship in the First Season and were the runner-up in the Finals of the 2017 International Aftershock Detection Contest hosted by the China Earthquake Administration, in which 1,143 teams participated from around the world. </br></br>

<a href='http://arxiv.org/pdf/1911.08915.pdf'>1911.08915</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.1819баллов, №576</br>
<b>Universal and non-universal text statistics: Clustering coefficient for\n  language identification</b></br>
Authors: , Espitia, Diego, Larralde, Hern&#xe1;n</br>
  In this work we analyze statistical properties of 91 relatively small texts in 7 different languages (Spanish, English, French, German, Turkish, <font color="#00be00">Russia</font>n, Icelandic) as well as texts with randomly inserted spaces. Despite the size (around 11260 different words), the well known universal statistical laws -- namely Zipf and Herdan-Heap\'s laws -- are confirmed, and are in close agreement with results obtained elsewhere. We also construct a word co-occurrence network of each text. While the degree distribution is again universal, we note that the distribution of <font color="#be00be">Clustering</font> Coefficients, which depend strongly on the local structure of networks, can be used to differentiate between languages, as well as to distinguish natural languages from random texts. </br></br>

<a href='http://arxiv.org/pdf/1911.07068.pdf'>1911.07068</a> &nbsp&nbsp (cs:AI, cs:CV, cs:ML) &nbsp&nbsp -0.1821баллов, №577</br>
<b>Sensory Optimization: Neural Networks as a Model for Understanding and\n  Creating Art</b></br>
Authors: , Evans, Owain</br>
  This article is about the cognitive science of visual art. Artists create physical artifacts (such as sculptures or paintings) which depict people, objects, and events. These depictions are usually stylized rather than photo-realistic. How is it that humans are able to understand and create stylized representations? Does this ability depend on general cognitive capacities or an evolutionary adaptation for art? What role is played by learning and culture?   Machine Learning can shed light on these questions. It\'s possible to train convolutional neural networks (CNNs) to recognize objects without training them on any visual art. If such CNNs can generalize to visual art (by creating and understanding stylized representations), then CNNs provide a model for how humans could understand art without innate adaptations or cultural learning. I argue that Deep Dream and <font color="#be00be">Style</font> Transfer show that CNNs can create a basic form of visual art, and that humans could create art by similar processes. This suggests that artists make art by optimizing for effects on the human object-recognition system. Physical artifacts are optimized to evoke <font color="#009600">real-world</font> objects for this system (e.g. to evoke people or landscapes) and to serve as superstimuli for this system. </br></br>

<a href='http://arxiv.org/pdf/1911.07931.pdf'>1911.07931</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1834баллов, №578</br>
<b>CAGFuzz: Coverage-Guided Adversarial Generative Fuzzing Testing of Deep\n  Learning Systems</b></br>
Authors: , Zhang, Pengcheng, Dai, Qiyin, Pelliccione, Patrizio</br>
  Deep Learning systems (DL) based on Deep Neural Networks (DNNs) are more and more used in various aspects of our life, including unmanned vehicles, speech processing, and robotics. However, due to the limited dataset and the dependence on manual labeling data, DNNs often fail to detect their erroneous behaviors, which may lead to serious problems. Several approaches have been proposed to enhance the input examples for testing DL systems. However, they have the following limitations. First, they design and generate adversarial examples from the perspective of model, which may cause low generalization ability when they are applied to other models. Second, they only use surface feature constraints to judge the difference between the adversarial example generated and the original example. The deep feature constraints, which contain high-level semantic information, such as image object category and scene semantics are completely neglected. To address these two problems, in this paper, we propose CAGFuzz, a Coverage-guided Adversarial Generative Fuzzing testing approach, which generates adversarial examples for a targeted DNN to discover its potential defects. First, we train an adversarial case generator (AEG) from the perspective of general data set. Second, we extract the depth features of the original and adversarial examples, and constrain the adversarial examples by cosine similarity to ensure that the semantic information of adversarial examples remains unchanged. Finally, we retrain effective adversarial examples to improve neuron testing coverage rate. Based on several popular data sets, we design a set of dedicated experiments to evaluate CAGFuzz. The experimental results show that CAGFuzz can improve the neuron coverage rate, detect hidden errors, and also improve the accuracy of the target DNN. </br></br>

<a href='http://arxiv.org/pdf/1911.06964.pdf'>1911.06964</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp -0.1849баллов, №579</br>
<b>Learning Autocomplete Systems as a Communication Game</b></br>
Authors: , Lee, Mina, Hashimoto, Tatsunori B., Liang, Percy</br>
  We study textual autocomplete---the task of predicting a full sentence from a partial sentence---as a human-machine communication game. Specifically, we consider three competing goals for effective communication: use as few tokens as possible (efficiency), transmit sentences faithfully (accuracy), and be learnable to humans (<font color="#be00be">interpret</font>ability). We propose an unsupervised approach which tackles all three desiderata by constraining the communication scheme to keywords extracted from a source sentence for interpretability and optimizing the efficiency-accuracy tradeoff. Our experiments show that this approach results in an autocomplete system that is 52% more accurate at a given efficiency level compared to baselines, is robust to user variations, and saves time by nearly 50% compared to typing full sentences. </br></br>

<a href='http://arxiv.org/pdf/1911.05589.pdf'>1911.05589</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.1861баллов, №580</br>
<b>MOTH- Mobility-induced Outages in THz: A Beyond 5G (B5G) application</b></br>
Authors: , Singh, Rohit, Sicker, Douglas, Huq, Kazi Mohammed Saidul</br>
  5G will enable the growing demand for Internet of Things (IoT), high-resolution video streaming, and low latency wireless services. Demand for such services is expected to growth rapid, which will require a search for Beyond 5G technological advancements in wireless communications. Part of these advancements is the need for additional spectrum, namely moving toward the terahertz (THz) range. To compensate for the high path loss in THz, narrow beamwidths are used to improve antenna gains. However, with narrow beamwidths, even minor fluctuations in device location (such as through body movement) can cause frequent link failures due to beam misalignment. In this paper, we provide a solution to these small-scale indoor movement that result in mobility-induced outages. Like a moth randomly flutters about, Mobility-induced Outages in THz (MOTH) can be ephemeral in nature and hard to avoid. To deal with MOTH we propose two methods to predict these outage scenarios: (i) Align-After-Failure (AAF), which predicts based on fixed time margins, and (ii) Align-Before-Failure (ABF), which learns the time margins through user mobility patterns. In this paper, two different online classifiers were used to train the ABF model to predicate if a mobility-induced outage is going to occur; thereby, significantly reducing the time spent in outage scenarios. Simulation results demonstrate a relationship between optimal beamwidth and human mobility patterns. Additionally, to cater to a future with dense deployment of Wireless Personal Area Network (WPAN), it is necessary that we have efficient deployment of resources (e.g., THz-APs). One solution is to maximize the user coverage for a single AP, which might be dependent on multiple parameters. We identify these parameters and observe their tradeoffs for improving user coverage through a single THz-AP. </br></br>

<a href='http://arxiv.org/pdf/1911.08002.pdf'>1911.08002</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.1882баллов, №581</br>
<b>Robot Calligraphy using Pseudospectral Optimal Control in Conjunction\n  with a Simulated Brush Model</b></br>
Authors: , Wang, Sen, Chen, Jiaqi, Deng, Xuanliang, Hutchinson, Seth, Dellaert, Frank</br>
  <font color="#be00be">Chinese</font> calligraphy is a unique form of art that has great artistic value but is difficult to master. In this paper, we make robots write calligraphy. Learning methods could teach robots to write, but may not be able to generalize to new characters. As such, we formulate the calligraphy writing problem as a trajectory optimization problem, and propose a new virtual brush model for simulating the real dynamic writing process. Our optimization approach is taken from pseudospectral optimal control, where the proposed dynamic virtual brush model plays a key role in formulating the objective function to be optimized. We also propose a stroke-level optimization to achieve better performance compared to the character-level optimization proposed in previous work. Our methodology shows good performance in drawing aesthetically pleasing characters. </br></br>

<a href='http://arxiv.org/pdf/1911.08762.pdf'>1911.08762</a> &nbsp&nbsp (cs:CL, cs:AI) &nbsp&nbsp -0.1904баллов, №582</br>
<b>Paraphrasing Verbs for Noun Compound Interpretation</b></br>
Authors: , Nakov, Preslav</br>
  An important challenge for the automatic analysis of English written text is the abundance of noun compounds: sequences of nouns acting as a single noun. In our view, their semantics is best characterized by the set of all possible paraphrasing verbs, with associated weights, e.g., malaria mosquito is carry (23), spread (16), cause (12), transmit (9), etc. Using Amazon\'s Mechanical Turk, we collect paraphrasing verbs for 250 noun-noun compounds previously proposed in the linguistic literature, thus creating a valuable resource for noun compound <font color="#be00be">interpret</font>ation. Using these verbs, we further construct a dataset of pairs of sentences representing a special kind of textual entailment task, where a binary decision is to be made about whether an expression involving a verb and two nouns can be transformed into a noun compound, while preserving the sentence meaning. </br></br>

<a href='http://arxiv.org/pdf/1911.07937.pdf'>1911.07937</a> &nbsp&nbsp (cs:CV, cs:ML, stat:ML) &nbsp&nbsp -0.1912баллов, №583</br>
<b>Inverse Graphics: Unsupervised Learning of 3D Shapes from Single Images</b></br>
Authors: , Ucar, Talip</br>
  Using generative models for Inverse Graphics is an active area of research. However, most works focus on developing models for supervised and semi-supervised methods. In this paper, we study the problem of unsupervised learning of 3D geometry from single images. Our approach is to use a generative model that produces 2-D images as projections of a latent 3D voxel grid, which we train either as a variational auto-encoder or using adversarial methods. Our contributions are as follows: First, we show how to recover 3D shape and pose from general datasets such as MNIST, and MNIST Fashion in good quality. Second, we compare the shapes learned using adversarial and variational methods. Adversarial approach gives denser 3D shapes. Third, we explore the idea of modelling the pose of an object as uniform distribution to recover 3D shape from a single image. Our experiment with the CelebA dataset \\cite{liu2015faceattributes} proves that we can recover complete 3D shape from a single image when the object is symmetric along one, or more axis whilst results obtained using ModelNet40 \\cite{wu20153d} show the potential side-effects, in which the model learns 3D shapes such that it can render the same image from any viewpoint. Forth, we present a general end-to-end approach to learning 3D shapes from single images in a completely unsupervised fashion by modelling the factors of variation such as azimuth as independent latent variables. Our method makes no assumptions about the dataset, and can work with synthetic as well as real images (i.e. unsupervised in true sense). We present our results, by training the model using the $\\mu$-VAE objective \\cite{ucar2019bridging} and a dataset combining all images from MNIST, MNIST Fashion, CelebA and six categories of ModelNet40. The model is able to learn 3D shapes and the pose in qood quality and leverages information learned across all datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.08056.pdf'>1911.08056</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.1916баллов, №584</br>
<b>Modelling pressure-Hessian from local velocity gradients information in\n  an incompressible turbulent flow field using deep neural networks</b></br>
Authors: , Parashar, Nishant, Sinha, Sawan S., Srinivasan, Balaji</br>
  The understanding of the dynamics of the velocity gradients in turbulent flows is critical to understanding various non-linear turbulent processes. The pressure-Hessian and the viscous-Laplacian govern the evolution of the velocity-gradients and are known to be non-local in nature. Over the years, several simplified dynamical models have been proposed that models the viscous-Laplacian and the pressure-Hessian primarily in terms of local velocity gradients information. These models can also serve as closure models for the Lagrangian PDF methods. The recent fluid deformation closure model (RFDM) has been shown to retrieve excellent one-time statistics of the viscous process. However, the pressure-Hessian modelled by the RFDM has various physical limitations. In this work, we first demonstrate the limitations of the RFDM in estimating the pressure-Hessian. Further, we employ a tensor basis neural network (TBNN) to model the pressure-Hessian from the velocity gradient tensor itself. The neural network is trained on high-resolution data obtained from direct numerical simulation (DNS) of isotropic turbulence at Reynolds number of 433 (JHU turbulence database, JHTD). The predictions made by the TBNN are tested against two different isotropic turbulence datasets at Reynolds number of 433 (JHTD) and 315 (UP Madrid turbulence database, UPMTD) and channel flow dataset at Reynolds number of 1000 (UT Texas and JHTD). The evaluation of the neural network output is made in terms of the alignment statistics of the predicted pressure-Hessian eigenvectors with the strain-rate eigenvectors for turbulent isotropic flow as well as channel flow. Our analysis of the predicted solution leads to the discovery of ten unique coefficients of the tensor basis of strain-rate and rotation-rate tensors, the linear combination over which accurately captures key alignment statistics of the pressure-Hessian tensor. </br></br>

<a href='http://arxiv.org/pdf/1911.07205.pdf'>1911.07205</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.1920баллов, №585</br>
<b>REFIT: a Unified Watermark Removal Framework for Deep Learning Systems\n  with Limited Data</b></br>
Authors: , Chen, Xinyun, Wang, Wenxiao, Bender, Chris, Ding, Yiming, Jia, Ruoxi, Li, Bo, Song, Dawn</br>
  Deep neural networks (DNNs) have achieved tremendous success in various fields; however, training these models from scratch could be computationally expensive and requires a lot of training data. Recent work has explored different watermarking techniques to protect the pre-trained deep neural networks from potential copyright infringements; however, they could be vulnerable to adversaries who aim at removing the watermarks. In this work, we propose REFIT, a unified watermark removal framework based on fine-tuning, which does not rely on the knowledge of the watermarks and even the watermarking schemes. Firstly, we demonstrate that by properly designing the learning rate schedule for fine-tuning, an adversary is always able to remove the watermarks. Furthermore, we conduct a comprehensive study of a realistic attack scenario where the adversary has limited training data. To effectively remove the watermarks without compromising the model functionality under this weak threat model, we propose to incorporate two techniques: (1) an adaption of the elastic weight consolidation (EWC) algorithm, which is originally proposed for mitigating the catastrophic forgetting phenomenon; and (2) unlabeled data augmentation (AU), where we leverage auxiliary unlabeled data from other sources. Our extensive evaluation shows the effectiveness of REFIT against diverse watermark embedding schemes. In particular, both EWC and AU significantly decrease the amount of labeled training data needed for effective watermark removal, and the unlabeled data samples used for AU do not necessarily need to be drawn from the same distribution as the benign data for model evaluation. The experimental results demonstrate that our fine-tuning based watermark removal attacks could pose real threats to the copyright of pre-trained models, and thus highlights the importance of further investigation of the watermarking problem. </br></br>

<a href='http://arxiv.org/pdf/1911.09199.pdf'>1911.09199</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.1932баллов, №586</br>
<b>Object-Guided Instance Segmentation for Biological Images</b></br>
Authors: , Yi, Jingru, Tang, Hui, Wu, Pengxiang, Liu, Bo, Hoeppner, Daniel J., Metaxas, Dimitris N., Han, Lianyi, Fan, Wei</br>
  Instance <font color="#be00be">segmentation</font> of biological images is essential for studying object behaviors and properties. The challenges, such as <font color="#be00be">clustering</font>, occlusion, and adhesion problems of the objects, make instance segmentation a non-trivial task. Current box-free instance segmentation methods typically rely on local pixel-level information. Due to a lack of global object view, these methods are prone to over- or under-segmentation. On the contrary, the box-based instance segmentation methods incorporate <font color="#be00be">object detection</font> into the segmentation, performing better in identifying the individual instances. In this paper, we propose a new box-based instance segmentation method. Mainly, we locate the object bounding boxes from their center points. The object features are subsequently reused in the segmentation branch as a guide to separate the clustered instances within an RoI patch. Along with the instance normalization, the model is able to recover the target object distribution and suppress the distribution of neighboring attached objects. Consequently, the proposed model performs excellently in segmenting the clustered objects while retaining the target object details. The proposed method achieves <font color="red">state-of-the-art</font> performances on three biological datasets: cell nuclei, plant phenotyping dataset, and neural cells. </br></br>

<a href='http://arxiv.org/pdf/1911.06992.pdf'>1911.06992</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.1973баллов, №587</br>
<b>Learning Efficient Multi-agent Communication: An Information Bottleneck\n  Approach</b></br>
Authors: , Wang, Rundong, He, Xu, Yu, Runsheng, Qiu, Wei, An, Bo, Rabinovich, Zinovi</br>
  Many <font color="#009600">real-world</font> multi-agent <font color="#00be00">reinforcement learning</font> applications require agents to communicate, assisted by a communication protocol. These applications<font color="#be00be"> face </font>a common and critical issue of communication\'s limited bandwidth that constrains agents\' ability to cooperate successfully. In this paper, rather than proposing a fixed communication protocol, we develop an Informative Multi-Agent Communication (IMAC) method to learn efficient communication protocols. Our contributions are threefold. First, we notice a fact that a limited bandwidth translates into a constraint on the communicated message entropy, thus paving the way of controlling the bandwidth. Second, we introduce a customized batch-norm layer, which controls the messages\' entropy to simulate the limited bandwidth constraint. Third, we apply the information bottleneck method to discover the optimal communication protocol, which can satisfy a bandwidth constraint via training with the prior distribution in the method. To demonstrate the efficacy of our method, we conduct extensive experiments in various cooperative and <font color="#960096">competitive</font> multi-agent tasks across two dimensions: the number of agents and different bandwidths. We show that IMAC converges fast, and leads to efficient communication among agents under the limited-bandwidth constraint as compared to many baseline methods. </br></br>

<a href='http://arxiv.org/pdf/1911.08877.pdf'>1911.08877</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.2030баллов, №588</br>
<b>Improving Semantic Segmentation of Aerial Images Using Patch-based\n  Attention</b></br>
Authors: , Ding, Lei, Tang, Hao, Bruzzone, Lorenzo</br>
  The trade-off between feature representation power and spatial localization accuracy is crucial for the dense classification/semantic <font color="#be00be">segmentation</font> of aerial images. High-level features extracted from the late layers of a neural network are rich in semantic information, yet have blurred spatial details; low-level features extracted from the early layers of a network contain more pixel-level information, but are isolated and noisy. It is therefore difficult to bridge the gap between high and low-level features due to their difference in terms of physical information content and spatial distribution. In this work, we contribute to solve this problem by enhancing the feature representation in two ways. On the one hand, a patch attention module (PAM) is proposed to enhance the embedding of context information based on a patch-wise calculation of local attention. On the other hand, an attention embedding module (AEM) is proposed to enrich the semantic information of low-level features by embedding local focus from high-level features. Both of the proposed modules are light-weight and can be applied to process the extracted features of convolutional neural networks (CNNs). Experiments show that, by integrating the proposed modules into the baseline Fully Convolutional Network (FCN), the resulting local attention network (LANet) greatly improves the performance over the baseline and <font color="#00be00">outperform</font>s other attention based methods on two aerial image datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.07340.pdf'>1911.07340</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.2134баллов, №589</br>
<b>A Sketch-Based System for Human-Guided Constrained Object Manipulation</b></br>
Authors: , Masnadi, Sina, LaViola Jr., Joseph J., Pavlasek, Jana, Zhu, Xiaofan, Desingh, Karthik, Jenkins, Odest Chadwicke</br>
  In this paper, we present an easy to use sketch-based interface to extract geometries and generate affordance files from 3D <font color="#be00be">point cloud</font>s for robot-object interaction tasks. Using our system, even novice users can perform robot task planning by employing such sketch tools. Our focus in this paper is employing human-in-the-loop approach to assist in the generation of more accurate affordance templates and guidance of robot through the task execution process. Since we do not employ any unsupervised learning to generate affordance templates, our system performs much faster and is more versatile for template generation. Our system is based on the extraction of geometries for generalized cylindrical and cuboid shapes, after extracting the geometries, affordances are generated for objects by applying simple sketches. We evaluated our technique by asking users to define affordances by employing sketches on the 3D scenes of a door handle and a drawer handle and used the resulting extracted affordance template files to perform the tasks of turning a door handle and opening a drawer by the robot. </br></br>

<a href='http://arxiv.org/pdf/1911.07933.pdf'>1911.07933</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.2135баллов, №590</br>
<b>Dont Even Look Once: Synthesizing Features for Zero-Shot Detection</b></br>
Authors: , Zhu, Pengkai, Wang, Hanxiao, Saligrama, Venkatesh</br>
  <font color="#00be00">Zero-shot</font> detection, namely, localizing both seen and unseen objects, increasingly gains importance for large-scale applications, with large number of object classes, since, collecting sufficient annotated data with ground truth bounding boxes is simply not scalable. While vanilla deep neural networks deliver high performance for objects available during training, unseen <font color="#be00be">object detection</font> degrades significantly. At a fundamental level, while vanilla detectors are capable of proposing bounding boxes, which include unseen objects, they are often incapable of assigning high-confidence to unseen objects, due to the inherent precision/recall tradeoffs that requires rejecting background objects. We propose a novel detection algorithm Dont Even Look Once (DELO), that synthesizes visual features for unseen objects and augments existing training algorithms to incorporate unseen object detection. Our proposed scheme is evaluated on Pascal VOC and MSCOCO, and we demonstrate significant improvements in test accuracy over vanilla and other state-of-art zero-shot detectors </br></br>

<a href='http://arxiv.org/pdf/1911.06464.pdf'>1911.06464</a> &nbsp&nbsp (cs:CV, cs:RO) &nbsp&nbsp -0.2137баллов, №591</br>
<b>Multiple Style-Transfer in Real-Time</b></br>
Authors: , Maring, Michael, Chakraborty, Kaustav</br>
  <font color="#be00be">Style</font> transfer aims to combine the content of one image with the artistic style of another. It was discovered that lower levels of convolutional networks captured style information, while higher levels captures content information. The original style transfer formulation used a weighted combination of VGG-16 layer activations to achieve this goal. Later, this was accomplished in real-time using a feed-forward network to learn the optimal combination of style and content features from the respective images. The first aim of our project was to introduce a framework for capturing the style from several images at once. We propose a method that extends the original real-time style transfer formulation by combining the features of several style images. This method successfully captures color information from the separate style images. The other aim of our project was to improve the temporal style continuity from frame to frame. Accordingly, we have experimented with the temporal stability of the output images and discussed the various available techniques that could be employed as alternatives. </br></br>

<a href='http://arxiv.org/pdf/1911.07849.pdf'>1911.07849</a> &nbsp&nbsp (cs:CV, cs:ML, stat:ML) &nbsp&nbsp -0.2150баллов, №592</br>
<b>Co-Attentive Equivariant Neural Networks: Focusing Equivariance On\n  Transformations Co-Occurring In Data</b></br>
Authors: , Romero, David W., Hoogendoorn, Mark</br>
  Equivariance is a nice property to have as it produces much more parameter efficient neural architectures and preserves the structure of the input through the feature mapping. Even though some combinations of transformations might never appear (e.g. an upright<font color="#be00be"> face </font>with a horizontal nose), current equivariant architectures consider the set of all possible transformations in a transformation group when learning feature representations. Contrarily, the human visual system is able to attend to the set of relevant transformations occurring in the environment and utilizes this information to assist and improve object recognition. Based on this observation, we modify conventional equivariant feature mappings such that they are able to attend to the set of co-occurring transformations in data and generalize this notion to act on groups consisting of multiple symmetries. We show that our proposed co-attentive equivariant neural networks consistently <font color="#00be00">outperform</font> conventional rotation equivariant and rotation &amp; reflection equivariant neural networks on rotated MNIST and CIFAR-10. </br></br>

<a href='http://arxiv.org/pdf/1910.01299.pdf'>1910.01299</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp -0.2157баллов, №593</br>
<b>Hitachi at MRP 2019: Unified Encoder-to-Biaffine Network for\n  Cross-Framework Meaning Representation Parsing</b></br>
Authors: , Koreeda, Yuta, Morio, Gaku, Morishita, Terufumi, Ozaki, Hiroaki, Yanai, Kohsuke</br>
  This paper describes the proposed system of the Hitachi team for the Cross-Framework Meaning Representation <font color="#be00be">Parsing</font> (MRP 2019) shared task. In this shared task, the participating systems were asked to predict nodes, edges and their attributes for five frameworks, each with different order of &quot;abstraction&quot; from input tokens. We proposed a unified encoder-to-biaffine network for all five frameworks, which effectively incorporates a shared encoder to extract rich input features, decoder networks to generate anchorless nodes in UCCA and AMR, and biaffine networks to predict edges. Our system was ranked fifth with the macro-averaged MRP F1 score of 0.7604, and <font color="#00be00">outperform</font>ed the baseline unified transition-based MRP. Furthermore, post-evaluation experiments showed that we can boost the performance of the proposed system by incorporating multi-task learning, whereas the baseline could not. These imply efficacy of incorporating the biaffine network to the shared architecture for MRP and that learning heterogeneous meaning representations at once can boost the system performance. </br></br>

<a href='http://arxiv.org/pdf/1910.12381.pdf'>1910.12381</a> &nbsp&nbsp (cs:SD, stat:ML) &nbsp&nbsp -0.2188баллов, №594</br>
<b>Transferring neural speech waveform synthesizers to musical instrument\n  sounds generation</b></br>
Authors: , Zhao, Yi, Wang, Xin, Juvela, Lauri, Yamagishi, Junichi</br>
  Recent neural waveform synthesizers such as WaveNet, WaveGlow, and the neural-source-filter (NSF) model have shown good performance in speech synthesis despite their different methods of waveform generation. The similarity between speech and<font color="#be00be"> music </font>audio synthesis techniques suggests interesting avenues to explore in terms of the best way to apply speech synthesizers in the music domain. This work compares three neural synthesizers used for musical instrument sounds generation under three scenarios: training from scratch on music data, <font color="#00be00">zero-shot</font> learning from the speech domain, and fine-tuning-based adaptation from the speech to the music domain. The results of a large-scale perceptual test demonstrated that the performance of three synthesizers improved when they were pre-trained on speech data and fine-tuned on music data, which indicates the usefulness of knowledge from speech data for music audio generation. Among the synthesizers, WaveGlow showed the best potential in zero-shot learning while NSF performed best in the other scenarios and could generate samples that were perceptually close to natural audio. </br></br>

<a href='http://arxiv.org/pdf/1911.09661.pdf'>1911.09661</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp -0.2195баллов, №595</br>
<b>Paraphrasing with Large Language Models</b></br>
Authors: , Witteveen, Sam, Andrews, Martin</br>
  Recently, large language models such as<font color="#00be00"> GPT</font>-2 have shown themselves to be extremely adept at text generation and have also been able to achieve high-quality results in many downstream NLP tasks such as text classification, <font color="#be00be">sentiment</font> analysis and question answering with the aid of fine-tuning. We present a useful technique for using a large language model to perform the task of paraphrasing on a variety of texts and subjects. Our approach is demonstrated to be capable of generating paraphrases not only at a sentence level but also for longer spans of text such as paragraphs without needing to break the text into smaller chunks. </br></br>

<a href='http://arxiv.org/pdf/1911.09271.pdf'>1911.09271</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.2227баллов, №596</br>
<b>Cantonese Automatic Speech Recognition Using Transfer Learning from\n  Mandarin</b></br>
Authors: , Li, Bryan, Wang, Xinyue, Beigi, Homayoon</br>
  We propose a system to develop a basic automatic speech recognizer(ASR) for Cantonese, a <font color="#be00be">low-resource</font> language, through transfer learning of Mandarin, a high-resource language. We take a time-delayed neural network trained on Mandarin, and perform weight transfer of several layers to a newly initialized model for Cantonese. We experiment with the number of layers transferred, their learning rates, and pretraining i-vectors. Key findings are that this approach allows for quicker training time with less data. We find that for every epoch, log-probability is smaller for transfer learning models compared to a Cantonese-only model. The transfer learning models show slight improvement in CER. </br></br>

<a href='http://arxiv.org/pdf/1911.07615.pdf'>1911.07615</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.2288баллов, №597</br>
<b>Bandwidth Slicing to Boost Federated Learning in Edge Computing</b></br>
Authors: , Li, Jun, Shen, Xiaoman, Chen, Lei, Chen, Jiajia</br>
  Bandwidth slicing is introduced to support <font color="#be00be">federated</font> learning in edge computing to assure low communication delay for training traffic. Results reveal that bandwidth slicing significantly improves training efficiency while achieving good learning accuracy. </br></br>

<a href='http://arxiv.org/pdf/1911.09146.pdf'>1911.09146</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.2306баллов, №598</br>
<b>Deadlock Analysis and Resolution in Multi-Robot Systems: The Two Robot\n  Case</b></br>
Authors: , Grover, Jaskaran, Liu, Changliu, Sycara, Katia</br>
  Collision avoidance for multirobot systems is a well studied problem. Recently, control barrier functions (CBFs) have been proposed for synthesizing decentralized controllers that guarantee collision avoidance (safety) and goal stabilization (performance) for multiple robots. However, it has been noted in several works that reactive control synthesis methods (such as CBFs) are prone to deadlock, an equilibrium of system dynamics that causes the robots to come to a standstill before they reach their goals. In this paper, we analyze the incidence of deadlocks in a multirobot system that uses CBFs for goal stabilization and collision avoidance. Our analysis is formal, in that we demonstrate that system deadlock is indeed the result of a force-equilibrium on robots. We show how to <font color="#be00be">interpret</font> deadlock as a subset of the state space and prove that this set is non-empty, bounded, of measure zero and located on the boundary of the safe set. Based on this analysis, we develop a decentralized three-phase algorithm that uses feedback linearization to ensure that the robots provably exit the deadlock set and converge to their goals while avoiding collisions. We show simulation results and experimentally validate the deadlock resolution algorithm on Khepera-IV robots. </br></br>

<a href='http://arxiv.org/pdf/1911.08008.pdf'>1911.08008</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.2345баллов, №599</br>
<b>Towards a complete 3D morphable model of the human head</b></br>
Authors: , Ploumpis, Stylianos, Ververas, Evangelos, Sullivan, Eimear O\', Moschoglou, Stylianos, Wang, Haoyang, Pears, Nick, Smith, William A. P., Gecer, Baris, Zafeiriou, Stefanos</br>
  Three-dimensional Morphable Models (3DMMs) are powerful statistical tools for representing the 3D shapes and textures of an object class. Here we present the most complete 3DMM of the human head to date that includes face, cranium, ears, eyes, teeth and tongue. To achieve this, we propose two methods for combining existing 3DMMs of different overlapping head parts: i. use a regressor to complete missing parts of one model using the other, ii. use the <font color="blue">Gaussi</font>an Process framework to blend covariance matrices from multiple models. Thus we build a new combined face-and-head shape model that blends the variability and<font color="#be00be"> facial </font>detail of an existing<font color="#be00be"> face </font>model (the LSFM) with the full head modelling capability of an existing head model (the LYHM). Then we construct and fuse a highly-detailed ear model to extend the variation of the ear shape. Eye and eye region models are incorporated into the head model, along with basic models of the teeth, tongue and inner mouth cavity. The new model achieves <font color="red">state-of-the-art</font> performance. We use our model to reconstruct full head representations from single, unconstrained images allowing us to parameterize craniofacial shape and texture, along with the ear shape, eye gaze and eye color. </br></br>

<a href='http://arxiv.org/pdf/1911.07588.pdf'>1911.07588</a> &nbsp&nbsp (cs:CL, cs:AI) &nbsp&nbsp -0.2345баллов, №600</br>
<b>An Annotated Corpus of Reference Resolution for Interpreting Common\n  Grounding</b></br>
Authors: , Udagawa, Takuma, Aizawa, Akiko</br>
  Common grounding is the process of creating, repairing and updating mutual understandings, which is a fundamental aspect of natural language conversation. However, <font color="#be00be">interpret</font>ing the process of common grounding is a challenging task, especially under continuous and partially-observable context where complex ambiguity, uncertainty, partial understandings and misunderstandings are introduced. Interpretation becomes even more challenging when we deal with dialogue systems which still have limited capability of natural language understanding and generation. To address this problem, we consider reference resolution as the central subtask of common grounding and propose a new resource to study its intermediate process. Based on a simple and general annotation schema, we collected a total of 40,172 referring expressions in 5,191 dialogues curated from an existing corpus, along with multiple judgements of referent interpretations. We show that our annotation is highly reliable, captures the complexity of common grounding through a natural degree of reasonable disagreements, and allows for more detailed and quantitative analyses of common grounding strategies. Finally, we demonstrate the advantages of our annotation for interpreting, analyzing and improving common grounding in baseline dialogue systems. </br></br>

<a href='http://arxiv.org/pdf/1911.08212.pdf'>1911.08212</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.2402баллов, №601</br>
<b>Deep Poetry: A Chinese Classical Poetry Generation System</b></br>
Authors: , Liu, Yusen, Liu, Dayiheng, Lv, Jiancheng</br>
  In this work, we demonstrate a <font color="#be00be">Chinese</font> classical poetry generation system called Deep Poetry. Existing systems for Chinese classical poetry generation are mostly template-based and very few of them can accept multi-modal input. Unlike previous systems, Deep Poetry uses neural networks that are trained on over 200 thousand poems and 3 million ancient Chinese prose. Our system can accept plain text, images or artistic conceptions as inputs to generate Chinese classical poetry. More importantly, users are allowed to participate in the process of writing poetry by our system. For the user\'s convenience, we deploy the system at the WeChat applet platform, users can use the system on the<font color="#960096"> mobile </font>device whenever and wherever possible. The demo video of this paper is available at <font color="#006400">http</font>s://youtu.be/jD1R_u9TA3M. </br></br>

<a href='http://arxiv.org/pdf/1911.03042.pdf'>1911.03042</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.2409баллов, №602</br>
<b>Neural Graph Embedding Methods for Natural Language Processing</b></br>
Authors: , Vashishth, Shikhar</br>
  <font color="#960096">Knowledge graph</font>s are structured representations of facts in a graph, where nodes represent entities and edges represent relationships between them. Recent research has resulted in the development of several large KGs. However, all of them tend to be sparse with very few facts per entity. In the first part of the thesis, we propose three solutions to alleviate this problem: (1) KG Canonicalization, i.e., identifying and merging duplicate entities in a KG, (2) Relation Extraction which involves automating the process of extracting semantic relationships between entities from unstructured text, and (3) Link prediction which includes inferring missing facts based on the known facts in a KG. Traditional Neural Networks like CNNs and RNNs are constrained to handle Euclidean data. However, graphs in Natural Language Processing (NLP) are prominent. Recently, Graph Convolutional Networks (GCNs) have been proposed to address this shortcoming and have been successfully applied for several problems. In the second part of the thesis, we utilize GCNs for Document Timestamping problem and for learning word embeddings using dependency context of a word instead of sequential context. In this third part of the thesis, we address two limitations of existing GCN models, i.e., (1) The standard neighborhood aggregation scheme puts no constraints on the number of nodes that can influence the representation of a target node. This leads to a noisy representation of hub-nodes which coves almost the entire graph in a few hops. (2) Most of the existing GCN models are limited to handle undirected graphs. However, a more general and pervasive class of graphs are relational graphs where each edge has a label and direction associated with it. Existing approaches to handle such graphs suffer from over-parameterization and are restricted to learning representation of nodes only. </br></br>

<a href='http://arxiv.org/pdf/1911.07964.pdf'>1911.07964</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.2442баллов, №603</br>
<b>Eigenvalue Normalized Recurrent Neural Networks for Short Term Memory</b></br>
Authors: , Helfrich, Kyle, Ye, Qiang</br>
  Several variants of recurrent neural networks (RNNs) with orthogonal or unitary recurrent matrices have recently been developed to mitigate the vanishing/exploding gradient problem and to model long-term dependencies of sequences. However, with the eigenvalues of the recurrent matrix on the unit circle, the recurrent state retains all input information which may unnecessarily consume model capacity. In this paper, we address this issue by proposing an architecture that expands upon an orthogonal/unitary RNN with a state that is generated by a recurrent matrix with eigenvalues in the unit disc. Any input to this state dissipates in time and is replaced with new inputs, simulating short-term memory. A gradient descent algorithm is derived for learning such a recurrent matrix. The resulting method, called the Eigenvalue Normalized RNN (ENRNN), is shown to be highly <font color="#960096">competitive</font> in several experiments. </br></br>

<a href='http://arxiv.org/pdf/1911.07980.pdf'>1911.07980</a> &nbsp&nbsp (cs:CV, cs:RO) &nbsp&nbsp -0.2456баллов, №604</br>
<b>Simultaneous Mapping and Target Driven Navigation</b></br>
Authors: , Georgakis, Georgios, Li, Yimeng, Kosecka, Jana</br>
  This work presents a modular architecture for simultaneous mapping and target driven navigation in indoors environments. The semantic and appearance stored in 2.5D map is distilled from RGB images, semantic <font color="#be00be">segmentation</font> and outputs of object detectors by convolutional neural networks. Given this representation, the mapping module learns to localize the agent and register consecutive observations in the map. The navigation task is then formulated as a problem of learning a policy for reaching semantic targets using current observations and the up-to-date map. We demonstrate that the use of semantic information improves localization accuracy and the ability of storing spatial semantic map aids the target driven navigation policy. The two modules are evaluated separately and jointly on Active Vision Dataset and Matterport3D environments, demonstrating improved performance on both localization and navigation tasks. </br></br>

<a href='http://arxiv.org/pdf/1910.03087.pdf'>1910.03087</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.2494баллов, №605</br>
<b>Force Field Generalization and the Internal Representation of Motor\n  Learning</b></br>
Authors: , Rezazadeh, Alireza, Berniker, Max</br>
  When learning a new motor behavior, e.g. reaching in a force field, the nervous system builds an internal representation. Examining how subsequent reaches in unpracticed directions generalize reveals this representation. Though it is the subject of frequent studies, it is not known how this representation changes across training directions, or how changes in reach direction and the corresponding changes in limb impedance, influence measurements of it. We ran a force field adaptation experiment using eight groups of subjects each trained on one of eight standard directions and then tested for generalization in the remaining seven directions. Generalization in all directions was local and asymmetric, providing limited and unequal transfer to the left and right side of the trained target. These asymmetries were not consistent in either magnitude or direction even after correcting for changes in limb impedance, at odds with previous explanations. Relying on a standard model for generalization the inferred representations inconsistently shifted to one side or the other of their respective training direction. A second model that accounted for limb impedance and variations in baseline trajectories explained more data and the inferred representations were centered on their respective training directions. Our results highlight the influence of limb mechanics and impedance on psychophysical measurements and their <font color="#be00be">interpret</font>ations for motor learning. </br></br>

<a href='http://arxiv.org/pdf/1911.08089.pdf'>1911.08089</a> &nbsp&nbsp (cs:AI, cs:ML) &nbsp&nbsp -0.2511баллов, №606</br>
<b>&quot;The Human Body is a Black Box&quot;: Supporting Clinical Decision-Making\n  with Deep Learning</b></br>
Authors: , Sendak, Mark, Elish, Madeleine, Gao, Michael, Futoma, Joseph, Ratliff, William, Nichols, Marshall, Bedoya, Armando, Balu, Suresh, O\'Brien, Cara</br>
  Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating <font color="red">state-of-the-art</font> models, there has been less focus on <font color="#009600">real world</font> implementation and the associated challenges to accuracy, fairness, accountability, and transparency that come from actual, situated use. Serious questions remain under examined regarding how to ethically build models, <font color="#be00be">interpret</font> and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital <font color="blue">clinic</font>ians in the early <font color="blue">diagnos</font>is and treatment of sepsis. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing on model interpretability to ensure a fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice. </br></br>

<a href='http://arxiv.org/pdf/1911.09075.pdf'>1911.09075</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.2555баллов, №607</br>
<b>Real-Time Emotion Recognition via Attention Gated Hierarchical Memory\n  Network</b></br>
Authors: , Jiao, Wenxiang, Lyu, Michael R., King, Irwin</br>
  Real-time <font color="#be00be">emotion</font> recognition (RTER) in conversations is significant for developing emotionally intelligent chatting machines. Without the future context in RTER, it becomes critical to build the memory bank carefully for capturing historical context and summarize the memories appropriately to retrieve relevant information. We propose an Attention Gated <font color="#00be00">Hierarchical</font> Memory Network (AGHMN) to address the problems of prior work: (1) Commonly used convolutional neural networks (CNNs) for utterance feature extraction are less compatible in the memory modules; (2) Unidirectional gated recurrent units (GRUs) only allow each historical utterance to have context before it, preventing information propagation in the opposite direction; (3) The Soft Attention for summarizing loses the positional and ordering information of memories, regardless of how the memory bank is built. Particularly, we propose a Hierarchical Memory Network (HMN) with a bidirectional GRU (BiGRU) as the utterance reader and a BiGRU fusion layer for the interaction between historical utterances. For memory summarizing, we propose an Attention GRU (AGRU) where we utilize the attention weights to update the internal state of GRU. We further promote the AGRU to a bidirectional variant (BiAGRU) to balance the contextual information from recent memories and that from distant memories. We conduct experiments on two emotion conversation datasets with extensive analysis, demonstrating the efficacy of our AGHMN models. </br></br>

<a href='http://arxiv.org/pdf/1911.08769.pdf'>1911.08769</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp -0.2567баллов, №608</br>
<b>Inspect Transfer Learning Architecture with Dilated Convolution</b></br>
Authors: , Azim, Syeda Noor Jaha, Ratul, Md. Aminur Rab</br>
  There are many award-winning pre-trained Convolutional Neural Network (CNN), which have a common phenomenon of increasing depth in convolutional layers. However, I inspect on VGG network, which is one of the famous model submitted to ILSVRC-2014, to show that slight modification in the basic architecture can enhance the accuracy result of the image classification task. In this paper, We present two improve architectures of pre-trained VGG-16 and VGG-19 networks that apply transfer learning when trained on a different dataset. I report a series of experimental result on various modification of the primary VGG networks and achieved significant out-performance on image classification task by: (1) freezing the first two blocks of the convolutional layers to prevent over-fitting and (2) applying different combination of dilation rate in the last three blocks of convolutional layer to reduce image resolution for feature extraction. Both the proposed architecture achieves a <font color="#960096">competitive</font> result on CIFAR-10 and CIFAR-100 dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.07446.pdf'>1911.07446</a> &nbsp&nbsp (cs:ML, cs:CV, cs:NE, stat:ML) &nbsp&nbsp -0.2579баллов, №609</br>
<b>NAIS: Neural Architecture and Implementation Search and its Applications\n  in Autonomous Driving</b></br>
Authors: , Hao, Cong, Chen, Yao, Liu, Xinheng, Sarwari, Atif, Sew, Daryl, Dhar, Ashutosh, Wu, Bryan, Fu, Dongdong, Xiong, Jinjun, Hwu, Wen-mei, Gu, Junli, Chen, Deming</br>
  The rapidly growing demands for powerful AI algorithms in many application domains have motivated massive investment in both high-quality deep neural network (DNN) models and high-efficiency implementations. In this position paper, we argue that a simultaneous DNN/implementation co-design methodology, named Neural Architecture and Implementation Search (NAIS), deserves more research attention to boost the development productivity and efficiency of both DNN models and implementation optimization. We propose a stylized design methodology that can drastically cut down the search cost while preserving the quality of the end solution.As an illustration, we discuss this DNN/implementation methodology in the context of both <font color="#be00be">FPGA</font>s and GPUs. We take autonomous driving as a key use case as it is one of the most demanding areas for high quality AI algorithms and accelerators. We discuss how such a co-design methodology can impact the autonomous driving industry significantly. We identify several research opportunities in this exciting domain. </br></br>

<a href='http://arxiv.org/pdf/1911.09669.pdf'>1911.09669</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.2595баллов, №610</br>
<b>Regularizing Neural Networks by Stochastically Training Layer Ensembles</b></br>
Authors: , Labach, Alex, Valaee, Shahrokh</br>
  Dropout and similar stochastic neural network regularization methods are often <font color="#be00be">interpret</font>ed as implicitly averaging over a large ensemble of models. We propose STE (stochastically trained ensemble) layers, which enhance the averaging properties of such methods by training an ensemble of weight matrices with stochastic regularization while explicitly averaging outputs. This provides stronger regularization with no additional computational cost at test time. We show consistent improvement on various image classification tasks using standard network topologies. </br></br>

<a href='http://arxiv.org/pdf/1911.09017.pdf'>1911.09017</a> &nbsp&nbsp (cs:ML, cs:AI, cs:CV, stat:ML) &nbsp&nbsp -0.2597баллов, №611</br>
<b>Towards a Unified Evaluation of Explanation Methods without Ground Truth</b></br>
Authors: , Zhang, Hao, Chen, Jiayi, Xue, Haotian, Zhang, Quanshi</br>
  This paper proposes a set of criteria to evaluate the objectiveness of explanation methods of neural networks, which is crucial for the development of explainable AI, but it also presents significant challenges. The core challenge is that people usually cannot obtain ground-truth explanations of the neural network. To this end, we design four metrics to evaluate explanation results without ground-truth explanations. Our metrics can be broadly applied to nine benchmark methods of <font color="#be00be">interpret</font>ing neural networks, which provides new insights of explanation methods. </br></br>

<a href='http://arxiv.org/pdf/1911.07109.pdf'>1911.07109</a> &nbsp&nbsp (cs:ML, cs:AI, cs:RO, stat:ML) &nbsp&nbsp -0.2625баллов, №612</br>
<b>Reinforcement Learning from Imperfect Demonstrations under Soft Expert\n  Guidance</b></br>
Authors: , Jing, Mingxuan, Ma, Xiaojian, Huang, Wenbing, Sun, Fuchun, Yang, Chao, Fang, Bin, Liu, Huaping</br>
  In this paper, we study <font color="#00be00">Reinforcement Learning</font> from Demonstrations (RLfD) that improves the exploration efficiency of Reinforcement Learning (RL) by providing expert demonstrations. Most of existing RLfD methods require demonstrations to be perfect and sufficient, which yet is unrealistic to meet in practice. To work on imperfect demonstrations, we first define an imperfect expert setting for RLfD in a formal way, and then point out that previous methods suffer from two issues in terms of optimality and convergence, respectively. Upon the <font color="blue">theor</font>etical findings we have derived, we tackle these two issues by regarding the expert guidance as a soft constraint on regulating the policy exploration of the agent, which eventually leads to a constrained optimization problem. We further demonstrate that such problem is able to be addressed efficiently by performing a local linear search on its dual form. Considerable empirical evaluations on a comprehensive collection of benchmarks indicate our method attains consistent improvement over other RLfD counterparts. </br></br>

<a href='http://arxiv.org/pdf/1911.08288.pdf'>1911.08288</a> &nbsp&nbsp (cs:ML, cs:RO) &nbsp&nbsp -0.2636баллов, №613</br>
<b>Smoke Sky -- Exploring New Frontiers of Unmanned Aerial Systems for\n  Wildland Fire Science and Applications</b></br>
Authors: , Stavros, E. Natasha, Agha, Ali, Sirota, Allen, Quadrelli, Marco, Ebadi, Kamak, Yun, Kyongsik</br>
  Wildfire has had increasing impacts on society as the <font color="#be00be">climate</font> changes and the wildland urban interface grows. As such, there is a demand for innovative solutions to help manage fire. Managing wildfire can include proactive fire management such as prescribed burning within constrained areas or advancements for reactive fire management (e.g., fire suppression). Because of the growing societal impact, the JPL BlueSky program sought to assess the current state of fire management and technology and determine areas with high return on investment. To accomplish this, we met with the national interagency Unmanned Aerial System (UAS) Advisory Group (UASAG) and with leading technology transfer experts for fire science and management applications. We provide an overview of the current state as well as an analysis of the impact, maturity and feasibility of integrating different technologies that can be developed by JPL. Based on the findings, the highest return on investment technologies for fire management are first to develop single micro-aerial vehicle (MAV) autonomy, autonomous sensing over fire, and the associated data and information system for active fire local environment mapping. Once this is completed for a single MAV, expanding the work to include many in a swarm would require further investment of distributed MAV autonomy and MAV swarm mechanics, but could greatly expand the breadth of application over large fires. Important to investing in these technologies will be in developing collaborations with the key influencers and champions for using UAS technology in fire management. </br></br>

<a href='http://arxiv.org/pdf/1911.07555.pdf'>1911.07555</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.2665баллов, №614</br>
<b>Short Text Language Identification for Under Resourced Languages</b></br>
Authors: , Duvenhage, Bernardt</br>
  The paper presents a <font color="#00be00">hierarchical</font> naive <font color="blue">Bayes</font>ian and lexicon based classifier for short text language identification (LID) useful for under resourced languages. The algorithm is evaluated on short pieces of text for the 11 official South African languages some of which are similar languages. The algorithm is compared to recent approaches using test sets from previous works on South African languages as well as the Discriminating between Similar Languages (DSL) shared tasks\' datasets. Remaining research opportunities and pressing concerns in evaluating and comparing LID approaches are also discussed. </br></br>

<a href='http://arxiv.org/pdf/1911.08373.pdf'>1911.08373</a> &nbsp&nbsp (cs:NE, cs:CL) &nbsp&nbsp -0.2682баллов, №615</br>
<b>Deep Spiking Neural Networks for Large Vocabulary Automatic Speech\n  Recognition</b></br>
Authors: , Wu, Jibin, Yilmaz, Emre, Zhang, Malu, Li, Haizhou, Tan, Kay Chen</br>
  Artificial neural networks (ANN) have become the mainstream acoustic modeling technique for large vocabulary automatic <font color="#be00be">speech recognition</font> (ASR). A conventional ANN features a multi-layer architecture that requires massive amounts of computation. The <font color="#00be00">brain</font>-inspired spiking neural networks (SNN) closely mimic the biological neural networks and can operate on low-power neuromorphic hardware with spike-based computation. Motivated by their unprecedented energyefficiency and rapid information processing capability, we explore the use of SNNs for speech recognition. In this work, we use SNNs for acoustic modeling and evaluate their performance on several large vocabulary recognition scenarios. The experimental results demonstrate <font color="#960096">competitive</font> ASR accuracies to their ANN counterparts, while require significantly reduced computational cost and inference time. Integrating the algorithmic power of deep SNNs with energy-efficient neuromorphic hardware, therefore, offer an attractive solution for ASR applications running locally on<font color="#960096"> mobile </font>and embedded devices. </br></br>

<a href='http://arxiv.org/pdf/1911.09011.pdf'>1911.09011</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -0.2714баллов, №616</br>
<b>Bayesian interpretation of SGD as Ito process</b></br>
Authors: , Yokoi, Soma, Sato, Issei</br>
  The current <font color="#be00be">interpret</font>ation of stochastic gradient descent (SGD) as a stochastic process lacks generality in that its numerical scheme restricts continuous-time dynamics as well as the loss function and the distribution of gradient noise. We introduce a simplified scheme with milder conditions that flexibly interprets SGD as a discrete-time approximation of an Ito process. The scheme also works as a common foundation of SGD and stochastic gradient Langevin dynamics (SGLD), providing insights into their asymptotic properties. We investigate the convergence of SGD with biased gradient in terms of the equilibrium mode and the overestimation problem of the second moment of SGLD. </br></br>

<a href='http://arxiv.org/pdf/1911.07451.pdf'>1911.07451</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.2747баллов, №617</br>
<b>DirectPose: Direct End-to-End Multi-Person Pose Estimation</b></br>
Authors: , Tian, Zhi, Chen, Hao, Shen, Chunhua</br>
  We propose the first direct end-to-end multi-person pose estimation framework, termed DirectPose. Inspired by recent anchor-free object detectors, which directly regress the two corners of target bounding-boxes, the proposed framework directly predicts instance-aware keypoints for all the instances from a raw input image, eliminating the need for heuristic grouping in bottom-up methods or bounding-box detection and RoI operations in top-down ones. We also propose a novel Keypoint Alignment (KPAlign) mechanism, which overcomes the main difficulty: lack of the alignment between the convolutional features and predictions in this end-to-end framework. KPAlign improves the framework\'s performance by a large margin while still keeping the framework end-to-end trainable. With the only postprocessing non-maximum suppression (NMS), our proposed framework can detect multi-person keypoints with or without bounding-boxes in a single shot. Experiments demonstrate that the end-to-end paradigm can achieve <font color="#960096">competitive</font> or better performance than previous strong baselines, in both bottom-up and top-down methods. We hope that our end-to-end approach can provide a new perspective for the human pose estimation task. </br></br>

<a href='http://arxiv.org/pdf/1911.09040.pdf'>1911.09040</a> &nbsp&nbsp (cs:CV, cs:ML, stat:ML) &nbsp&nbsp -0.2764баллов, №618</br>
<b>3D-Rotation-Equivariant Quaternion Neural Networks</b></br>
Authors: , Zhang, Binbin, Shen, Wen, Huang, Shikun, Wei, Zhihua, Zhang, Quanshi</br>
  This paper proposes a set of rules to revise various neural networks for 3D <font color="#be00be">point cloud</font> processing to rotation-equivariant quaternion neural networks (REQNNs). We find that when a neural network uses quaternion features under certain conditions, the network feature naturally has the rotation-equivariance property. Rotation equivariance means that applying a specific rotation transformation to the input point cloud is equivalent to applying the same rotation transformation to all intermediate-layer quaternion features. Besides, the REQNN also ensures that the intermediate-layer features are invariant to the permutation of input points. Compared with the original neural network, the REQNN exhibits higher rotation robustness. </br></br>

<a href='http://arxiv.org/pdf/1911.08478.pdf'>1911.08478</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.2766баллов, №619</br>
<b>Sibling Neural Estimators: Improving Iterative Image Decoding with\n  Gradient Communication</b></br>
Authors: , Mali, Ankur, Ororbia, Alexander G., Giles, Clyde Lee</br>
  For lossy image compression, we develop a neural-based system which learns a nonlinear estimator for decoding from quantized representations. The system links two recurrent networks that \\help&quot; each other reconstruct same target image patches using complementary portions of spatial context that communicate via gradient signals. This dual agent system builds upon prior work that proposed the iterative refinement algorithm for recurrent neural network (RNN)based decoding which improved image reconstruction compared to standard decoding techniques. Our approach, which works with any encoder, neural or non-neural, This system progressively reduces image patch reconstruction error over a fixed number of steps. Experiment with variants of RNN memory cells, with and without future information, find that our model consistently creates lower distortion images of higher perceptual quality compared to other approaches. Specifically, on the Kodak Lossless True Color Image Suite, we observe as much as a 1:64 decibel (dB) gain over JPEG, a 1:46 dB gain over JPEG 2000, a 1:34 dB gain over the GOOG neural baseline, 0:36 over E2E (a modern <font color="#960096">competitive</font> neural compression model), and 0:37 over a single iterative neural decoder. </br></br>

<a href='http://arxiv.org/pdf/1911.02718.pdf'>1911.02718</a> &nbsp&nbsp (cs:CV, cs:RO) &nbsp&nbsp -0.2810баллов, №620</br>
<b>Model Adaption Object Detection System for Robot</b></br>
Authors: , Fu, Jingwen, Zong, Licheng, Li, Yinbing, Li, Ke, Yang, Bingqian, Liu, Xibei</br>
  <font color="#be00be">Object detection</font> for robot guidance is a crucial mission for autonomous robots, which has provoked extensive attention for researchers. However, the changing view of robot movement and limited available data hinder the research in this area. To address these matters, we proposed a new vision system for robots, the model adaptation object detection system. Instead of using a single one to solve problems, We made use of different object detection neural networks to guide the robot in accordance with various situations, with the help of a meta neural network to allocate the object detection neural networks. Furthermore, taking advantage of transfer learning technology and depthwise separable convolutions, our model is easy to train and can address small dataset problems. </br></br>

<a href='http://arxiv.org/pdf/1911.07959.pdf'>1911.07959</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.2810баллов, №621</br>
<b>TracKlinic: Diagnosis of Challenge Factors in Visual Tracking</b></br>
Authors: , Fan, Heng, Yang, Fan, Chu, Peng, Yuan, Lin, Ling, Haibin</br>
  Generic visual <font color="#be00be">tracking</font> is difficult due to many challenge factors (e.g., occlusion, blur, etc.). Each of these factors may cause serious problems for a tracking algorithm, and when they work together can make things even more complicated. Despite a great amount of efforts devoted to understanding the behavior of tracking algorithms, reliable and quantifiable ways for studying the per factor tracking behavior remain barely available. Addressing this issue, in this paper we contribute to the community a tracking <font color="blue">diagnos</font>is toolkit, TracKlinic, for diagnosis of challenge factors of tracking algorithms.   TracKlinic consists of two novel components focusing on the data and analysis aspects, respectively. For the data component, we carefully prepare a set of 2,390 annotated videos, each involving one and only one major challenge factor. When analyzing an algorithm for a specific challenge factor, such one-factor-per-sequence rule greatly inhibits the disturbance from other factors and consequently leads to more faithful analysis. For the analysis component, given the tracking results on all sequences, it investigates the behavior of the <font color="#be00be">tracker</font> under each individual factor and generates the report automatically. With TracKlinic, a thorough study is conducted on ten <font color="red">state-of-the-art</font> trackers on nine challenge factors (including two compound ones). The results suggest that, heavy shape variation and occlusion are the two most challenging factors faced by most trackers. Besides, out-of-view, though does not happen frequently, is often fatal. By sharing TracKlinic, we expect to make it much easier for diagnosing tracking algorithms, and to thus facilitate developing better ones. </br></br>

<a href='http://arxiv.org/pdf/1911.09033.pdf'>1911.09033</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp -0.2825баллов, №622</br>
<b>Exploiting Spatial Invariance for Scalable Unsupervised Object Tracking</b></br>
Authors: , Crawford, Eric, Pineau, Joelle</br>
  The ability to detect and track objects in the visual world is a crucial skill for any intelligent agent, as it is a necessary precursor to any object-level reasoning process. Moreover, it is important that agents learn to track objects without supervision (i.e. without access to annotated training videos) since this will allow agents to begin operating in new environments with minimal human assistance. The task of learning to discover and track objects in videos, which we call \\textit{unsupervised object <font color="#be00be">tracking</font>}, has grown in prominence in recent years; however, most architectures that address it still struggle to deal with large scenes containing many objects. In the current work, we propose an architecture that scales well to the large-scene, many-object setting by employing spatially invariant computations (convolutions and spatial attention) and representations (a spatially local object specification scheme). In a series of experiments, we demonstrate a number of attractive features of our architecture; most notably, that it <font color="#00be00">outperform</font>s competing methods at tracking objects in cluttered scenes with many objects, and that it can generalize well to videos that are larger and/or contain more objects than videos encountered during training. </br></br>

<a href='http://arxiv.org/pdf/1911.04542.pdf'>1911.04542</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.2829баллов, №623</br>
<b>Explainable Artificial Intelligence (XAI) for 6G: Improving Trust\n  between Human and Machine</b></br>
Authors: , Guo, Weisi</br>
  As the 5th Generation (5G)<font color="#960096"> mobile </font>networks are bringing about global societal benefits, the design phase for the 6th Generation (6G) has started. 6G will need to enable greater levels of autonomy, improve human machine interfacing, and achieve deep connectivity in more diverse environments. The need for increased explainability to enable trust is critical for 6G as it manages a wide range of mission critical services (e.g. autonomous driving) to safety critical tasks (e.g. remote surgery). As we migrate from traditional model-based optimisation to deep learning, the trust we have in our optimisation modules decrease. This loss of trust means we cannot understand the impact of: 1) poor/bias/malicious data, and 2) neural network design on decisions; nor can we explain to the engineer or the public the network\'s actions. In this review, we outline the core concepts of Explainable Artificial Intelligence (XAI) for 6G, including: public and legal motivations, definitions of explainability, performance vs. explainability trade-offs, methods to improve explainability, and frameworks to incorporate XAI into future wireless systems. Our review is grounded in cases studies for both PHY and MAC layer optimisation, and provide the community with an important research area to embark upon. </br></br>

<a href='http://arxiv.org/pdf/1911.07123.pdf'>1911.07123</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.2832баллов, №624</br>
<b>Graph-Revised Convolutional Network</b></br>
Authors: , Yu, Donghan, Zhang, Ruohong, Jiang, Zhengbao, Wu, Yuexin, Yang, Yiming</br>
  Graph Convolutional Networks (GCNs) have received increasing attention in the machine learning community for effectively leveraging both the content features of nodes and the linkage patterns across graphs in various applications. As <font color="#009600">real-world</font> graphs are often incomplete and noisy, treating them as ground-truth information, which is a common practice in most GCNs, unavoidably leads to sub-optimal solutions. Existing efforts for addressing this problem either involve an over-parameterized model which is difficult to scale, or simply re-weight observed edges without dealing with the missing-edge issue. This paper proposes a novel framework called Graph-Revised Convolutional Network (GRCN), which avoids both extremes. Specifically, a GCN-based graph revision module is introduced for predicting missing edges and revising edge weights w.r.t. downstream tasks via joint optimization. A <font color="blue">theor</font>etical analysis reveals the connection between GRCN and previous work on multigraph belief propagation. Experiments on six benchmark datasets show that GRCN consistently <font color="#00be00">outperform</font>s strong baseline methods by a large margin, especially when the original graphs are severely incomplete or the labeled instances for model training are highly sparse. </br></br>

<a href='http://arxiv.org/pdf/1910.09113.pdf'>1910.09113</a> &nbsp&nbsp (cs:ML, cs:CL, stat:ML) &nbsp&nbsp -0.2837баллов, №625</br>
<b>Discovering the Compositional Structure of Vector Representations with\n  Role Learning Networks</b></br>
Authors: , Soulos, Paul, McCoy, Tom, Linzen, Tal, Smolensky, Paul</br>
  Neural networks are able to perform tasks that rely on compositional structure even though they lack obvious mechanisms for representing this structure. To analyze the internal representations that enable such success, we propose ROLE, a technique that detects whether these representations implicitly encode symbolic structure. ROLE learns to approximate the representations of a target encoder E by learning a symbolic constituent structure and an embedding of that structure into E\'s representational vector space. The constituents of the approximating symbol structure are defined by structural positions - roles - that can be filled by symbols. We show that when E is constructed to explicitly embed a particular type of structure (string or tree), ROLE successfully extracts the ground-truth roles defining that structure. We then analyze a GRU seq2seq network trained to perform a more complex compositional task (SCAN), where there is no ground truth role scheme available. For this model, ROLE successfully discovers an <font color="#be00be">interpret</font>able symbolic structure that the model implicitly uses to perform the SCAN task, providing a comprehensive account of the representations that drive the behavior of a frequently-used but hard-to-interpret type of model. We verify the causal importance of the discovered symbolic structure by showing that, when we systematically manipulate hidden embeddings based on this symbolic structure, the model\'s resulting output is changed in the way predicted by our analysis. Finally, we use ROLE to explore whether popular sentence embedding models are capturing compositional structure and find evidence that they are not; we conclude by suggesting how insights from ROLE can be used to impart new inductive biases to improve the compositional abilities of such models. </br></br>

<a href='http://arxiv.org/pdf/1911.07228.pdf'>1911.07228</a> &nbsp&nbsp (cs:CL, cs:ML, cs:NE) &nbsp&nbsp -0.2880баллов, №626</br>
<b>Error Analysis for Vietnamese Named Entity Recognition on Deep Neural\n  Network Models</b></br>
Authors: , Nguyen, Binh An, Van Nguyen, Kiet, Nguyen, Ngan Luu-Thuy</br>
  In recent years, <font color="#be00be">Vietnamese</font> <font color="blue">Named Entity</font> Recognition (NER) systems have had a great breakthrough when using Deep Neural Network methods. This paper describes the primary errors of the <font color="red">state-of-the-art</font><font color="#be00be"> NER </font>systems on Vietnamese language. After conducting experiments on BLSTM-CNN-CRF and BLSTM-CRF models with different word embeddings on the Vietnamese NER dataset. This dataset is provided by VLSP in 2016 and used to evaluate most of the current Vietnamese NER systems. We noticed that BLSTM-CNN-CRF gives better results, therefore, we analyze the errors on this model in detail. Our error-analysis results provide us thorough insights in order to increase the performance of NER for the Vietnamese language and improve the quality of the corpus in the future works. </br></br>

<a href='http://arxiv.org/pdf/1911.09515.pdf'>1911.09515</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.2902баллов, №627</br>
<b>An analysis of observation length requirements in spoken language for\n  machine understanding of human behaviors</b></br>
Authors: , Chakravarthula, Sandeep Nallan, Baucom, Brian, Narayanan, Shrikanth, Georgiou, Panayiotis</br>
  Automatic quantification of human interaction behaviors based on language information has been shown to be effective in psychotherapy research domains such as marital therapy and <font color="#be00be">cancer</font> care. Existing systems typically use a moving-window approach where the target behavior construct is first quantified based on observations inside a window, such as a fixed number of words or turns, and then integrated over all the windows in that interaction. Given a behavior of interest, it is important to employ the appropriate length of observation, since too short a window might not contain sufficient information. Unfortunately, the link between behavior and observation length for lexical cues has not been well studied and it is not clear how these requirements relate to the characteristics of the target behavior construct. Therefore, in this paper, we investigate how the choice of window length affects the efficacy of language-based behavior quantification, by analyzing (a) the similarity between system predictions and human expert assessments for the same behavior construct and (b) the consistency in relations between predictions of related behavior constructs. We apply our analysis to a large and diverse set of behavior codes that are used to annotate real-life interactions and find that behaviors related to negative affect can be quantified from just a few words whereas those related to positive traits and problem solving require much longer observation windows. On the other hand, constructs that describe dysphoric affect do not appear to be quantifiable from language information alone, regardless of how long they are observed. We compare our findings with related work on behavior quantification based on acoustic vocal cues as well as with prior work on thin slices and human personality predictions and find that, in general, they are in agreement. </br></br>

<a href='http://arxiv.org/pdf/1911.07644.pdf'>1911.07644</a> &nbsp&nbsp (cs:CV, cs:ML, stat:ML) &nbsp&nbsp -0.2913баллов, №628</br>
<b>A Molecular-MNIST Dataset for Machine Learning Study on Diffraction\n  Imaging and Microscopy</b></br>
Authors: , Zhang, Yan, Farrell, Steve, Crowley, Michael, Makowski, Lee, Deslippe, Jack</br>
  An image dataset of 10 different size <font color="#be00be">molecule</font>s, where each molecule has 2,000 structural variants, is generated from the 2D cross-sectional projection of Molecular Dynamics trajectories. The purpose of this dataset is to provide a benchmark dataset for the increasing need of machine learning, deep learning and image processing on the study of scattering, imaging and microscopy. </br></br>

<a href='http://arxiv.org/pdf/1911.07939.pdf'>1911.07939</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.2922баллов, №629</br>
<b>Towards Good Practices for Instance Segmentation</b></br>
Authors: , Yu, Dongdong, Yuan, Zehuan, Liu, Jinlai, Yuan, Kun, Wang, Changhu</br>
  Instance <font color="#be00be">Segmentation</font> is an interesting yet challenging task in computer vision. In this paper, we conduct a series of refinements with the Hybrid Task Cascade (HTC) Network, and empirically evaluate their impact on the final model performance through ablation studies. By taking all the refinements, we achieve 0.47 on the COCO test-dev dataset and 0.47 on the COCO test-challenge dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.07585.pdf'>1911.07585</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.2924баллов, №630</br>
<b>Pattern-based design applied to cultural heritage knowledge graphs</b></br>
Authors: , Carriero, Valentina Anita, Gangemi, Aldo, Mancinelli, Maria Letizia, Nuzzolese, Andrea Giovanni, Presutti, Valentina, Veninata, Chiara</br>
  Ontology Design Patterns (ODPs) have become an established and recognised practice for guaranteeing good quality ontology engineering. There are several ODP repositories where ODPs are shared as well as ontology design methodologies recommending their reuse. Performing rigorous testing is recommended as well for supporting ontology maintenance and validating the resulting resource against its motivating requirements. Nevertheless, it is less than straightforward to find guidelines on how to apply such methodologies for developing domain-specific <font color="#960096">knowledge graph</font>s. ArCo is the knowledge graph of Italian Cultural Heritage and has been developed by using eXtreme Design (XD), an ODP- and test-driven methodology. During its development, XD has been adapted to the need of the CH domain e.g. gathering requirements from an open, diverse community of consumers, a new ODP has been defined and many have been specialised to address specific CH requirements. This paper presents ArCo and describes how to apply XD to the development and validation of a CH knowledge graph, also detailing the (intellectual) process implemented for matching the encountered modelling problems to ODPs. Relevant contributions also include a novel web tool for supporting unit-testing of knowledge graphs, a rigorous evaluation of ArCo, and a discussion of methodological lessons learned during ArCo development. </br></br>

<a href='http://arxiv.org/pdf/1911.08051.pdf'>1911.08051</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -0.2932баллов, №631</br>
<b>SimVAE: Simulator-Assisted Training forInterpretable Generative Models</b></br>
Authors: , Srivastava, Akash, Rosenberg, Jessie, Gutfreund, Dan, Cox, David D.</br>
  This paper presents a simulator-assisted training method (SimVAE) for variational autoencoders (VAE) that leads to a disentangled and <font color="#be00be">interpret</font>able latent space. Training SimVAE is a two-step process in which first a deep generator network(decoder) is trained to approximate the simulator. During this step, the simulator acts as the data source or as a teacher network. Then an inference network (encoder)is trained to invert the decoder. As such, upon complete training, the encoder represents an approximately inverted simulator. By decoupling the training of the encoder and decoder we bypass some of the difficulties that arise in training generative models such as VAEs and generative adversarial networks (GANs). We show applications of our approach in a variety of domains such as circuit design, graphics de-rendering and other natural science problems that involve inference via simulation. </br></br>

<a href='http://arxiv.org/pdf/1911.07690.pdf'>1911.07690</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.2944баллов, №632</br>
<b>Leveraging Decentralized Artificial Intelligence to Enhance Resilience\n  of Energy Networks</b></br>
Authors: , Imteaj, Ahmed, Amini, M. Hadi, Mohammadi, Javad</br>
  This paper reintroduces the notion of resilience in the context of recent issues originated from <font color="#be00be">climate</font> change triggered events including severe hurricanes and wildfires. A recent example is PG&amp;E\'s forced power outage to contain wildfire risk which led to widespread power disruption. This paper focuses on answering two questions: who is responsible for resilience? and how to quantify the monetary value of resilience? To this end, we first provide preliminary definitions of resilience for power systems. We then investigate the role of natural hazards, especially wildfire, on power system resilience. Finally, we will propose a decentralized strategy for a resilient management system using distributed storage and demand response resources. Our proposed high fidelity model provides utilities, operators, and policymakers with a clearer picture for strategic decision making and preventive decisions. </br></br>

<a href='http://arxiv.org/pdf/1911.09143.pdf'>1911.09143</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.2949баллов, №633</br>
<b>ID-aware Quality for Set-based Person Re-identification</b></br>
Authors: , Wang, Xinshao, Kodirov, Elyor, Hua, Yang, Robertson, Neil M.</br>
  Set-based person <font color="blue">re-identification</font> (SReID) is a matching problem that aims to verify whether two sets are of the same identity (ID). Existing SReID models typically generate a feature representation per image and aggregate them to represent the set as a single embedding. However, they can easily be perturbed by noises--perceptually/semantically low quality images--which are inevitable due to imperfect <font color="#be00be">tracking</font>/detection systems, or overfit to trivial images. In this work, we present a novel and simple solution to this problem based on ID-aware quality that measures the perceptual and semantic quality of images guided by their ID information. Specifically, we propose an ID-aware Embedding that consists of two key components: (1) Feature learning attention that aims to learn robust image embeddings by focusing on \'medium\' hard images. This way it can prevent overfitting to trivial images, and alleviate the influence of <font color="#be00be">outlier</font>s. (2) Feature fusion attention is to fuse image embeddings in the set to obtain the set-level embedding. It ignores noisy information and pays more attention to discriminative images to aggregate more discriminative information. Experimental results on four datasets show that our method <font color="#00be00">outperform</font>s <font color="red">state-of-the-art</font> approaches despite the simplicity of our approach. </br></br>

<a href='http://arxiv.org/pdf/1911.09054.pdf'>1911.09054</a> &nbsp&nbsp (cs:RO, cs:CV) &nbsp&nbsp -0.2949баллов, №634</br>
<b>Robust Lane Marking Detection Algorithm Using Drivable Area Segmentation\n  and Extended SLT</b></br>
Authors: , Ozgunalp, Umar, Fan, Rui, Cheng, Shanshan, Sun, Yuxiang, Zuo, Weixun, Zhu, Yilong, Xue, Bohuan, Zheng, Linwei, Liang, Qing, Liu, Ming</br>
  In this paper, a robust lane detection algorithm is proposed, where the vertical road profile of the road is estimated using dynamic programming from the v-disparity map and, based on the estimated profile, the road area is segmented. Since the lane markings are on the road area and any feature point above the ground will be a noise source for the lane detection, a mask is created for the road area to remove some of the noise for lane detection. The estimated mask is multiplied by the lane feature map in a bird\'s eye view (BEV). The lane feature points are extracted by using an extended version of symmetrical local threshold (SLT), which not only considers dark light dark transition (DLD) of the lane markings, like (SLT), but also considers parallelism on the lane marking borders. The <font color="#be00be">segmentation</font> then uses only the feature points that are on the road area. A maximum of two linear lane markings are detected using an efficient 1D Hough transform. Then, the detected linear lane markings are used to create a region of interest (ROI) for parabolic lane detection. Finally, based on the estimated region of interest, parabolic lane models are fitted using robust fitting. Due to the robust lane feature extraction and road area segmentation, the proposed algorithm robustly detects lane markings and achieves lane marking detection with an accuracy of 91% when tested on a sequence from the KITTI dataset. </br></br>

<a href='http://arxiv.org/pdf/1910.08007.pdf'>1910.08007</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -0.2993баллов, №635</br>
<b>Faster feature selection with a Dropping Forward-Backward algorithm</b></br>
Authors: , Nguyen, Thu</br>
  In this era of big data, feature selection techniques, which have long been proven to simplify the model, makes the model more comprehensible, speed up the process of learning, have become more and more important. Among many developed methods, forward and stepwise feature selection <font color="#be00be">regression</font> remained widely used due to their simplicity and efficiency. However, they all involving rescanning all the un-selected features again and again. Moreover, many times, the backward steps in stepwise deem unnecessary, as we will illustrate in our example. These remarks motivate us to introduce a novel algorithm that may boost the speed up to 65.77% compared to the stepwise procedure while maintaining good performance in terms of the number of selected features and error rates. Also, our experiments illustrate that feature selection procedures may be a better choice for high-dimensional problems where the number of features highly exceeds the number of samples. </br></br>

<a href='http://arxiv.org/pdf/1910.13063.pdf'>1910.13063</a> &nbsp&nbsp (cs:NE) &nbsp&nbsp -0.3001баллов, №636</br>
<b>MaskedNet: The First Hardware Inference Engine Aiming Power Side-Channel\n  Protection</b></br>
Authors: , Dubey, Anuj, Cammarota, Rosario, Aysu, Aydin</br>
  Differential Power Analysis (DPA) has been an active area of research for the past two decades to study the attacks for extracting secret information from cryptographic implementations through power measurements and their defenses. Unfortunately, the research on power side-channels have so far predominantly focused on analyzing implementations of ciphers such as AES, DES, RSA, and recently post-quantum cryptography primitives (e.g., lattices). Meanwhile, machine-learning, and in particular deep-learning applications are becoming ubiquitous with several scenarios where the Machine Learning Models are Intellectual Properties requiring confidentiality. Expanding side-channel analysis to Machine Learning Model extraction, however, is largely unexplored.   This paper expands the DPA framework to neural-network classifiers. First, it shows DPA attacks during inference to extract the secret model parameters such as weights and biases of a neural network. Second, it proposes the $\\textit{first countermeasures}$ against these attacks by augmenting $\\textit{masking}$. The resulting design uses novel masked components such as masked adder trees for fully-connected layers and masked Rectifier Linear Units for activation functions. On a SAKURA-X <font color="#be00be">FPGA</font> board, experiments show that the first-order DPA attacks on the unprotected implementation can succeed with only 200 traces and our protection respectively increases the latency and area-cost by 2.8x and 2.3x. </br></br>

<a href='http://arxiv.org/pdf/1911.08169.pdf'>1911.08169</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.3007баллов, №637</br>
<b>Dense Fusion Classmate Network for Land Cover Classification</b></br>
Authors: , Tian, Chao, Li, Cong, Shi, Jianping</br>
  Recently, FCNs based methods have made great progress in semantic <font color="#be00be">segmentation</font>. Different with ordinary scenes, satellite image owns specific characteristics, which elements always extend to large scope and no regular or clear boundaries. Therefore, effective mid-level structure information extremely missing, precise pixel-level classification becomes tough issues. In this paper, a Dense Fusion Classmate Network (DFCNet) is proposed to adopt in land cover classification. </br></br>

<a href='http://arxiv.org/pdf/1911.07140.pdf'>1911.07140</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.3014баллов, №638</br>
<b>Black-Box Adversarial Attack with Transferable Model-based Embedding</b></br>
Authors: , Huang, Zhichao, Zhang, Tong</br>
  We present a new method for black-box <font color="blue">adversarial att</font>ack. Unlike previous methods that combined transfer-based and scored-based methods by using the gradient or initialization of a surrogate white-box model, this new method tries to learn a low-dimensional embedding using a pretrained model, and then performs efficient search within the embedding space to attack an unknown target network. The method produces adversarial perturbations with high level semantic patterns that are easily transferable. We show that this approach can greatly improve the query efficiency of black-box adversarial attack across different target network architectures. We evaluate our approach on MNIST, ImageNet and <font color="#00be00">Google</font> Cloud Vision API, resulting in a significant reduction on the number of queries. We also attack adversarially defended networks on CIFAR10 and ImageNet, where our method not only reduces the number of queries, but also improves the attack success rate. </br></br>

<a href='http://arxiv.org/pdf/1911.07613.pdf'>1911.07613</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp -0.3035баллов, №639</br>
<b>A Subword Level Language Model for Bangla Language</b></br>
Authors: , Khatun, Aisha, Rahman, Anisur, Chowdhury, Hemayet Ahmed, Islam, Md. Saiful, Tasnim, Ayesha</br>
  Language models are at the core of natural language processing. The ability to represent natural language gives rise to its applications in numerous NLP tasks including text classification, <font color="#be00be">summarization</font>, and translation. Research in this area is very limited in Bangla due to the scarcity of resources, except for some count-based models and very recent neural language models being proposed, which are all based on words and limited in practical tasks due to their high perplexity. This paper attempts to approach this issue of perplexity and proposes a subword level neural language model with the AWD-LSTM architecture and various other techniques suitable for training in Bangla language. The model is trained on a corpus of Bangla newspaper articles of an appreciable size consisting of more than 28.5 million word tokens. The performance comparison with various other models depicts the significant reduction in perplexity the proposed model provides, reaching as low as 39.84, in just 20 epochs. </br></br>

<a href='http://arxiv.org/pdf/1911.09228.pdf'>1911.09228</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.3102баллов, №640</br>
<b>Unsupervised Object Segmentation with Explicit Localization Module</b></br>
Authors: , Liu, Weitang, Wei, Lifeng, Sharpnack, James, Owens, John D.</br>
  In this paper, we propose a novel architecture that iteratively discovers and segments out the objects of a scene based on the image reconstruction quality. Different from other approaches, our model uses an explicit localization module that localizes objects of the scene based on the pixel-level reconstruction qualities at each iteration, where simpler objects tend to be reconstructed better at earlier iterations and thus are segmented out first. We show that our localization module improves the quality of the <font color="#be00be">segmentation</font>, especially on a challenging background. </br></br>

<a href='http://arxiv.org/pdf/1911.09418.pdf'>1911.09418</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.3110баллов, №641</br>
<b>MSD: Multi-Self-Distillation Learning via Multi-classifiers within Deep\n  Neural Networks</b></br>
Authors: , Luan, Yunteng, Zhao, Hanyu, Yang, Zhi, Dai, Yafei</br>
  As the development of neural networks, more and more deep neural networks are adopted in various tasks, such as image classification. However, as the huge computational overhead, these networks could not be applied on<font color="#960096"> mobile </font>devices or other low latency scenes. To address this dilemma, muti-exit convolutional network is proposed to allow faster inference via early exits with the corresponding classifiers. These networks utilize sophisticated designing to increase the early exit accuracy. However, naively training the multi-exit network could hurt the performance (accuracy) of deep neural networks as early-exit classifiers throughout interfere with the feature generation process.   In this paper, we propose a general training framework named multi-self-distillation learning (MSD), which mining knowledge of different classifiers within the same network and boost every classifier accuracy. Our approach can be applied not only to multi-exit networks, but also modern CNNs (e.g., ResNet Series) augmented with additional side branch classifiers. We use sampling-based branch augmentation technique to transform a single-exit network into a multi-exit network. This reduces the gap of capacity between different classifiers, and improves the effectiveness of applying MSD. Our experiments show that MSD improves the accuracy of various networks: enhancing the accuracy of every classifier significantly for existing multi-exit network (MSDNet), improving vanilla single-exit networks with internal classifiers with high accuracy, while also improving the final accuracy. </br></br>

<a href='http://arxiv.org/pdf/1911.06915.pdf'>1911.06915</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.3114баллов, №642</br>
<b>Evaluating robustness of language models for chief complaint extraction\n  from patient-generated text</b></br>
Authors: , Valmianski, Ilya, Goodwin, Caleb, Finn, Ian M., Khan, Naqi, Zisook, Daniel S.</br>
  Automated classification of chief complaints from <font color="blue">patient</font>-generated text is a critical first step in developing scalable platforms to triage patients without human intervention. In this work, we evaluate several approaches to chief complaint classification using a novel Chief Complaint (CC) Dataset that contains ~200,000 patient-generated reasons-for-visit entries mapped to a set of 795 discrete chief complaints. We examine the use of several fine-tuned bidirectional transformer (BERT) models trained on both unrelated texts as well as on the CC dataset. We contrast this performance with a TF-IDF baseline. Our evaluation has three components: (1) a random test hold-out from the original dataset; (2) a &quot;misspelling set,&quot; consisting of a hand-selected subset of the test set, where every entry has at least one misspelling; (3) a separate experimenter-generated free-text set. We find that the TF-IDF model performs significantly better than the strongest BERT-based model on the test (best<font color="#00be00"> BERT </font>PR-AUC $0.3597 \\pm 0.0041$ vs TF-IDF PR-AUC $0.3878 \\pm 0.0148$, $p=7\\cdot 10^{-5}$), and is statistically comparable to the misspelling sets (best BERT PR-AUC $0.2579 \\pm 0.0079$ vs TF-IDF PR-AUC $0.2733 \\pm 0.0130$, $p=0.06$). However, when examining model predictions on experimenter-generated queries, some concerns arise about TF-IDF baseline\'s robustness. Our results suggest that in certain tasks, simple language embedding baselines may be very performant; however, truly understanding their robustness requires further analysis. </br></br>

<a href='http://arxiv.org/pdf/1911.08437.pdf'>1911.08437</a> &nbsp&nbsp (cs:ML, cs:AI, cs:CL, stat:ML) &nbsp&nbsp -0.3122баллов, №643</br>
<b>Towards unstructured mortality prediction with free-text clinical notes</b></br>
Authors: , Hashir, Mohammad, Sawhney, Rapinder</br>
  Healthcare data continues to flourish yet a relatively small portion, mostly structured, is being utilized effectively for predicting <font color="blue">clinic</font>al outcomes. The rich subjective information available in unstructured clinical notes can possibly facilitate higher discrimination but tends to be under-utilized in mortality prediction. This work attempts to assess the gain in performance when multiple notes that have been minimally preprocessed are used as an input for prediction. A <font color="#00be00">hierarchical</font> architecture consisting of both convolutional and recurrent layers is used to concurrently model the different notes compiled in an individual hospital stay. This approach is evaluated on predicting in-hospital mortality on the MIMIC-III dataset. On comparison to approaches utilizing structured data, it achieved higher metrics despite requiring less cleaning and preprocessing. This demonstrates the potential of unstructured data in enhancing mortality prediction and signifies the need to incorporate more raw unstructured data into current clinical prediction methods. </br></br>

<a href='http://arxiv.org/pdf/1911.08584.pdf'>1911.08584</a> &nbsp&nbsp (cs:AI, cs:ML, cs:NE) &nbsp&nbsp -0.3122баллов, №644</br>
<b>Neocortical plasticity: an unsupervised cake but no free lunch</b></br>
Authors: , Muller, Eilif B., Beaudoin, Philippe</br>
  The fields of artificial intelligence and neuroscience have a long history of fertile bi-directional interactions. On the one hand, important inspiration for the development of artificial intelligence systems has come from the study of natural systems of intelligence, the mammalian neocortex in particular. On the other, important inspiration for models and <font color="blue">theor</font>ies of the <font color="#00be00">brain</font> have emerged from artificial intelligence research. A central question at the intersection of these two areas is concerned with the processes by which neocortex learns, and the extent to which they are analogous to the back-propagation training algorithm of deep networks. Matching the data efficiency, transfer and generalization properties of neocortical learning remains an area of active research in the field of deep learning. Recent advances in our understanding of neuronal, synaptic and dendritic physiology of the neocortex suggest new approaches for unsupervised representation learning, perhaps through a new class of objective functions, which could act alongside or in lieu of back-propagation. Such local learning rules have implicit rather than explicit objectives with respect to the training data, facilitating domain adaptation and generalization. Incorporating them into deep networks for representation learning could better leverage unlabelled datasets to offer significant improvements in data efficiency of downstream supervised readout learning, and reduce susceptibility to adversarial perturbations, at the cost of a more restricted domain of applicability. </br></br>

<a href='http://arxiv.org/pdf/1911.07919.pdf'>1911.07919</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.3150баллов, №645</br>
<b>ASV: Accelerated Stereo Vision System</b></br>
Authors: , Feng, Yu, Whatmough, Paul, Zhu, Yuhao</br>
  Estimating depth from stereo vision cameras, i.e., &quot;depth from stereo&quot;, is critical to emerging intelligent applications deployed in energy- and performance-constrained devices, such as augmented reality headsets and<font color="#960096"> mobile </font>autonomous robots. While existing stereo vision systems make trade-offs between accuracy, performance and energy-efficiency, we describe ASV, an accelerated stereo vision system that simultaneously improves both performance and energy-efficiency while achieving high accuracy. The key to ASV is to exploit unique characteristics inherent to stereo vision, and apply stereo-specific optimizations, both algorithmically and computationally. We make two contributions. Firstly, we propose a new stereo algorithm, invariant-based stereo matching (ISM), that achieves significant speedup while retaining high accuracy. The algorithm combines classic &quot;hand-crafted&quot; stereo algorithms with recent developments in Deep Neural Networks (DNNs), by leveraging the correspondence invariant unique to stereo vision systems. Secondly, we observe that the bottleneck of the ISM algorithm is the DNN inference, and in particular the deconvolution operations that introduce massive compute-inefficiencies. We propose a set of software optimizations that mitigate these inefficiencies. We show that with less than 0.5% hardware area overhead, these algorithmic and computational optimizations can be effectively integrated within a conventional DNN accelerator. Overall, ASV achieves 5x speedup and 85% energy saving with 0.02% accuracy loss compared to today DNN-based stereo vision systems. </br></br>

<a href='http://arxiv.org/pdf/1911.09652.pdf'>1911.09652</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.3158баллов, №646</br>
<b>Unsupervised Domain Adaptation by Optical Flow Augmentation in Semantic\n  Segmentation</b></br>
Authors: , Azeez, Oluwafemi</br>
  It is expensive to generate real-life image labels and there is a domain gap between real-life and simulated images, hence a model trained on the latter cannot adapt to the former. Solving this can totally eliminate the need for labeling real-life datasets completely. Class balanced self-training is one of the existing techniques that attempt to reduce the domain gap. Moreover, augmenting RGB with flow maps has improved performance in simple semantic <font color="#be00be">segmentation</font> and geometry is preserved across domains. Hence, by augmenting images with dense optical flow map, domain adaptation in semantic segmentation can be improved. </br></br>

<a href='http://arxiv.org/pdf/1911.09158.pdf'>1911.09158</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.3171баллов, №647</br>
<b>Random Fourier Features via Fast Surrogate Leverage Weighted Sampling</b></br>
Authors: , Liu, Fanghui, Huang, Xiaolin, Chen, Yudong, Yang, Jie, Suykens, Johan A. K.</br>
  In this paper, we propose a fast surrogate leverage weighted sampling strategy to generate refined random Fourier features for <font color="blue">kernel</font> approximation. Compared to the current <font color="red">state-of-the-art</font> method that uses the leverage weighted scheme [Li-ICML2019], our new strategy is simpler and more effective. It uses kernel alignment to guide the sampling process and it can avoid the matrix inversion operator when we compute the leverage function. Given n observations and s random features, our strategy can reduce the time complexity from O(ns^2+s^3) to O(ns^2), while achieving comparable (or even slightly better) prediction performance when applied to kernel ridge <font color="#be00be">regression</font> (KRR). In addition, we provide <font color="blue">theor</font>etical guarantees on the generalization performance of our approach, and in particular characterize the number of random features required to achieve statistical guarantees in KRR. Experiments on several benchmark datasets demonstrate that our algorithm achieves comparable prediction performance and takes less time cost when compared to [Li-ICML2019]. </br></br>

<a href='http://arxiv.org/pdf/1911.08568.pdf'>1911.08568</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.3175баллов, №648</br>
<b>Accurate Trajectory Prediction for Autonomous Vehicles</b></br>
Authors: , Diodato, Michael, Li, Yu, Lovjer, Antonia, Yeom, Minsu, Song, Albert, Zeng, Yiyang, Khosla, Abhay, Schifferer, Benedikt, Goyal, Manik, Drori, Iddo</br>
  Predicting vehicle trajectories, angle and speed is important for safe and comfortable driving. We demonstrate the best predicted angle, speed, and best performance overall winning the top three places of the ICCV 2019 Learning to Drive challenge. Our key contributions are (i) a general neural network system architecture which embeds and fuses together multiple inputs by encoding, and decodes multiple outputs using neural networks, (ii) using pre-trained neural networks for augmenting the given input data with <font color="#be00be">segmentation</font> maps and semantic information, and (iii) leveraging the form and distribution of the expected output in the model. </br></br>

<a href='http://arxiv.org/pdf/1911.09105.pdf'>1911.09105</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.3179баллов, №649</br>
<b>On Universal Features for High-Dimensional Learning and Inference</b></br>
Authors: , Huang, Shao-Lun, Makur, Anuran, Wornell, Gregory W., Zheng, Lizhong</br>
  We consider the problem of identifying universal low-dimensional features from high-dimensional data for inference tasks in settings involving learning. For such problems, we introduce natural notions of universality and we show a local equivalence among them. Our analysis is naturally expressed via information geometry, and represents a conceptually and computationally useful analysis. The development reveals the complementary roles of the singular value decomposition, Hirschfeld-Gebelein-R\\\'enyi maximal correlation, the canonical correlation and principle component analyses of Hotelling and Pearson, Tishby\'s information bottleneck, Wyner\'s common information, Ky Fan $k$-norms, and Brieman and Friedman\'s alternating conditional expectations algorithm. We further illustrate how this framework facilitates understanding and optimizing aspects of learning systems, including multinomial logistic (softmax) <font color="#be00be">regression</font> and the associated neural network architecture, matrix factorization methods for collaborative filtering and other applications, rank-constrained multivariate linear regression, and forms of semi-supervised learning. </br></br>

<a href='http://arxiv.org/pdf/1911.07831.pdf'>1911.07831</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.3181баллов, №650</br>
<b>Periodic Spectral Ergodicity: A Complexity Measure for Deep Neural\n  Networks and Neural Architecture Search</b></br>
Authors: , S&#xfc;zen, Mehmet, Cerd&#xe0;, J. J., Weber, Cornelius</br>
  Establishing associations between the structure and the learning ability of deep neural networks (DNNs) is a challenging task in modern machine learning. Producing solutions to this challenge will bring progress both in the <font color="blue">theor</font>etical understanding of DNNs and in building new architectures efficiently. In this work, we address this challenge by developing a new simple complexity measure based on another new measure called Periodic Spectral Ergodicity (PSE) originating from quantum statistical mechanics. Based on this measure a framework is devised in quantifying the complexity of deep neural network from its learned weights and traversing network connectivity in a sequential manner, hence the term cascading PSE (cPSE) as an empirical complexity measure. Because of this cascading approach, i.e., a symmetric divergence of PSE on the consecutive layers, it is possible to use this measure in addition for Neural <font color="#00be00">Architecture Search</font> (NAS). We demonstrate the usefulness of this measure in practice on two sets of vision models, ResNet and VGG and sketch the computation of cPSE for more complex network structures. </br></br>

<a href='http://arxiv.org/pdf/1911.06961.pdf'>1911.06961</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.3191баллов, №651</br>
<b>Towards Automated Sexual Violence Report Tracking</b></br>
Authors: , Hassan, Naeemul, Poudel, Amrit, Hale, Jason, Hubacek, Claire, Huq, Khandakar Tasnim, Santu, Shubhra Kanti Karmaker, Ahmed, Syed Ishtiaque</br>
  <font color="#be00be">Tracking</font> sexual violence is a challenging task. In this paper, we present a supervised learning-based automated sexual violence report tracking model that is more scalable, and reliable than its crowdsource based counterparts. We define the sexual violence report tracking problem by considering victim, perpetrator contexts and the nature of the violence. We find that our model could identify sexual violence reports with a precision and recall of 80.4% and 83.4%, respectively. Moreover, we also applied the model during and after the \\#MeToo movement. Several interesting findings are discovered which are not easily identifiable from a shallow analysis. </br></br>

<a href='http://arxiv.org/pdf/1911.02668.pdf'>1911.02668</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.3209баллов, №652</br>
<b>Certain Answers to a SPARQL Query over a Knowledge Base (extended\n  version)</b></br>
Authors: , Corman, Julien, Xiao, Guohui</br>
  Ontology-Mediated Query Answering (OMQA) is a well-established framework to answer queries over an RDFS or OWL Knowledge Base (KB). OMQA was originally designed for unions of conjunctive queries (UCQs), and based on certain answers. More recently, OMQA has been extended to SPARQL queries, but to our knowledge, none of the efforts made in this direction (either in the literature, or the so-called SPARQL entailment regimes) is able to capture both certain answers for UCQs and the standard <font color="#be00be">interpret</font>ation of SPARQL over a plain graph. We formalize these as requirements to be met by any semantics aiming at conciliating certain answers and SPARQL answers, and define three additional requirements, which generalize to KBs some basic properties of SPARQL answers. Then we show that a semantics can be defined that satisfies all requirements for SPARQL queries with SELECT, UNION, and OPTIONAL, and for DLs with the canonical model property. We also investigate combined complexity for query answering under such a semantics over DL-Lite R KBs. In particular, we show for different fragments of SPARQL that known upper-bounds for query answering over a plain graph are matched. </br></br>

<a href='http://arxiv.org/pdf/1911.08708.pdf'>1911.08708</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.3222баллов, №653</br>
<b>Take an Emotion Walk: Perceiving Emotions from Gaits Using Hierarchical\n  Attention Pooling and Affective Mapping</b></br>
Authors: , Bhattacharya, Uttaran, Roncal, Christian, Mittal, Trisha, Chandra, Rohan, Bera, Aniket, Manocha, Dinesh</br>
  We present an autoencoder-based semi-supervised approach to classify perceived human <font color="#be00be">emotion</font>s from walking <font color="#be00be">style</font>s obtained from videos or from motion-captured data and represented as sequences of 3D poses. Given the motion on each joint in the pose at each time step extracted from 3D pose sequences, we <font color="#00be00">hierarchical</font>ly pool these joint motions in a bottom-up manner in the encoder, following the kinematic chains in the human body. We also constrain the latent embeddings of the encoder to contain the space of psychologically-motivated affective features underlying the gaits. We train the decoder to reconstruct the motions per joint per time step in a top-down manner from the latent embeddings. For the annotated data, we also train a classifier to map the latent embeddings to emotion labels. Our semi-supervised approach achieves a mean average precision of 0.84 on the Emotion-Gait benchmark dataset, which contains gaits collected from multiple sources. We <font color="#00be00">outperform</font> current state-of-art algorithms for both emotion recognition and action recognition from 3D gaits by 7% -- 23% on the absolute. </br></br>

<a href='http://arxiv.org/pdf/1911.08362.pdf'>1911.08362</a> &nbsp&nbsp (cs:ML, cs:AI) &nbsp&nbsp -0.3224баллов, №654</br>
<b>Variance Reduced Advantage Estimation with $\\delta$ Hindsight Credit\n  Assignment</b></br>
Authors: , Young, Kenny</br>
  Hindsight Credit Assignment (HCA) refers to a recently proposed family of methods for producing more efficient credit assignment in <font color="#00be00">reinforcement learning</font>. These methods work by explicitly estimating the probability that certain actions were taken in the past given present information. Prior work has studied the properties of such methods and demonstrated their behaviour empirically. We extend this work by introducing a particular HCA algorithm which has provably lower variance than the conventional Monte-Carlo estimator when the necessary functions can be estimated exactly. This result provides a strong <font color="blue">theor</font>etical basis for how HCA could be broadly useful. </br></br>

<a href='http://arxiv.org/pdf/1911.07192.pdf'>1911.07192</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.3232баллов, №655</br>
<b>Transductive Zero-Shot Hashing for Multi-Label Image Retrieval</b></br>
Authors: , Zou, Qin, Zhang, Zheng, Cao, Ling, Chen, Long, Wang, Song</br>
  Hash coding has been widely used in approximate <font color="#be00be">nearest neighbo</font>r search for large-scale image retrieval. Given semantic annotations such as class labels and pairwise similarities of the training data, hashing methods can learn and generate effective and compact binary codes. While some newly introduced images may contain undefined semantic labels, which we call unseen images, zeor-shot hashing techniques have been studied. However, existing zeor-shot hashing methods focus on the retrieval of single-label images, and cannot handle multi-label images. In this paper, for the first time, a novel transductive <font color="#00be00">zero-shot</font> hashing method is proposed for multi-label unseen image retrieval. In order to predict the labels of the unseen/target data, a visual-semantic bridge is built via instance-concept coherence ranking on the seen/source data. Then, pairwise similarity loss and focal quantization loss are constructed for training a hashing model using both the seen/source and unseen/target data. Extensive evaluations on three popular multi-label datasets demonstrate that, the proposed hashing method achieves significantly better results than the competing methods. </br></br>

<a href='http://arxiv.org/pdf/1911.08953.pdf'>1911.08953</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.3246баллов, №656</br>
<b>MetH: A family of high-resolution and variable-shape image challenges</b></br>
Authors: , Pont, Ferran Par&#xe9;s, Garcia-Gasulla, Dario, Servat, Harald, Labarta, Jes&#xfa;s, Ayguad&#xe9;, Eduard</br>
  High-resolution and variable-shape images have not yet been properly addressed by the AI community. The approach of down-sampling data often used with convolutional neural networks is sub-optimal for many tasks, and has too many drawbacks to be considered a sustainable alternative. In sight of the increasing importance of problems that can benefit from exploiting high-resolution (HR) and variable-shape, and with the goal of promoting research in that direction, we introduce a new family of datasets (MetH). The four proposed problems include two image classification, one image <font color="#be00be">regression</font> and one super resolution task. Each of these datasets contains thousands of art pieces captured by HR and variable-shape images, labeled by experts at the Metropolitan Museum of Art. We perform an analysis, which shows how the proposed tasks go well beyond current public alternatives in both pixel size and aspect ratio variance. At the same time, the performance obtained by popular architectures on these tasks shows that there is ample room for improvement. To wrap up the relevance of the contribution we review the fields, both in AI and high-performance computing, that could benefit from the proposed challenges. </br></br>

<a href='http://arxiv.org/pdf/1911.09461.pdf'>1911.09461</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.3260баллов, №657</br>
<b>JANOS: An Integrated Predictive and Prescriptive Modeling Framework</b></br>
Authors: , Bergman, David, Huang, Teng, Brooks, Philip, Lodi, Andrea, Raghunathan, Arvind U.</br>
  Business research practice is witnessing a surge in the integration of predictive modeling and prescriptive analysis. We describe a modeling framework JANOS that seamlessly integrates the two streams of analytics, for the first time allowing researchers and practitioners to embed machine learning models in an optimization framework. JANOS allows for specifying a prescriptive model using standard optimization modeling elements such as constraints and variables. The key novelty lies in providing modeling constructs that allow for the specification of commonly used predictive models and their features as constraints and variables in the optimization model. The framework considers two sets of decision variables; regular and predicted. The relationship between the regular and the predicted variables are specified by the user as pre-trained predictive models. JANOS currently supports linear <font color="#be00be">regression</font>, logistic regression, and neural network with rectified linear activation functions, but we plan to expand on this set in the future. In this paper, we demonstrate the flexibility of the framework through an example on scholarship allocation in a student enrollment problem and provide a numeric performance evaluation. </br></br>

<a href='http://arxiv.org/pdf/1911.08567.pdf'>1911.08567</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.3267баллов, №658</br>
<b>Multi-domain Conversation Quality Evaluation via User Satisfaction\n  Estimation</b></br>
Authors: , Bodigutla, Praveen Kumar, Polymenakos, Lazaros, Matsoukas, Spyros</br>
  An automated metric to evaluate dialogue quality is vital for optimizing data driven dialogue management. The common approach of relying on explicit user feedback during a conversation is intrusive and sparse. Current models to estimate user satisfaction use limited feature sets and employ annotation schemes with limited generalizability to conversations spanning multiple domains. To address these gaps, we created a new Response Quality annotation scheme, introduced five new domain-independent feature sets and experimented with six machine learning models to estimate User Satisfaction at both turn and dialogue level.   Response Quality ratings achieved significantly high correlation (0.76) with explicit turn-level user ratings. Using the new feature sets we introduced, Gradient Boosting <font color="#be00be">Regression</font> model achieved best (rating [1-5]) prediction performance on 26 seen (linear correlation ~0.79) and one new multi-turn domain (linear correlation 0.67). We observed a 16% relative improvement (68% -&gt; 79%) in binary (&quot;satisfactory/dissatisfactory&quot;) class prediction accuracy of a domain-independent dialogue-level satisfaction estimation model after including predicted turn-level satisfaction ratings as features. </br></br>

<a href='http://arxiv.org/pdf/1911.07956.pdf'>1911.07956</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp -0.3302баллов, №659</br>
<b>Implicit Regularization of Normalization Methods</b></br>
Authors: , Wu, Xiaoxia, Dobriban, Edgar, Ren, Tongzheng, Wu, Shanshan, Li, Zhiyuan, Gunasekar, Suriya, Ward, Rachel, Liu, Qiang</br>
  Normalization methods such as batch normalization are commonly used in overparametrized models like neural networks. Here, we study the weight normalization (WN) method (Salimans &amp; Kingma, 2016) and a variant called reparametrized projected gradient descent (rPGD) for overparametrized least squares <font color="#be00be">regression</font> and some more general loss functions. WN and rPGD reparametrize the weights with a scale $g$ and a unit vector such that the objective function becomes \\emph{non-convex}. We show that this non-convex formulation has beneficial regularization effects compared to gradient descent on the original objective. We show that these methods adaptively regularize the weights and \\emph{converge with exponential rate} to the minimum $\\ell_2$ norm solution (or close to it) even for initializations \\emph{far from zero}. This is different from the behavior of gradient descent, which only converges to the min norm solution when started at zero, and is more sensitive to initialization. Some of our proof techniques are different from many related works; for instance we find explicit invariants along the gradient flow paths. We verify our results experimentally and suggest that there may be a similar phenomenon for nonlinear problems such as matrix sensing. </br></br>

<a href='http://arxiv.org/pdf/1911.09336.pdf'>1911.09336</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.3313баллов, №660</br>
<b>Multi-objective Neural Architecture Search via Predictive Network\n  Performance Optimization</b></br>
Authors: , Shi, Han, Pi, Renjie, Xu, Hang, Li, Zhenguo, Kwok, James T., Zhang, Tong</br>
  Neural <font color="#00be00">Architecture Search</font> (NAS) has shown great potentials in finding a better neural network design than human design. Sample-based NAS is the most fundamental method aiming at exploring the search space and evaluating the most promising architecture. However, few works have focused on improving the sampling efficiency for a multi-objective NAS. Inspired by the nature of the graph structure of a neural network, we propose BOGCN-NAS, a NAS algorithm using <font color="blue">Bayes</font>ian Optimization with Graph Convolutional Network (GCN) predictor. Specifically, we apply GCN as a surrogate model to adaptively discover and incorporate nodes structure to approximate the performance of the architecture. For NAS-oriented tasks, we also design a weighted loss focusing on architectures with high performance. Our method further considers an efficient multi-objective search which can be flexibly injected into any sample-based NAS pipelines to efficiently find the best speed/accuracy trade-off. Extensive experiments are conducted to verify the effectiveness of our method over many competing methods, e.g. 128.4x more efficient than Random Search and 7.8x more efficient than previous SOTA LaNAS for finding the best architecture on the largest NAS dataset NasBench-101. </br></br>

<a href='http://arxiv.org/pdf/1911.08068.pdf'>1911.08068</a> &nbsp&nbsp (cs:ML, cs:AI) &nbsp&nbsp -0.3350баллов, №661</br>
<b>Deep Tile Coder: an Efficient Sparse Representation Learning Approach\n  with applications in Reinforcement Learning</b></br>
Authors: , Pan, Yangchen</br>
  Representation learning is critical to the success of modern large-scale <font color="#00be00">reinforcement learning</font> systems. Previous works show that sparse representation can effectively reduce catastrophic interference and hence provide relatively stable and consistent boostrap targets when training reinforcement learning algorithms. Tile coding is a well-known sparse feature generation method in reinforcement learning. However, its application is largely restricted to small, low dimensional domains, as its computational and memory requirement grows exponentially as dimension increases. This paper proposes a simple and novel tile coding operation---deep tile coder, which adapts tile coding into deep learning setting, and can be easily scaled to high dimensional problems. The key distinction of our method with previous sparse representation learning method is that, we generate sparse feature by construction, while most previous works focus on designing regularization techniques. We are able to <font color="blue">theor</font>etically guarantee sparsity and importantly, our method ensures sparsity from the beginning of learning, without the need of tuning regularization weight. Furthermore, our approach maps from low dimension feature space to high dimension sparse feature space without introducing any additional training parameters. Our empirical demonstration covers classic discrete action control and <font color="#006400">Mujoco</font> continuous robotics control problems. We show that reinforcement learning algorithms equipped with our deep tile coder achieves superior performance. To our best knowledge, our work is the first to demonstrate successful application of sparse representation learning method in online deep reinforcement learning algorithms for challenging tasks without using a target network. </br></br>

<a href='http://arxiv.org/pdf/1911.08139.pdf'>1911.08139</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.3367баллов, №662</br>
<b>MarioNETte: Few-shot Face Reenactment Preserving Identity of Unseen\n  Targets</b></br>
Authors: , Ha, Sungjoo, Kersner, Martin, Kim, Beomsu, Seo, Seokjun, Kim, Dongyoung</br>
  When there is a mismatch between the target identity and the driver identity,<font color="#be00be"> face </font>reenactment suffers severe degradation in the quality of the result, especially in a <font color="#00be00">few-shot</font> setting. The identity preservation problem, where the model loses the detailed information of the target leading to a defective output, is the most common failure mode. The problem has several potential sources such as the identity of the driver leaking due to the identity mismatch, or dealing with unseen large poses. To overcome such problems, we introduce components that address the mentioned problem: image attention block, target feature alignment, and landmark transformer. Through attending and warping the relevant features, the proposed architecture, called MarioNETte, produces high-quality reenactments of unseen identities in a few-shot setting. In addition, the landmark transformer dramatically alleviates the identity preservation problem by isolating the expression geometry through landmark disentanglement. Comprehensive experiments are performed to verify that the proposed framework can generate highly realistic faces, <font color="#00be00">outperform</font>ing all other baselines, even under a significant mismatch of<font color="#be00be"> facial </font>characteristics between the target and the driver. </br></br>

<a href='http://arxiv.org/pdf/1911.07511.pdf'>1911.07511</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -0.3396баллов, №663</br>
<b>Benchmarking time series classification -- Functional data vs machine\n  learning approaches</b></br>
Authors: , Pfisterer, Florian, Beggel, Laura, Sun, Xudong, Scheipl, Fabian, Bischl, Bernd</br>
  Time series classification problems have drawn increasing attention in the machine learning and statistical community. Closely related is the field of functional data analysis (FDA): it refers to the range of problems that deal with the analysis of data that is continuously indexed over some domain. While often employing different methods, both fields strive to answer similar questions, a common example being classification or <font color="#be00be">regression</font> problems with functional covariates. We study methods from functional data analysis, such as functional generalized additive models, as well as functionality to concatenate (functional-) feature extraction or basis representations with traditional machine learning algorithms like support vector machines or classification trees. In order to assess the methods and implementations, we run a benchmark on a wide variety of representative (time series) data sets, with in-depth analysis of empirical results, and strive to provide a reference ranking for which method(s) to use for non-expert practitioners. Additionally, we provide a software framework in R for functional data analysis for supervised learning, including machine learning and more linear approaches from statistics. This allows convenient access, and in connection with the machine-learning toolbox mlr, those methods can now also be tuned and benchmarked. </br></br>

<a href='http://arxiv.org/pdf/1911.08007.pdf'>1911.08007</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.3439баллов, №664</br>
<b>Streetify: Using Street View Imagery And Deep Learning For Urban Streets\n  Development</b></br>
Authors: , Alhasoun, Fahad, Gonzalez, Marta</br>
  The classification of streets on road networks has been focused on the vehicular transportational features of streets such as arterials, major roads, minor roads and so forth based on their transportational use. City authorities on the other hand have been shifting to more urban inclusive planning of streets, encompassing the side use of a street combined with the transportational features of a street. In such classification schemes, streets are labeled for example as commercial throughway, residential neighborhood, park etc. This modern approach to urban planning has been adopted by major cities such as the city of San Francisco, the states of Florida and Pennsylvania among many others. Currently, the process of labeling streets according to their contexts is manual and hence is tedious and time consuming. In this paper, we propose an approach to collect and label imagery data then deploy advancements in computer vision towards modern urban planning. We collect and label street imagery then train deep convolutional neural networks (CNN) to perform the classification of street context. We show that CNN models can perform well achieving accuracies in the 81% to 87%, we then visualize samples from the embedding space of streets using the t-SNE method and apply class activation mapping methods to <font color="#be00be">interpret</font> the features in street imagery contributing to output classification from a model. </br></br>

<a href='http://arxiv.org/pdf/1911.03059.pdf'>1911.03059</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.3450баллов, №665</br>
<b>A Comprehensive Comparison of Machine Learning Based Methods Used in\n  Bengali Question Classification</b></br>
Authors: , Anika, Afra, Rahman, Md. Hasibur, Islam, Salekul, Jameel, Abu Shafin Mohammad Mahdee, Rahman, Chowdhury Rafeed</br>
  QA classification system maps questions asked by humans to an appropriate answer category. A sound question classification (QC) system model is the pre-requisite of a sound QA system. This work demonstrates phases of assembling a QA type classification model. We present a comprehensive comparison (performance and computational complexity) among some machine learning based approaches used in QC for <font color="#be00be">Bengali</font> language. </br></br>

<a href='http://arxiv.org/pdf/1911.07110.pdf'>1911.07110</a> &nbsp&nbsp (cs:ET) &nbsp&nbsp -0.3508баллов, №666</br>
<b>Training DNA Perceptrons via Fractional Coding</b></br>
Authors: , Liu, Xingyi, Parhi, Keshab K.</br>
  This paper describes a novel approach to synthesize molecular reactions to train a perceptron, i.e., a single-layered neural network, with sigmoidal activation function. The approach is based on fractional coding where a variable is represented by two <font color="#be00be">molecule</font>s. The synergy between fractional coding in molecular computing and stochastic logic implementations in electronic computing is key to translating known stochastic logic circuits to molecular computing. In prior work, a DNA perceptron with bipolar inputs and unipolar output was proposed for inference. The focus of this paper is on synthesis of molecular reactions for training of the DNA perceptron. A new molecular scaler that performs multiplication by a factor greater than 1 is proposed based on fractional coding. The training of the perceptron proposed in this paper is based on a modified backpropagation equation as the exact equation cannot be easily mapped to molecular reactions using fractional coding. </br></br>

<a href='http://arxiv.org/pdf/1910.09701.pdf'>1910.09701</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -0.3510баллов, №667</br>
<b>Direct Estimation of Differential Functional Graphical Models</b></br>
Authors: , Zhao, Boxin, Wang, Y. Samuel, Kolar, Mladen</br>
  We consider the problem of estimating the difference between two functional undirected graphical models with shared structures. In many applications, data are naturally regarded as high-dimensional random function vectors rather than multivariate scalars. For example, electroencephalography (EEG) data are more appropriately treated as functions of time. In these problems, not only can the number of functions measured per sample be large, but each function is itself an infinite dimensional object, making estimation of model parameters challenging. We develop a method that directly estimates the difference of graphs, avoiding separate estimation of each graph, and show it is consistent in certain high-dimensional settings. We illustrate finite sample properties of our method through simulation studies. Finally, we apply our method to<font color="#be00be"> EEG </font>data to uncover differences in functional <font color="#00be00">brain</font> connectivity between alcoholics and control subjects. </br></br>

<a href='http://arxiv.org/pdf/1911.07841.pdf'>1911.07841</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.3516баллов, №668</br>
<b>Tigris: Architecture and Algorithms for 3D Perception in Point Clouds</b></br>
Authors: , Xu, Tiancheng, Tian, Boyuan, Zhu, Yuhao</br>
  Machine perception applications are increasingly moving toward manipulating and processing 3D <font color="#be00be">point cloud</font>. This paper focuses on point cloud registration, a key primitive of 3D data processing widely used in high-level tasks such as odometry, simultaneous localization and mapping, and 3D reconstruction. As these applications are routinely deployed in energy-constrained environments, real-time and energy-efficient point cloud registration is critical.   We present Tigris, an algorithm-architecture co-designed system specialized for point cloud registration. Through an extensive exploration of the registration pipeline design space, we find that, while different design points make vastly different trade-offs between accuracy and performance, KD-tree search is a common performance bottleneck, and thus is an ideal candidate for architectural specialization. While KD-tree search is inherently sequential, we propose an acceleration-amenable data structure and search algorithm that exposes different forms of parallelism of KD-tree search in the context of point cloud registration. The co-designed accelerator systematically exploits the parallelism while incorporating a set of architectural techniques that further improve the accelerator efficiency. Overall, Tigris achieves 77.2$\\times$ speedup and 7.4$\\times$ power reduction in KD-tree search over an RTX 2080 Ti GPU, which translates to a 41.7% registration performance improvements and 3.0$\\times$ power reduction. </br></br>

<a href='http://arxiv.org/pdf/1911.09299.pdf'>1911.09299</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.3525баллов, №669</br>
<b>Furnishing Your Room by What You See: An End-to-End Furniture Set\n  Retrieval Framework with Rich Annotated Benchmark Dataset</b></br>
Authors: , Liu, Bingyuan, Zhang, Jiantao, Zhang, Xiaoting, Zhang, Wei, Yu, Chuanhui, Zhou, Yuan</br>
  Understanding interior scenes has attracted enormous interest in computer vision community. However, few works focus on the understanding of furniture within the scenes and a large-scale dataset is also lacked to advance the field. In this paper, we first fill the gap by presenting DeepFurniture, a richly annotated large indoor scene dataset, including 24k indoor images, 170k furniture instances and 20k unique furniture identities. On the dataset, we introduce a new benchmark, named furniture set retrieval. Given an indoor photo as input, the task requires to detect all the furniture instances and search a matched set of furniture identities. To address this challenging task, we propose a feature and context embedding based framework. It contains 3 major contributions: (1) An improved Mask-RCNN model with an additional mask-based classifier is introduced for better utilizing the mask information to relieve the occlusion problems in furniture detection context. (2) A multi-task <font color="#be00be">style</font> Siamese network is proposed to train the feature embedding model for retrieval, which is composed of a classification subnet supervised by self-clustered pseudo attributes and a verification subnet to estimate whether the input pair is matched. (3) In order to model the relationship of the furniture entities in an interior design, a context embedding model is employed to re-rank the retrieval results. Extensive experiments demonstrate the effectiveness of each module and the overall system. </br></br>

<a href='http://arxiv.org/pdf/1911.07685.pdf'>1911.07685</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.3544баллов, №670</br>
<b>Automatic Image Co-Segmentation: A Survey</b></br>
Authors: , Liu, Xiabi, Duan, Xin</br>
  Image co-<font color="#be00be">segmentation</font> is important for its advantage of alleviating the ill-pose nature of image segmentation through exploring the correlation between related images. Many automatic image co-segmentation algorithms have been developed in the last decade, which are investigated comprehensively in this paper. We firstly analyze visual/semantic cues for guiding image co-segmentation, including object cues and correlation cues. Then we describe the traditional methods in three categories of object elements based, object regions/contours based, common object model based. In the next part, deep learning based methods are reviewed. Furthermore, widely used test datasets and evaluation criteria are introduced and the reported performances of the surveyed algorithms are compared with each other. Finally, we discuss the current challenges and possible future directions and conclude the paper. Hopefully, this comprehensive investigation will be helpful for the development of image co-segmentation technique. </br></br>

<a href='http://arxiv.org/pdf/1911.07217.pdf'>1911.07217</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.3602баллов, №671</br>
<b>Real-Time Semantic Segmentation via Multiply Spatial Fusion Network</b></br>
Authors: , Si, Haiyang, Zhang, Zhiqiang, Lv, Feifan, Yu, Gang, Lu, Feng</br>
  Real-time semantic <font color="#be00be">segmentation</font> plays a significant role in industry applications, such as autonomous driving, robotics and so on. It is a challenging task as both efficiency and performance need to be considered simultaneously. To address such a complex task, this paper proposes an efficient CNN called Multiply Spatial Fusion Network (MSFNet) to achieve fast and accurate perception. The proposed MSFNet uses Class Boundary Supervision to process the relevant boundary information based on our proposed Multi-features Fusion Module which can obtain spatial information and enlarge receptive field. Therefore, the final upsampling of the feature maps of 1/8 original image size can achieve impressive results while maintaining a high speed. Experiments on Cityscapes and Camvid datasets show an obvious advantage of the proposed approach compared with the existing approaches. Specifically, it achieves 77.1% Mean IOU on the Cityscapes test dataset with the speed of 41 FPS for a 1024*2048 input, and 75.4% Mean IOU with the speed of 91 FPS on the Camvid test dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.08395.pdf'>1911.08395</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.3625баллов, №672</br>
<b>Towards non-toxic landscapes: Automatic toxic comment detection using\n  DNN</b></br>
Authors: , D\'Sa, Ashwin Geet, Illina, Irina, Fohr, Dominique</br>
  The spectacular expansion of the Internet led to the development of a new research problem in the natural language processing field: automatic toxic comment detection, since many countries prohibit hate speech in public media. There is no clear and formal definition of hate, offensive, toxic and abusive speeches. In this article, we put all these terms under the &quot;umbrella&quot; of toxic speech. The contribution of this paper is the design of binary classification and <font color="#be00be">regression</font>-based approaches aiming to predict whether a comment is toxic or not. We compare different unsupervised word representations and different DNN classifiers. Moreover, we study the robustness of the proposed approaches to <font color="blue">adversarial att</font>acks by adding one (healthy or toxic) word. We evaluate the proposed methodology on the English Wikipedia Detox corpus. Our experiments show that using<font color="#00be00"> BERT </font>fine-tuning <font color="#00be00">outperform</font>s feature-based BERT, Mikolov\'s word embedding or fastText representations with different DNN classifiers. </br></br>

<a href='http://arxiv.org/pdf/1910.13349.pdf'>1910.13349</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.3687баллов, №673</br>
<b>E2-Train: Training State-of-the-art CNNs with Over 80% Energy Savings</b></br>
Authors: , Wang, Yue, Jiang, Ziyu, Chen, Xiaohan, Xu, Pengfei, Zhao, Yang, Lin, Yingyan, Wang, Zhangyang</br>
  Convolutional neural networks (CNNs) have been increasingly deployed to edge devices. Hence, many efforts have been made towards efficient CNN inference in resource-constrained platforms. This paper attempts to explore an orthogonal direction: how to conduct more energy-efficient training of CNNs, so as to enable on-device training. We strive to reduce the energy cost during training, by dropping unnecessary computations from three complementary levels: stochastic mini-batch dropping on the data level; selective layer update on the model level; and sign prediction for low-cost, low-precision back-propagation, on the algorithm level. Extensive simulations and ablation studies, with real energy measurements from an <font color="#be00be">FPGA</font> board, confirm the superiority of our proposed strategies and demonstrate remarkable energy savings for training. For example, when training ResNet-74 on CIFAR-10, we achieve aggressive energy savings of &gt;90% and &gt;60%, while incurring a top-1 accuracy loss of only about 2% and 1.2%, respectively. When training ResNet-110 on CIFAR-100, an over 84% training energy saving is achieved without degrading inference accuracy. </br></br>

<a href='http://arxiv.org/pdf/1911.08591.pdf'>1911.08591</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.3692баллов, №674</br>
<b>Learning Stylized Character Expressions from Humans</b></br>
Authors: , Aneja, Deepali, Colburn, Alex, Faigin, Gary, Shapiro, Linda, Mones, Barbara</br>
  We present DeepExpr, a novel expression transfer system from humans to multiple stylized characters via deep learning. We developed : 1) a data-driven perceptual model of<font color="#be00be"> facial </font>expressions, 2) a novel stylized character data set with cardinal expression annotations : FERG (Facial Expression Research Group) - DB (added two new characters), and 3) . We evaluated our method on a set of retrieval tasks on our collected stylized character dataset of expressions. We have also shown that the ranking order predicted by the proposed features is highly correlated with the ranking order provided by a facial expression expert and Mechanical Turk (MT) experiments. </br></br>

<a href='http://arxiv.org/pdf/1911.07027.pdf'>1911.07027</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp -0.3695баллов, №675</br>
<b>On Value Discrepancy of Imitation Learning</b></br>
Authors: , Xu, Tian, Li, Ziniu, Yu, Yang</br>
  Imitation learning trains a policy from expert demonstrations. Imitation learning approaches have been designed from various principles, such as behavioral cloning via supervised learning, apprenticeship learning via inverse <font color="#00be00">reinforcement learning</font>, and GAIL via generative adversarial learning. In this paper, we propose a framework to analyze the <font color="blue">theor</font>etical property of imitation learning approaches based on discrepancy propagation analysis. Under the infinite-horizon setting, the framework leads to the value discrepancy of behavioral cloning in an order of O((1-\\gamma)^{-2}). We also show that the framework leads to the value discrepancy of GAIL in an order of O((1-\\gamma)^{-1}). It implies that GAIL has less compounding errors than behavioral cloning, which is also verified empirically in this paper. To the best of our knowledge, we are the first one to analyze GAIL\'s performance theoretically. The above results indicate that the proposed framework is a general tool to analyze imitation learning approaches. We hope our theoretical results can provide insights for future improvements in imitation learning algorithms. </br></br>

<a href='http://arxiv.org/pdf/1911.07850.pdf'>1911.07850</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.3707баллов, №676</br>
<b>Frequency Separation for Real-World Super-Resolution</b></br>
Authors: , Fritsche, Manuel, Gu, Shuhang, Timofte, Radu</br>
  Most of the recent literature on image <font color="#be00be">super-resolution</font> (SR) assumes the availability of training data in the form of paired low resolution (LR) and high resolution (HR) images or the knowledge of the downgrading operator (usually bicubic downscaling). While the proposed methods perform well on standard benchmarks, they often fail to produce convincing results in <font color="#009600">real-world</font> settings. This is because real-world images can be subject to corruptions such as sensor noise, which are severely altered by bicubic downscaling. Therefore, the models never see a real-world image during training, which limits their generalization capabilities. Moreover, it is cumbersome to collect paired LR and HR images in the same source domain.   To address this problem, we propose DSGAN to introduce natural image characteristics in bicubically downscaled images. It can be trained in an unsupervised fashion on HR images, thereby generating LR images with the same characteristics as the original images. We then use the generated data to train a SR model, which greatly improves its performance on real-world images. Furthermore, we propose to separate the low and high image frequencies and treat them differently during training. Since the low frequencies are preserved by downsampling operations, we only require adversarial training to modify the high frequencies. This idea is applied to our DSGAN model as well as the SR model. We demonstrate the effectiveness of our method in several experiments through quantitative and qualitative analysis. Our solution is the winner of the AIM Challenge on <font color="#009600">Real World</font> SR at ICCV 2019. </br></br>

<a href='http://arxiv.org/pdf/1911.09001.pdf'>1911.09001</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.3742баллов, №677</br>
<b>Weather event severity prediction using buoy data and machine learning</b></br>
Authors: , Ramachandra, Vikas</br>
  In this paper, we predict severity of extreme <font color="#be00be">weather</font> events (tropical storms, hurricanes, etc.) using buoy data time series variables such as wind speed and air temperature. The prediction/forecasting method is based on various forecasting and machine learning models. The following steps are used. Data sources for the buoys and weather events are identified, aggregated and merged. For missing data imputation, we use Kalman filters as well as splines for multivariate time series. Then, statistical tests are run to ascertain increasing trends in weather event severity. Next, we use machine learning to predict/forecast event severity using buoy variables, and report good accuracies for the models built. </br></br>

<a href='http://arxiv.org/pdf/1911.09298.pdf'>1911.09298</a> &nbsp&nbsp (cs:ML, cs:CV) &nbsp&nbsp -0.3773баллов, №678</br>
<b>Robust Conditional GAN from Uncertainty-Aware Pairwise Comparisons</b></br>
Authors: , Han, Ligong, Gao, Ruijiang, Kim, Mun, Tao, Xin, Liu, Bo, Metaxas, Dimitris</br>
  Conditional generative adversarial networks have shown exceptional generation performance over the past few years. However, they require large numbers of annotations. To address this problem, we propose a novel generative adversarial network utilizing weak supervision in the form of pairwise comparisons (PC-GAN) for image attribute editing. In the light of <font color="blue">Bayes</font>ian uncertainty estimation and noise-tolerant adversarial training, PC-GAN can estimate attribute rating efficiently and demonstrate robust performance in noise resistance. Through extensive experiments, we show both qualitatively and quantitatively that PC-GAN performs comparably with fully-supervised methods and <font color="#00be00">outperform</font>s unsupervised baselines. </br></br>

<a href='http://arxiv.org/pdf/1911.07721.pdf'>1911.07721</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.3781баллов, №679</br>
<b>Program synthesis performance constrained by non-linear spatial\n  relations in Synthetic Visual Reasoning Test</b></br>
Authors: , Yihe, Lu, Lowe, Scott C., Lewis, Penelope A., van Rossum, Mark C. W.</br>
  Despite remarkable advances in automated visual recognition by machines, some visual tasks remain challenging for machines. Fleuret et al. (2011) introduced the Synthetic Visual Reasoning Test (SVRT) to highlight this point, which required classification of images consisting of randomly generated shapes based on hidden abstract rules using only a few examples. Ellis et al. (2015) demonstrated that a program synthesis approach could solve some of the SVRT problems with unsupervised, <font color="#00be00">few-shot</font> learning, whereas they remained challenging for several convolutional neural networks trained with thousands of examples. Here we re-considered the human and machine experiments, because they followed different protocols and yielded different statistics. We thus proposed a quantitative reintepretation of the data between the protocols, so that we could make fair comparison between human and machine performance. We improved the program synthesis classifier by correcting the image <font color="#be00be">parsing</font>s, and compared the results to the performance of other machine agents and human subjects. We grouped the SVRT problems into different types by the two aspects of the core characteristics for classification: shape specification and location relation. We found that the program synthesis classifier could not solve problems involving shape distances, because it relied on symbolic computation which scales poorly with input dimension and adding distances into such computation would increase the dimension combinatorially with the number of shapes in an image. Therefore, although the program synthesis classifier is capable of abstract reasoning, its performance is highly constrained by the accessible information in image parsings. </br></br>

<a href='http://arxiv.org/pdf/1911.08085.pdf'>1911.08085</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.3871баллов, №680</br>
<b>Outlier-Robust High-Dimensional Sparse Estimation via Iterative\n  Filtering</b></br>
Authors: , Diakonikolas, Ilias, Karmalkar, Sushrut, Kane, Daniel, Price, Eric, Stewart, Alistair</br>
  We study high-dimensional sparse estimation tasks in a robust setting where a constant fraction of the dataset is adversarially corrupted. Specifically, we focus on the fundamental problems of robust sparse mean estimation and robust sparse PCA. We give the first practically viable robust estimators for these problems. In more detail, our algorithms are sample and computationally efficient and achieve near-optimal robustness guarantees. In contrast to prior provable algorithms which relied on the ellipsoid method, our algorithms use spectral techniques to iteratively remove <font color="#be00be">outlier</font>s from the dataset. Our experimental evaluation on synthetic data shows that our algorithms are scalable and significantly <font color="#00be00">outperform</font> a range of previous approaches, nearly matching the best error rate without corruptions. </br></br>

<a href='http://arxiv.org/pdf/1911.08382.pdf'>1911.08382</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.3885баллов, №681</br>
<b>A model for predicting price polarity of real estate properties using\n  information of real estate market websites</b></br>
Authors: , Vargas-Calder&#xf3;n, Vladimir, Camargo, Jorge E.</br>
  This paper presents a model that uses the information that sellers publish in real estate <font color="#be00be">market</font> websites to predict whether a property has higher or lower price than the average price of its similar properties. The model learns the correlation between price and information (text descriptions and features) of real estate properties through automatic identification of latent semantic content given by a machine learning model based on doc2vec and xgboost. The proposed model was evaluated with a data set of 57,516 publications of real estate properties collected from 2016 to 2018 of Bogot\\\'a city. Results show that the accuracy of a classifier that involves text descriptions is slightly higher than a classifier that only uses features of the real estate properties, as text descriptions tends to contain detailed information about the property. </br></br>

<a href='http://arxiv.org/pdf/1911.09572.pdf'>1911.09572</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp -0.3898баллов, №682</br>
<b>Automatically Generating Macro Research Reports from a Piece of News</b></br>
Authors: , Hu, Wenxin, Zhang, Xiaofeng, Yang, Gang</br>
  Automatically generating macro research reports from <font color="#be00be">economic</font> news is an important yet challenging task. As we all know, it requires the macro analysts to write such reports within a short period of time after the important economic news are released. This motivates our work, i.e., using AI techniques to save manual cost. The goal of the proposed system is to generate macro research reports as the draft for macro analysts. Essentially, the core challenge is the long text generation issue. To address this issue, we propose a novel deep learning technique based approach which includes two components, i.e., outline generation and macro research report generation.For the model performance evaluation, we first crawl a large news-to-report dataset and then evaluate our approach on this dataset, and the generated reports are given for the subjective evaluation. </br></br>

<a href='http://arxiv.org/pdf/1911.06084.pdf'>1911.06084</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.3946баллов, №683</br>
<b>PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based\n  Attentive Cont-conv Fusion Module</b></br>
Authors: , Xie, Liang, Xiang, Chao, Yu, Zhengxu, Xu, Guodong, Yang, Zheng, Cai, Deng, He, Xiaofei</br>
  <font color="blue">LIDAR</font> <font color="#be00be">point cloud</font>s and RGB-images are both extremely essential for 3D <font color="#be00be">object detection</font>. So many <font color="red">state-of-the-art</font> 3D detection algorithms dedicate in fusing these two types of data effectively. However, their fusion methods based on Birds Eye View (BEV) or voxel format are not accurate. In this paper, we propose a novel fusion approach named Point-based Attentive Cont-conv Fusion(PACF) module, which fuses multi-sensor features directly on 3D points. Except for continuous convolution, we additionally add a Point-Pooling and an Attentive Aggregation to make the fused features more expressive. Moreover, based on the PACF module, we propose a 3D multi-sensor multi-task network called Pointcloud-Image RCNN(PI-RCNN as brief), which handles the image <font color="#be00be">segmentation</font> and 3D object detection tasks. PI-RCNN employs a segmentation sub-network to extract full-resolution semantic feature maps from images and then fuses the multi-sensor features via powerful PACF module. Beneficial from the effectiveness of the PACF module and the expressive semantic features from the segmentation module, PI-RCNN can improve much in 3D object detection. We demonstrate the effectiveness of the PACF module and PI-RCNN on the KITTI 3D Detection benchmark, and our method can achieve state-of-the-art on the metric of 3D AP. </br></br>

<a href='http://arxiv.org/pdf/1910.13824.pdf'>1910.13824</a> &nbsp&nbsp (cs:CV, cs:ML, stat:ML) &nbsp&nbsp -0.3975баллов, №684</br>
<b>Traffic4cast-Traffic Map Movie Forecasting -- Team MIE-Lab</b></br>
Authors: , Martin, Henry, Hong, Ye, Bucher, Dominik, Rupprecht, Christian, Buffat, Ren&#xe9;</br>
  The goal of the IARAI competition traffic4cast was to predict the city-wide traffic status within a 15-minute time window, based on information from the previous hour. The traffic status was given as multi-channel images (one pixel roughly corresponds to 100x100 meters), where one channel indicated the traffic volume, another one the average speed of vehicles, and a third one their rough heading. As part of our work on the competition, we evaluated many different network architectures, analyzed the statistical properties of the given data in detail, and thought about how to transform the problem to be able to take additional spatio-temporal context-information into account, such as the street network, the positions of traffic lights, or the <font color="#be00be">weather</font>. This document summarizes our efforts that led to our best submission, and gives some insights about which other approaches we evaluated, and why they did not work as well as imagined. </br></br>

<a href='http://arxiv.org/pdf/1911.07394.pdf'>1911.07394</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.3993баллов, №685</br>
<b>Strategy Synthesis for Surveillance-Evasion Games with Learning-Enabled\n  Visibility Optimization</b></br>
Authors: , Bharadwaj, Suda, Ly, Louis, Wu, Bo, Tsai, Richard, Topcu, Ufuk</br>
  This paper studies a two-player game with a quantitative <font color="#be00be">surveillance</font> requirement on an adversarial target moving in a discrete state space and a secondary objective to maximize short-term visibility of the environment. We impose the surveillance requirement as a temporal logic constraint.We then use a greedy approach to determine vantage points that optimize a notion of information gain, namely, the number of newly-seen states. By using a convolutional neural network trained on a class of environments, we can efficiently approximate the information gain at each potential vantage point.Subsequent vantage points are chosen such that moving to that location will not jeopardize the surveillance requirement, regardless of any future action chosen by the target. Our method combines guarantees of correctness from formal methods with the scalability of machine learning to provide an efficient approach for surveillance-constrained visibility optimization. </br></br>

<a href='http://arxiv.org/pdf/1911.08201.pdf'>1911.08201</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.4009баллов, №686</br>
<b>Survival and Neural Models for Private Equity Exit Prediction</b></br>
Authors: , Calafiore, Giuseppe C., Morales, Marisa H., Tiozzo, Vittorio, Fracastoro, Giulia, Marquie, Serge</br>
  Within the <font color="#be00be">Private</font> Equity (PE) <font color="#be00be">market</font>, the event of a private company undertaking an Initial Public Offering (IPO) is usually a very high-return one for the investors in the company. For this reason, an effective predictive model for the IPO event is considered as a valuable tool in the PE market, an endeavor in which <font color="#00be00">publicly available</font> quantitative information is generally scarce. In this paper, we describe a data-analytic procedure for predicting the probability with which a company will go public in a given forward period of time. The proposed method is based on the interplay of a neural network (NN) model for estimating the overall event probability, and Survival Analysis (SA) for further modeling the probability of the IPO event in any given interval of time. The proposed neuro-survival model is tuned and tested across nine industrial sectors using real data from the Thomson Reuters Eikon PE database. </br></br>

<a href='http://arxiv.org/pdf/1911.08817.pdf'>1911.08817</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.4041баллов, №687</br>
<b>Black-box Combinatorial Optimization using Models with Integer-valued\n  Minima</b></br>
Authors: , Bliek, Laurens, Verwer, Sicco, de Weerdt, Mathijs</br>
  When a black-box optimization objective can only be evaluated with costly or noisy measurements, most standard optimization algorithms are unsuited to find the optimal solution. Specialized algorithms that deal with exactly this situation make use of surrogate models. These models are usually continuous and smooth, which is beneficial for continuous optimization problems, but not necessarily for combinatorial problems. However, by choosing the basis functions of the surrogate model in a certain way, we show that it can be guaranteed that the optimal solution of the surrogate model is integer. This approach <font color="#00be00">outperform</font>s random search, simulated annealing and one <font color="blue">Bayes</font>ian optimization algorithm on the problem of finding robust routes for a noise-perturbed traveling salesman benchmark problem, with similar performance as another Bayesian optimization algorithm, and outperforms all compared algorithms on a convex binary optimization problem with a large number of variables. </br></br>

<a href='http://arxiv.org/pdf/1911.07405.pdf'>1911.07405</a> &nbsp&nbsp (cs:CL, cs:AI) &nbsp&nbsp -0.4048баллов, №688</br>
<b>Multi-task Sentence Encoding Model for Semantic Retrieval in Question\n  Answering Systems</b></br>
Authors: , Huang, Qiang, Bu, Jianhui, Xie, Weijian, Yang, Shengwen, Wu, Weijia, Liu, Liping</br>
  Question Answering (QA) systems are used to provide proper responses to users\' questions automatically. Sentence matching is an essential task in the QA systems and is usually reformulated as a Paraphrase Identification (PI) problem. Given a question, the aim of the task is to find the most similar question from a QA knowledge base. In this paper, we propose a Multi-task Sentence Encoding Model (MSEM) for the PI problem, wherein a connected graph is employed to depict the relation between sentences, and a multi-task learning model is applied to address both the sentence matching and sentence intent classification problem. In addition, we implement a general semantic retrieval framework that combines our proposed model and the Approximate <font color="#be00be">Nearest Neighbo</font>r (ANN) technology, which enables us to find the most similar question from all available candidates very quickly during online serving. The experiments show the superiority of our proposed method as compared with the existing sentence matching models. </br></br>

<a href='http://arxiv.org/pdf/1910.09600.pdf'>1910.09600</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.4057баллов, №689</br>
<b>Is graph-based feature selection of genes better than random?</b></br>
Authors: , Hashir, Mohammad, Bertin, Paul, Weiss, Martin, Frappier, Vincent, Perkins, Theodore J., Boucher, Genevi&#xe8;ve, Cohen, Joseph Paul</br>
  Gene interaction graphs aim to capture various relationships between genes and represent decades of biology research. When trying to make predictions from genomic data, those graphs could be used to overcome the curse of dimensionality by making machine learning models s<font color="#be00be">parser</font> and more consistent with biological common knowledge. In this work, we focus on assessing whether those graphs capture dependencies seen in gene expression data better than random. We formulate a condition that graphs should satisfy to provide a good prior knowledge and propose to test it using a `Single Gene Inference\' (SGI) task. We compare random graphs with seven major gene interaction graphs published by different research groups, aiming to measure the true benefit of using biologically relevant graphs in this context. Our analysis finds that dependencies can be captured almost as well at random which suggests that, in terms of gene expression levels, the relevant information about the state of the cell is spread across many genes. </br></br>

<a href='http://arxiv.org/pdf/1911.07273.pdf'>1911.07273</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.4063баллов, №690</br>
<b>Distribution Context Aware Loss for Person Re-identification</b></br>
Authors: , Chang, Zhigang, Zhou, Qin, Yu, Mingyang, Zheng, Shibao, Yang, Hua, Wu, Tai-Pang</br>
  To learn the optimal similarity function between probe and gallery images in Person <font color="blue">re-identification</font>, effective deep metric learning methods have been extensively explored to obtain discriminative feature embedding. However, existing metric loss like triplet loss and its variants always emphasize pair-wise relations but ignore the distribution context in feature space, leading to inconsistency and sub-optimal. In fact, the similarity of one pair not only decides the match of this pair, but also has potential impacts on other sample pairs. In this paper, we propose a novel Distribution Context Aware (DCA) loss based on triplet loss to combine both numerical similarity and relation similarity in feature space for better <font color="#be00be">clustering</font>. Extensive experiments on three benchmarks including <font color="#be00be">Market</font>-1501, DukeMTMC-reID and MSMT17, evidence the favorable performance of our method against the corresponding baseline and other <font color="red">state-of-the-art</font> methods. </br></br>

<a href='http://arxiv.org/pdf/1911.08689.pdf'>1911.08689</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp -0.4086баллов, №691</br>
<b>Corruption Robust Exploration in Episodic Reinforcement Learning</b></br>
Authors: , Lykouris, Thodoris, Simchowitz, Max, Slivkins, Aleksandrs, Sun, Wen</br>
  We initiate the study of multi-stage episodic <font color="#00be00">reinforcement learning</font> under adversarial manipulations in both the rewards and the transition probabilities of the underlying system. Existing efficient algorithms heavily rely on the &quot;optimism under uncertainty&quot; principle which dictates their behavior and does not allow flexibility to perform corruption-robust exploration. We address this by (i) departing from the optimistic behavior, and (ii) creating a general framework that incorporates the principle of action-elimination. (This principle has been essential for corruption-robust exploration in multi-armed <font color="blue">bandit</font>s, a degenerate special case of episodic reinforcement learning.) Despite constructing a lower bound for a straightforward implementation of action-elimination, we provide a clean and modular way to transfer it to episodic reinforcement learning. Our algorithm enjoys near-optimal guarantees in the absence of adversarial manipulations, has performance that degrades gracefully as the amount of corruption increases, and does not need to know this amount. Our results shed new light on the broader question of robust exploration, and suggest a way to address a rather daunting mismatch between optimistic algorithms and algorithms with higher flexibility. To demonstrate the applicability of our framework, we provide a second instantiation thereof, showing how it can provide efficient guarantees for the stochastic setting, despite doing almost uniform exploration across plausibly optimal actions. </br></br>

<a href='http://arxiv.org/pdf/1911.08097.pdf'>1911.08097</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.4099баллов, №692</br>
<b>AddNet: Deep Neural Networks Using FPGA-Optimized Multipliers</b></br>
Authors: , Faraone, Julian, Kumm, Martin, Hardieck, Martin, Zipf, Peter, Liu, Xueyuan, Boland, David, Leong, Philip H. W.</br>
  Low-precision arithmetic operations to accelerate deep-learning applications on field-programmable gate arrays (<font color="#be00be">FPGA</font>s) have been studied extensively, because they offer the potential to save silicon area or increase throughput. However, these benefits come at the cost of a decrease in accuracy. In this article, we demonstrate that reconfigurable constant coefficient multipliers (RCCMs) offer a better alternative for saving the silicon area than utilizing low-precision arithmetic. RCCMs multiply input values by a restricted choice of coefficients using only adders, subtractors, bit shifts, and multiplexers (MUXes), meaning that they can be heavily optimized for FPGAs. We propose a family of RCCMs tailored to FPGA logic elements to ensure their efficient utilization. To minimize information loss from quantization, we then develop novel training techniques that map the possible coefficient representations of the RCCMs to neural network weight parameter distributions. This enables the usage of the RCCMs in hardware, while maintaining high accuracy. We demonstrate the benefits of these techniques using AlexNet, ResNet-18, and ResNet-50 networks. The resulting implementations achieve up to 50% resource savings over traditional 8-bit quantized networks, translating to significant speedups and power savings. Our RCCM with the lowest resource requirements exceeds 6-bit fixed point accuracy, while all other implementations with RCCMs achieve at least similar accuracy to an 8-bit uniformly quantized design, while achieving significant resource savings. </br></br>

<a href='http://arxiv.org/pdf/1911.05887.pdf'>1911.05887</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.4124баллов, №693</br>
<b>Revenue Maximization of Airbnb Marketplace using Search Results</b></br>
Authors: , Wen, Jiawei, Vahabi, Hossein, Grbovic, Mihajlo</br>
  Correctly pricing products or services in an online <font color="#be00be">market</font>place presents a challenging problem and one of the critical factors for the success of the business. When users are looking to buy an item they typically search for it. Query relevance models are used at this stage to retrieve and rank the items on the search page from most relevant to least relevant. The presented items are naturally &quot;competing&quot; against each other for user purchases. We provide a practical two-stage model to price this set of retrieved items for which distributions of their values are learned. The initial output of the pricing strategy is a price vector for the top displayed items in one search event. We later aggregate these results over searches to provide the supplier with the optimal price for each item. We applied our solution to large-scale search data obtained from Airbnb Experiences marketplace. Offline evaluation results show that our strategy improves upon baseline pricing strategies on key metrics by at least +20% in terms of booking regret and +55% in terms of revenue potential. </br></br>

<a href='http://arxiv.org/pdf/1911.07412.pdf'>1911.07412</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.4166баллов, №694</br>
<b>Provable Filter Pruning for Efficient Neural Networks</b></br>
Authors: , Liebenwein, Lucas, Baykal, Cenk, Lang, Harry, Feldman, Dan, Rus, Daniela</br>
  We present a provable, sampling-based approach for generating compact Convolutional Neural Networks (CNNs) by identifying and removing redundant filters from an over-parameterized network. Our algorithm uses a small batch of input data points to assign a saliency score to each filter and constructs an importance sampling distribution where filters that highly affect the output are sampled with correspondingly high probability. In contrast to existing filter pruning approaches, our method is simultaneously data-informed, exhibits provable guarantees on the size and performance of the pruned network, and is widely applicable to varying network architectures and data sets. Our analytical bounds bridge the notions of compressibility and importance of network structures, which gives rise to a fully-automated procedure for identifying and preserving filters in layers that are essential to the network\'s performance. Our experimental evaluations on popular architectures and data sets show that our algorithm consistently generates s<font color="#be00be">parser</font> and more efficient models than those constructed by existing filter pruning approaches. </br></br>

<a href='http://arxiv.org/pdf/1911.08111.pdf'>1911.08111</a> &nbsp&nbsp (cs:AI, cs:ML) &nbsp&nbsp -0.4184баллов, №695</br>
<b>Placement Optimization of Aerial Base Stations with Deep Reinforcement\n  Learning</b></br>
Authors: , Qiu, Jin, Lyu, Jiangbin, Fu, Liqun</br>
  Unmanned aerial vehicles (UAVs) can be utilized as aerial base stations (ABSs) to assist terrestrial infrastructure for keeping wireless connectivity in various emergency scenarios. To maximize the coverage rate of N ground users (GUs) by jointly placing multiple ABSs with limited coverage range is known to be a NP-hard problem with exponential complexity in N. The problem is further complicated when the coverage range becomes irregular due to site-specific blockage (e.g., buildings) on the air-ground channel in the 3-dimensional (3D) space. To tackle this challenging problem, this paper applies the Deep <font color="#00be00">Reinforcement Learning</font> (DRL) method by 1) representing the state by a coverage bitmap to capture the spatial correlation of GUs/ABSs, whose dimension and associated neural network complexity is invariant with arbitrarily large N; and 2) designing the action and reward for the DRL agent to effectively learn from the dynamic interactions with the complicated propagation environment represented by a 3D Terrain Map. Specifically, a novel two-level design approach is proposed, consisting of a preliminary design based on the dominant line-of-sight (LoS) channel model, and an advanced design to further refine the ABS positions based on site-specific LoS/non-LoS channel states. The double deep Q-network (DQN) with Prioritized Experience Replay (Prioritized Replay DDQN) algorithm is applied to train the policy of multi-ABS placement decision. Numerical results show that the proposed approach significantly improves the coverage rate in complex environment, compared to the benchmark DQN and <font color="blue">K-mean</font>s algorithms. </br></br>

<a href='http://arxiv.org/pdf/1911.06949.pdf'>1911.06949</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.4288баллов, №696</br>
<b>Distributed Machine Learning through Heterogeneous Edge Systems</b></br>
Authors: , Hu, Hanpeng, Wang, Dan, Wu, Chuan</br>
  Many emerging AI applications request distributed machine learning (ML) among edge systems (e.g., IoT devices and PCs at the edge of the Internet), where data cannot be uploaded to a central venue for model training, due to their large volumes and/or security/<font color="#be00be">privacy</font> concerns. Edge devices are intrinsically heterogeneous in computing capacity, posing significant challenges to parameter synchronization for parallel training with the parameter server (PS) architecture. This paper proposes ADSP, a parameter synchronization scheme for distributed machine learning (ML) with heterogeneous edge systems. Eliminating the significant waiting time occurring with existing parameter synchronization models, the core idea of ADSP is to let faster edge devices continue training, while committing their model updates at strategically decided intervals. We design algorithms that decide time points for each worker to commit its model update, and ensure not only global model convergence but also faster convergence. Our testbed implementation and experiments show that ADSP <font color="#00be00">outperform</font>s existing parameter synchronization models significantly in terms of ML model convergence time, scalability and adaptability to large heterogeneity. </br></br>

<a href='http://arxiv.org/pdf/1911.07845.pdf'>1911.07845</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp -0.4293баллов, №697</br>
<b>Neural Forest Learning</b></br>
Authors: , Cao, Yun-Hao, Wu, Jianxin</br>
  We propose Neural Forest Learning (NFL), a novel deep learning based random-forest-like method. In contrast to previous forest methods, NFL enjoys the benefits of end-to-end, data-driven representation learning, as well as pervasive support from deep learning software and hardware platforms, hence achieving faster inference speed and higher accuracy than previous forest methods. Furthermore, NFL learns non-linear feature representations in CNNs more efficiently than previous higher-order pooling methods, producing good results with negligible increase in parameters, floating point operations (FLOPs) and real running time. We achieve superior performance on 7 machine learning datasets when compared to <font color="blue">random forest</font>s and GBDTs. On the fine-grained benchmarks CUB-200-2011, FGVC-aircraft and Stanford Cars, we achieve over 5.7%, 6.9% and 7.8% gains for VGG-16, respectively. Moreover, NFL can converge in much fewer epochs, further accelerating network training. On the large-scale ImageNet ILSVRC-12 validation set, integration of NFL into ResNet-18 achieves top-1/top-5 errors of 28.32%/9.77%, which <font color="#00be00">outperform</font>s ResNet-18 by 1.92%/1.15% with negligible extra cost and the improvement is consistent under various architectures. </br></br>

<a href='http://arxiv.org/pdf/1911.09646.pdf'>1911.09646</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.4294баллов, №698</br>
<b>Ocular Recognition Databases and Competitions: A Survey</b></br>
Authors: , Zanlorensi, Luiz A., Laroca, Rayson, Luz, Eduardo, Britto Jr., Alceu S., Oliveira, Luiz S., Menotti, David</br>
  The use of the iris and periocular region as biometric traits has been extensively investigated, mainly due to the singularity of the iris features and the use of the periocular region when the image resolution is not sufficient to extract iris information. In addition to providing information about an individual\'s identity, features extracted from these traits can also be explored to obtain other information such as the individual\'s gender, the influence of drug use, the use of contact lenses, <font color="#be00be">spoof</font>ing, among others. This work presents a survey of the databases created for ocular recognition, detailing their protocols and how their images were acquired. We also describe and discuss the most popular ocular recognition competitions (contests), highlighting the submitted algorithms that achieved the best results using only iris trait and also fusing iris and periocular region information. Finally, we describe some relevant works applying deep learning techniques to ocular recognition and point out new challenges and future directions. Considering that there are a large number of ocular databases, and each one is usually designed for a specific problem, we believe this survey can provide a broad overview of the challenges in ocular biometrics. </br></br>

<a href='http://arxiv.org/pdf/1911.07518.pdf'>1911.07518</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.4323баллов, №699</br>
<b>Constructing Multiple Tasks for Augmentation: Improving Neural Image\n  Classification With K-means Features</b></br>
Authors: , Gui, Tao, Qing, Lizhi, Zhang, Qi, Ye, Jiacheng, HangYan, Fei, Zichu, Huang, Xuanjing</br>
  Multi-task learning (MTL) has received considerable attention, and numerous deep learning applications benefit from MTL with multiple objectives. However, constructing multiple related tasks is difficult, and sometimes only a single task is available for training in a dataset. To tackle this problem, we explored the idea of using unsupervised <font color="#be00be">clustering</font> to construct a variety of auxiliary tasks from unlabeled data or existing labeled data. We found that some of these newly constructed tasks could exhibit semantic meanings corresponding to certain human-specific attributes, but some were non-ideal. In order to effectively reduce the impact of non-ideal auxiliary tasks on the main task, we further proposed a novel meta-learning-based multi-task learning approach, which trained the shared hidden layers on auxiliary tasks, while the meta-optimization objective was to minimize the loss on the main task, ensuring that the optimizing direction led to an improvement on the main task. Experimental results across five image datasets demonstrated that the proposed method significantly <font color="#00be00">outperform</font>ed existing single task learning, semi-supervised learning, and some data augmentation methods, including an improvement of more than 9% on the Omniglot dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.09159.pdf'>1911.09159</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -0.4339баллов, №700</br>
<b>Bayesian optimization with local search</b></br>
Authors: , Gao, Yuzhou, Yu, Tengchao, Li, Jinglai</br>
  Global optimization finds applications in a wide range of <font color="#009600">real world</font> problems. The multi-start methods are a popular class of global optimization techniques, which are based on the ideas of conducting local searches at multiple starting points, and then sequentially determine the starting points according to some prescribed rules. In this work we propose a new multi-start algorithm where the starting points are determined in a <font color="blue">Bayes</font>ian optimization framework. Specifically, the method can be understood as to construct a new function by conducting local searches of the original objective function, where the new function attains the same global optima as the original one. Bayesian optimization is then applied to find the global optima of the new local search based function. </br></br>

<a href='http://arxiv.org/pdf/1911.05733.pdf'>1911.05733</a> &nbsp&nbsp (cs:CL, cs:ML, cs:SD) &nbsp&nbsp -0.4340баллов, №701</br>
<b>The phonetic bases of vocal expressed emotion: natural versus acted</b></br>
Authors: , Dhamyal, Hira, Memon, Shahan A., Raj, Bhiksha, Singh, Rita</br>
  Can vocal <font color="#be00be">emotion</font>s be emulated? This question has been a recurrent concern of the speech community, and has also been vigorously investigated. It has been fueled further by its link to the issue of validity of acted emotion databases. Much of the speech and vocal emotion research has relied on acted emotion databases as valid proxies for studying natural emotions. To create models that generalize to natural settings, it is crucial to work with valid prototypes -- ones that can be assumed to reliably represent natural emotions. More concretely, it is important to study emulated emotions against natural emotions in terms of their physiological, and psychological concomitants. In this paper, we present an on-scale systematic study of the differences between natural and acted vocal emotions. We use a self-attention based emotion classification model to understand the phonetic bases of emotions by discovering the most attentive phonemes for each class of emotions. We then compare these attentive phonemes in their importance and distribution across acted and natural classes. Our conclusions show significant differences in the manner and choice of phonemes in acted and natural speech, concluding moderate to low validity and value in using acted speech databases for emotion classification tasks. </br></br>

<a href='http://arxiv.org/pdf/1911.09458.pdf'>1911.09458</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.4352баллов, №702</br>
<b>Observe Before Play: Multi-armed Bandit with Pre-observations</b></br>
Authors: , Zuo, Jinhang, Zhang, Xiaoxi, Joe-Wong, Carlee</br>
  We consider the stochastic multi-armed <font color="blue">bandit</font> (MAB) problem in a setting where a player can pay to pre-observe arm rewards before playing an arm in each round. Apart from the usual trade-off between exploring new arms to find the best one and exploiting the arm believed to offer the highest reward, we encounter an additional dilemma: pre-observing more arms gives a higher chance to play the best one, but incurs a larger cost. For the single-player setting, we design an Observe-Before-Play Upper Confidence Bound (OBP-UCB) algorithm for $K$ arms with Bernoulli rewards, and prove a $T$-round regret upper bound $O(K^2\\log T)$. In the multi-player setting, collisions will occur when players select the same arm to play in the same round. We design a centralized algorithm, C-MP-OBP, and prove its $T$-round regret relative to an offline greedy strategy is upper bounded in $O(\\frac{K^4}{M^2}\\log T)$ for $K$ arms and $M$ players. We also propose distributed versions of the C-MP-OBP policy, called D-MP-OBP and D-MP-Adapt-OBP, achieving logarithmic regret with respect to collision-free target policies. Experiments on synthetic data and wireless channel traces show that C-MP-OBP and D-MP-OBP <font color="#00be00">outperform</font> random heuristics and offline optimal policies that do not allow pre-observations. </br></br>

<a href='http://arxiv.org/pdf/1911.07790.pdf'>1911.07790</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -0.4369баллов, №703</br>
<b>A Simple Heuristic for Bayesian Optimization with A Low Budget</b></br>
Authors: , Nomura, Masahiro, Abe, Kenshi</br>
  The aim of black-box optimization is to optimize an objective function within the constraints of a given evaluation budget. In this problem, it is generally assumed that the computational cost for evaluating a point is large; thus, it is important to search efficiently with as low budget as possible. <font color="blue">Bayes</font>ian optimization is an efficient method for black-box optimization and provides exploration-exploitation trade-off by constructing a surrogate model that considers uncertainty of the objective function. However, because Bayesian optimization should construct the surrogate model for the entire search space, it does not exhibit good performance when points are not sampled sufficiently. In this study, we develop a heuristic method refining the search space for Bayesian optimization when the available evaluation budget is low. The proposed method refines a promising region by dividing the original region so that Bayesian optimization can be executed with the promising region as the initial search space. We confirm that Bayesian optimization with the proposed method <font color="#00be00">outperform</font>s Bayesian optimization alone and shows equal or better performance to two search-space division algorithms through experiments on the benchmark functions and the hyperparameter optimization of machine learning algorithms. </br></br>

<a href='http://arxiv.org/pdf/1911.09401.pdf'>1911.09401</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.4414баллов, №704</br>
<b>Segmenting Medical MRI via Recurrent Decoding Cell</b></br>
Authors: , Wen, Ying, Xie, Kai, He, Lianghua</br>
  The encoder-decoder networks are commonly used in <font color="blue">medic</font>al image <font color="#be00be">segmentation</font> due to their remarkable performance in <font color="#00be00">hierarchical</font> feature fusion. However, the expanding path for feature decoding and spatial recovery does not consider the long-term dependency when fusing feature maps from different layers, and the universal encoder-decoder network does not make full use of the multi-modality information to improve the network robustness especially for segmenting medical MRI. In this paper, we propose a novel feature fusion unit called Recurrent Decoding Cell (RDC) which leverages convolutional RNNs to memorize the long-term context information from the previous layers in the decoding phase. An encoder-decoder network, named Convolutional Recurrent Decoding Network (CRDN), is also proposed based on RDC for segmenting multi-modality medical MRI. CRDN adopts CNN backbone to encode image features and decode them hierarchically through a chain of RDCs to obtain the final high-resolution score map. The evaluation experiments on <font color="#00be00">Brain</font>Web, MRBrainS and HVSMR datasets demonstrate that the introduction of RDC effectively improves the segmentation accuracy as well as reduces the model size, and the proposed CRDN owns its robustness to image noise and intensity non-uniformity in medical MRI. </br></br>

<a href='http://arxiv.org/pdf/1911.08705.pdf'>1911.08705</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.4421баллов, №705</br>
<b>Computer-Aided Clinical Skin Disease Diagnosis Using CNN and Object\n  Detection Models</b></br>
Authors: , He, Xin, Wang, Shihao, Shi, Shaohuai, Tang, Zhenheng, Wang, Yuxin, Zhao, Zhihao, Dai, Jing, Ni, Ronghao, Zhang, Xiaofeng, Liu, Xiaoming, Wu, Zhili, Yu, Wu, Chu, Xiaowen</br>
  Skin <font color="blue">diseas</font>e is one of the most common types of human diseases, which may happen to everyone regardless of age, gender or race. Due to the high visual diversity, human <font color="blue">diagnos</font>is highly relies on personal experience; and there is a serious shortage of experienced dermatologists in many countries. To alleviate this problem, computer-aided diagnosis with <font color="red">state-of-the-art</font> (SOTA) machine learning techniques would be a promising solution. In this paper, we aim at understanding the performance of convolutional neural network (CNN) based approaches. We first build two versions of skin disease datasets from Internet images: (a) Skin-10, which contains 10 common classes of skin disease with a total of 10,218 images; (b) Skin-100, which is a larger dataset that consists of 19,807 images of 100 skin disease classes. Based on these datasets, we benchmark several SOTA CNN models and show that the accuracy of skin-100 is much lower than the accuracy of skin-10. We then implement an ensemble method based on several CNN models and achieve the best accuracy of 79.01\\% for Skin-10 and 53.54\\% for Skin-100. We also present an <font color="#be00be">object detection</font> based approach by introducing bounding boxes into the Skin-10 dataset. Our results show that object detection can help improve the accuracy of some skin disease classes. </br></br>

<a href='http://arxiv.org/pdf/1911.08888.pdf'>1911.08888</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp -0.4462баллов, №706</br>
<b>On using 2D sequence-to-sequence models for speech recognition</b></br>
Authors: , Bahar, Parnia, Zeyer, Albert, Schl&#xfc;ter, Ralf, Ney, Hermann</br>
  Attention-based sequence-to-sequence models have shown promising results in automatic <font color="#be00be">speech recognition</font>. Using these architectures, one-dimensional input and output sequences are related by an attention approach, thereby replacing more explicit alignment processes, like in classical HMM-based modeling. In contrast, here we apply a novel two-dimensional long short-term memory (2DLSTM) architecture to directly model the input/output relation between audio/feature vector sequences and word sequences. The proposed model is an alternative model such that instead of using any type of attention components, we apply a 2DLSTM layer to assimilate the context from both input observations and output transcriptions. The experimental evaluation on the Switchboard 300h automatic speech recognition task shows word error rates for the 2DLSTM model that are <font color="#960096">competitive</font> to end-to-end attention-based model. </br></br>

<a href='http://arxiv.org/pdf/1911.05268.pdf'>1911.05268</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp -0.4483баллов, №707</br>
<b>Adversarial Examples in Modern Machine Learning: A Review</b></br>
Authors: , Wiyatno, Rey Reza, Xu, Anqi, Dia, Ousmane, de Berker, Archy</br>
  Recent research has found that many families of machine learning models are vulnerable to adversarial examples: inputs that are specifically designed to cause the target model to produce erroneous outputs. In this survey, we focus on machine learning models in the visual domain, where methods for generating and detecting such examples have been most extensively studied. We explore a variety of <font color="blue">adversarial att</font>ack methods that apply to image-space content, <font color="#009600">real world</font> adversarial attacks, adversarial defenses, and the transferability property of adversarial examples. We also discuss strengths and weaknesses of various methods of adversarial attack and defense. Our aim is to provide an extensive coverage of the field, furnishing the reader with an intuitive understanding of the mechanics of adversarial attack and defense mechanisms and enlarging the community of researchers studying this fundamental set of problems. </br></br>

<a href='http://arxiv.org/pdf/1911.08200.pdf'>1911.08200</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.4514баллов, №708</br>
<b>On Performance Estimation in Automatic Algorithm Configuration</b></br>
Authors: , Liu, Shengcai, Tang, Ke, Lei, Yunwen, Yao, Xin</br>
  Over the last decade, research on automated parameter tuning, often referred to as automatic algorithm configuration (AAC), has made significant progress. Although the usefulness of such tools has been widely recognized in <font color="#009600">real world</font> applications, the <font color="blue">theor</font>etical foundations of AAC are still very weak. This paper addresses this gap by studying the performance estimation problem in AAC. More specifically, this paper first proves the universal best performance estimator in a practical setting, and then establishes theoretical bounds on the estimation error, i.e., the difference between the training performance and the true performance for a parameter configuration, considering finite and infinite configuration spaces respectively. These findings were verified in extensive experiments conducted on four algorithm configuration scenarios involving different problem domains. Moreover, insights for enhancing existing AAC methods are also identified. </br></br>

<a href='http://arxiv.org/pdf/1911.08946.pdf'>1911.08946</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.4549баллов, №709</br>
<b>Understanding Troll Writing as a Linguistic Phenomenon</b></br>
Authors: , Monakhov, Sergei</br>
  The current study yielded a number of important findings. We managed to build a neural network that achieved an accuracy score of 91 per cent in classifying troll and genuine tweets. By means of <font color="#be00be">regression</font> analysis, we identified a number of features that make a tweet more susceptible to correct labelling and found that they are inherently present in troll tweets as a special type of discourse. We hypothesised that those features are grounded in the sociolinguistic limitations of troll writing, which can be best described as a combination of two factors: speaking with a purpose and trying to mask the purpose of speaking. Next, we contended that the orthogonal nature of these factors must necessarily result in the skewed distribution of many different language parameters of troll messages. Having chosen as an example distribution of the topics and vocabulary associated with those topics, we showed some very pronounced distributional <font color="#be00be">anomal</font>ies, thus confirming our prediction. </br></br>

<a href='http://arxiv.org/pdf/1911.08723.pdf'>1911.08723</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.4563баллов, №710</br>
<b>Deep Minimax Probability Machine</b></br>
Authors: , He, Lirong, Guo, Ziyi, Huang, Kaizhu, Xu, Zenglin</br>
  Deep neural networks enjoy a powerful representation and have proven effective in a number of applications. However, recent advances show that deep neural networks are vulnerable to <font color="blue">adversarial att</font>acks incurred by the so-called adversarial examples. Although the adversarial example is only slightly different from the input sample, the neural network classifies it as the wrong class. In order to alleviate this problem, we propose the Deep Minimax Probability Machine (DeepMPM), which applies MPM to deep neural networks in an end-to-end fashion. In a worst-case scenario, MPM tries to minimize an upper bound of misclassification probabilities, considering the global information (i.e., mean and covariance information of each class). DeepMPM can be more robust since it learns the worst-case bound on the probability of misclassification of future data. Experiments on two <font color="#009600">real-world</font> datasets can achieve comparable classification performance with CNN, while can be more robust on adversarial attacks. </br></br>

<a href='http://arxiv.org/pdf/1911.03923.pdf'>1911.03923</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp -0.4601баллов, №711</br>
<b>A Dynamic Modelling Framework for Human Hand Gesture Task Recognition</b></br>
Authors: , Masoud, Sara, Chowdhury, Bijoy, Son, Young-Jun, Kubota, Chieri, Tronstad, Russell</br>
  Gesture recognition and hand motion <font color="#be00be">tracking</font> are important tasks in advanced gesture based interaction systems. In this paper, we propose to apply a sliding windows filtering approach to sample the incoming streams of data from data gloves and a decision tree model to recognize the gestures in real time for a manual grafting operation of a vegetable seedling propagation facility. The sequence of these recognized gestures defines the tasks that are taking place, which helps to evaluate individuals\' performances and to identify any bottlenecks in real time. In this work, two pairs of data gloves are utilized, which reports the location of the fingers, hands, and wrists wirelessly (i.e., via Bluetooth). To evaluate the performance of the proposed framework, a preliminary experiment was conducted in multiple lab settings of tomato grafting operations, where multiple subjects wear the data gloves while performing different tasks. Our results show an accuracy of 91% on average, in terms of gesture recognition in real time by employing our proposed framework. </br></br>

<a href='http://arxiv.org/pdf/1911.09447.pdf'>1911.09447</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.4626баллов, №712</br>
<b>S-RASTER: Contraction Clustering for Evolving Data Streams</b></br>
Authors: , Ulm, Gregor, Smith, Simon, Nilsson, Adrian, Gustavsson, Emil, Jirstrand, Mats</br>
  Contraction <font color="#be00be">Clustering</font> (RASTER) is a very fast algorithm for density-based clustering, which requires only a single pass. It can process arbitrary amounts of data in linear time and in constant memory, quickly identifying approximate clusters. It also exhibits good scalability in the presence of multiple CPU cores. Yet, RASTER is limited to batch processing. In contrast, S-RASTER is an adaptation of RASTER to the stream processing paradigm that is able to identify clusters in evolving data streams. This algorithm retains the main benefits of its parent algorithm, i.e. single-pass linear time cost and constant memory requirements for each discrete time step in the sliding window. The sliding window is efficiently pruned, and clustering is still performed in linear time. Like RASTER, S-RASTER trades off an often negligible amount of precision for speed. It is therefore very well suited to <font color="#009600">real-world</font> scenarios where clustering does not happen continually but only periodically. We describe the algorithm, including a discussion of implementation details. </br></br>

<a href='http://arxiv.org/pdf/1911.08966.pdf'>1911.08966</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.4660баллов, №713</br>
<b>Evaluating the Transferability and Adversarial Discrimination of\n  Convolutional Neural Networks for Threat Object Detection and Classification\n  within X-Ray Security Imagery</b></br>
Authors: , Gaus, Yona Falinie A., Bhowmik, Neelanjan, Akcay, Samet, Breckon, Toby P.</br>
  X-ray imagery security screening is essential to maintaining transport security against a varying profile of threat or prohibited items. Particular interest lies in the automatic detection and classification of weapons such as firearms and knives within complex and cluttered X-ray security imagery. Here, we address this problem by exploring various end-to-end <font color="#be00be">object detection</font> Convolutional Neural Network (CNN) architectures. We evaluate several leading variants spanning the Faster R-CNN, Mask R-CNN, and RetinaNet architectures to explore the transferability of such models between varying X-ray scanners with differing imaging geometries, image resolutions and material colour profiles. Whilst the limited availability of X-ray threat imagery can pose a challenge, we employ a transfer learning approach to evaluate whether such inter-scanner generalisation may exist over a multiple class detection problem. Overall, we achieve maximal detection performance using a Faster R-CNN architecture with a ResNet$_{101}$ classification network, obtaining 0.88 and 0.86 of mean Average Precision (mAP) for a three-class and two class item from varying X-ray imaging sources. Our results exhibit a remarkable degree of generalisability in terms of cross-scanner performance (mAP: 0.87, firearm detection: 0.94 AP). In addition, we examine the inherent adversarial discriminative capability of such networks using a specifically generated adversarial dataset for firearms detection - with a variable low false positive, as low as 5%, this shows both the challenge and promise of such threat detection within X-ray security imagery. </br></br>

<a href='http://arxiv.org/pdf/1911.07185.pdf'>1911.07185</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.4676баллов, №714</br>
<b>Towards the Automation of Deep Image Prior</b></br>
Authors: , Zhou, Qianwei, Zhou, Chen, Hu, Haigen, Chen, Yuhang, Chen, Shengyong, Li, Xiaoxin</br>
  Single image inverse problem is a notoriously challenging ill-posed problem that aims to restore the original image from one of its corrupted versions. Recently, this field has been immensely influenced by the emergence of deep-learning techniques. Deep Image Prior (DIP) offers a new approach that forces the recovered image to be synthesized from a given deep architecture. While DIP is quite an effective unsupervised approach, it is deprecated in <font color="#009600">real-world</font> applications because of the requirement of human assistance. In this work, we aim to find the best-recovered image without the assistance of humans by adding a stopping criterion, which will reach maximum when the iteration no longer improves the image quality. More specifically, we propose to add a pseudo noise to the corrupted image and measure the pseudo-noise component in the recovered image by the orthogonality between signal and noise. The accuracy of the orthogonal stopping criterion has been demonstrated for several tested problems such as denoising, <font color="#be00be">super-resolution</font>, and inpainting, in which 38 out of 40 experiments are higher than 95%. </br></br>

<a href='http://arxiv.org/pdf/1911.07323.pdf'>1911.07323</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.4680баллов, №715</br>
<b>Layer-Dependent Importance Sampling for Training Deep and Large Graph\n  Convolutional Networks</b></br>
Authors: , Zou, Difan, Hu, Ziniu, Wang, Yewen, Jiang, Song, Sun, Yizhou, Gu, Quanquan</br>
  Graph convolutional networks (GCNs) have recently received wide attentions, due to their successful applications in different graph tasks and different domains. Training GCNs for a large graph, however, is still a challenge. Original full-batch GCN training requires calculating the representation of all the nodes in the graph per GCN layer, which brings in high computation and memory costs. To alleviate this issue, several sampling-based methods have been proposed to train GCNs on a subset of nodes. Among them, the node-wise neighbor-sampling method recursively samples a fixed number of neighbor nodes, and thus its computation cost suffers from exponential growing neighbor size; while the layer-wise importance-sampling method discards the neighbor-dependent constraints, and thus the nodes sampled across layer suffer from sparse connection problem. To deal with the above two problems, we propose a new effective sampling algorithm called LAyer-Dependent ImportancE Sampling (LADIES). Based on the sampled nodes in the upper layer, LADIES selects their neighborhood nodes, constructs a bipartite subgraph and computes the importance probability accordingly. Then, it samples a fixed number of nodes by the calculated probability, and recursively conducts such procedure per layer to construct the whole computation graph. We prove <font color="blue">theor</font>etically and experimentally, that our proposed sampling algorithm <font color="#00be00">outperform</font>s the previous sampling methods in terms of both time and memory costs. Furthermore, LADIES is shown to have better generalization accuracy than original full-batch GCN, due to its stochastic nature. </br></br>

<a href='http://arxiv.org/pdf/1911.07967.pdf'>1911.07967</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.4705баллов, №716</br>
<b>DLBricks: Composable Benchmark Generation to Reduce Deep Learning\n  Benchmarking Effort on CPUs</b></br>
Authors: , Li, Cheng, Dakkak, Abdul, Xiong, Jinjun, Hwu, Wen-mei</br>
  The past few years have seen a surge of applying Deep Learning (DL) models for a wide array of tasks such as image classification, <font color="#be00be">object detection</font>, machine translation, etc. While DL models provide an opportunity to solve otherwise intractable tasks, their adoption relies on them being optimized to meet latency and resource requirements. Benchmarking is a key step in this process but has been hampered in part due to the lack of representative and up-to-date benchmarking suites. This is exacerbated by the fast-evolving pace of DL models.   This paper proposes DLBricks, a composable benchmark generation design that reduces the effort of developing, maintaining, and running DL benchmarks on CPUs. DLBricks decomposes DL models into a set of unique runnable networks and constructs the original model\'s performance using the performance of the generated benchmarks. DLBricks leverages two key observations: DL layers are the performance building blocks of DL models and layers are extensively repeated within and across DL models. Since benchmarks are generated automatically and the benchmarking time is minimized, DLBricks can keep up-to-date with the latest proposed models, relieving the pressure of selecting representative DL models. Moreover, DLBricks allows users to represent proprietary models within benchmark suites. We evaluate DLBricks using $50$ MXNet models spanning $5$ DL tasks on $4$ representative CPU systems. We show that DLBricks provides an accurate performance estimate for the DL models and reduces the benchmarking time across systems (e.g. within $95\\%$ accuracy and up to $4.4\\times$ benchmarking time speedup on Amazon EC2 c5.xlarge). </br></br>

<a href='http://arxiv.org/pdf/1911.07199.pdf'>1911.07199</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.4720баллов, №717</br>
<b>Rumor Detection on Social Media: Datasets, Methods and Opportunities</b></br>
Authors: , Li, Quanzhi, Zhang, Qiong, Si, Luo, Liu, Yingchi</br>
  Social media platforms have been used for information and news gathering, and they are very valuable in many applications. However, they also lead to the spreading of rumors and <font color="#be00be">fake news</font>. Many efforts have been taken to detect and debunk rumors on social media by analyzing their content and social context using machine learning techniques. This paper gives an overview of the recent studies in the rumor detection field. It provides a comprehensive list of datasets used for rumor detection, and reviews the important studies based on what types of information they exploit and the approaches they take. And more importantly, we also present several new directions for future research. </br></br>

<a href='http://arxiv.org/pdf/1911.08076.pdf'>1911.08076</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.4724баллов, №718</br>
<b>IFQ-Net: Integrated Fixed-point Quantization Networks for Embedded\n  Vision</b></br>
Authors: , Gao, Hongxing, Tao, Wei, Wen, Dongchao, Chen, Tse-Wei, Osa, Kinya, Kato, Masami</br>
  Deploying deep models on embedded devices has been a challenging problem since the great success of deep learning based networks. Fixed-point networks, which represent their data with low bits fixed-point and thus give remarkable savings on memory usage, are generally preferred. Even though current fixed-point networks employ relative low bits (e.g. 8-bits), the memory saving is far from enough for the embedded devices. On the other hand, quantization deep networks, for example XNOR-Net and HWGQNet, quantize the data into 1 or 2 bits resulting in more significant memory savings but still contain lots of floatingpoint data. In this paper, we propose a fixed-point network for embedded vision tasks through converting the floatingpoint data in a quantization network into fixed-point. Furthermore, to overcome the data loss caused by the conversion, we propose to compose floating-point data operations across multiple layers (e.g. convolution, batch normalization and quantization layers) and convert them into fixedpoint. We name the fixed-point network obtained through such integrated conversion as Integrated Fixed-point Quantization Networks (IFQ-Net). We demonstrate that our IFQNet gives 2.16x and 18x more savings on model size and runtime feature map memory respectively with similar accuracy on ImageNet. Furthermore, based on YOLOv2, we design IFQ-Tinier-YOLO<font color="#be00be"> face </font>detector which is a fixed-point network with 256x reduction in model size (246k Bytes) than Tiny-YOLO. We illustrate the promising performance of our face detector in terms of detection rate on Face Detection Data Set and Bencmark (FDDB) and qualitative results of detecting small faces of Wider Face dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.06968.pdf'>1911.06968</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.4776баллов, №719</br>
<b>Defensive Few-shot Adversarial Learning</b></br>
Authors: , Li, Wenbin, Wang, Lei, Zhang, Xingxing, Huo, Jing, Gao, Yang, Luo, Jiebo</br>
  The robustness of deep learning models against <font color="blue">adversarial att</font>acks has received increasing attention in recent years. However, both deep learning and adversarial training rely on the availability of a large amount of labeled data and usually do not generalize well to new, unseen classes when only a few training samples are accessible. To address this problem, we explicitly introduce a new challenging problem -- how to learn a robust deep model with limited training samples per class, called defensive <font color="#00be00">few-shot</font> learning in this paper. Simply employing the existing adversarial training techniques in the literature cannot solve this problem. This is because few-shot learning needs to learn transferable knowledge from disjoint auxiliary data, and thus it is invalid to assume the sample-level distribution consistency between the training and test sets as commonly assumed in existing adversarial training techniques. In this paper, instead of assuming such a distribution consistency, we propose to make this assumption at a task-level in the episodic training paradigm in order to better transfer the defense knowledge. Furthermore, inside each task, we design a task-conditioned distribution constraint to narrow the distribution gap between clean and adversarial examples at a sample-level. These give rise to a novel mechanism called multi-level distribution based adversarial training (MDAT) for learning transferable adversarial defense. In addition, a unified $\\mathcal{F}_{\\beta}$ score is introduced to evaluate different defense methods under the same principle. Extensive experiments demonstrate that MDAT achieves higher effectiveness and robustness over existing alternatives in the few-shot case. </br></br>

<a href='http://arxiv.org/pdf/1911.08555.pdf'>1911.08555</a> &nbsp&nbsp (cs:ET, cs:NE) &nbsp&nbsp -0.4805баллов, №720</br>
<b>Exploiting Oxide Based Resistive RAM Variability for Probabilistic AI\n  Hardware Design</b></br>
Authors: , Malhotra, Akul, Lu, Sen, Yang, Kezhou, Sengupta, Abhronil</br>
  Uncertainty plays a key role in real-time machine learning. As a significant shift from standard deep networks, which does not consider any uncertainty formulation during its training or inference, <font color="blue">Bayes</font>ian deep networks are being currently investigated where the network is envisaged as an ensemble of plausible models learnt by the Bayes\' formulation in response to uncertainties in sensory data. Bayesian deep networks consider each synaptic weight as a sample drawn from a probability distribution with learnt mean and variance. This paper elaborates on a hardware design that exploits cycle-to-cycle variability of oxide based Resistive Random Access Memories (RRAMs) as a means to realize such a probabilistic sampling function, instead of viewing it as a disadvantage. </br></br>

<a href='http://arxiv.org/pdf/1911.06847.pdf'>1911.06847</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.4876баллов, №721</br>
<b>A Sparse Bayesian Deep Learning Approach for Identification of Cascaded\n  Tanks Benchmark</b></br>
Authors: , Zhou, Hongpeng, Ibrahim, Chahine, Pan, Wei</br>
  Nonlinear system identification is important with a wide range of applications. The typical approaches for nonlinear system identification include Volterra series models, nonlinear autoregressive with exogenous inputs models, block-structured models, state-space models and neural network models. Among them, neural networks (NN) is an important black-box method thanks to its universal approximation capability and less dependency on prior information. However, there are several challenges associated with NN. The first one lies in the design of a proper neural network structure. A relatively simple network cannot approximate the feature of the system, while a complex model may lead to overfitting. The second lies in the availability of data for some nonlinear systems. For some systems, it is difficult to collect enough data to train a neural network. This raises the challenge that how to train a neural network for system identification with a small dataset. In addition, if the uncertainty of the NN parameter could be obtained, it would be also beneficial for further analysis. In this paper, we propose a sparse <font color="blue">Bayes</font>ian deep learning approach to address the above problems. Specifically, the Bayesian method can reinforce the regularization on neural networks by introducing introduced sparsity-inducing priors. The Bayesian method can also compute the uncertainty of the NN parameter. An efficient iterative re-weighted algorithm is presented in this paper. We also test the capacity of our method to identify the system on various ratios of the original dataset. The one-step-ahead prediction experiment on Cascaded Tank System shows the effectiveness of our method. Furthermore, we test our algorithm with more challenging simulation experiment on this benchmark, which also <font color="#00be00">outperform</font>s other methods. </br></br>

<a href='http://arxiv.org/pdf/1911.07820.pdf'>1911.07820</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.4898баллов, №722</br>
<b>Coordinate-wise Armijo\'s condition</b></br>
Authors: , Truong, Tuyen Trung</br>
  Let $z=(x,y)$ be coordinates for the product space $\\mathbb{R}^{m_1}\\times \\mathbb{R}^{m_2}$. Let $f:\\mathbb{R}^{m_1}\\times \\mathbb{R}^{m_2}\\rightarrow \\mathbb{R}$ be a $C^1$ function, and $\ abla f=(\\partial _xf,\\partial _yf)$ its gradient. Fix $0&lt;\\alpha &lt;1$. For a point $(x,y) \\in \\mathbb{R}^{m_1}\\times \\mathbb{R}^{m_2}$, a number $\\delta &gt;0$ satisfies Armijo\'s condition at $(x,y)$ if the following inequality holds: \\begin{eqnarray*} f(x-\\delta \\partial _xf,y-\\delta \\partial _yf)-f(x,y)\\leq -\\alpha \\delta (||\\partial _xf||^2+||\\partial _yf||^2). \\end{eqnarray*}   When $f(x,y)=f_1(x)+f_2(y)$ is a coordinate-wise sum map, we propose the following {\\bf coordinate-wise} Armijo\'s condition. Fix again $0&lt;\\alpha &lt;1$. A pair of positive numbers $\\delta _1,\\delta _2&gt;0$ satisfies the coordinate-wise variant of Armijo\'s condition at $(x,y)$ if the following inequality holds: \\begin{eqnarray*} [f_1(x-\\delta _1\ abla f_1(x))+f_2(y-\\delta _2\ abla f_2(y))]-[f_1(x)+f_2(y)]\\leq -\\alpha (\\delta _1||\ abla f_1(x)||^2+\\delta _2||\ abla f_2(y)||^2). \\end{eqnarray*}   We then extend results in our recent previous results, on Back<font color="#be00be">tracking</font> Gradient Descent and some variants, to this setting. We show by an example the advantage of using coordinate-wise Armijo\'s condition over the usual Armijo\'s condition. </br></br>

<a href='http://arxiv.org/pdf/1911.07184.pdf'>1911.07184</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.4916баллов, №723</br>
<b>Multi-Zone Unit for Recurrent Neural Networks</b></br>
Authors: , Meng, Fandong, Zhang, Jinchao, Liu, Yang, Zhou, Jie</br>
  Recurrent neural networks (RNNs) have been widely used to deal with sequence learning problems. The input-dependent transition function, which folds new observations into hidden states to sequentially construct fixed-length representations of arbitrary-length sequences, plays a critical role in RNNs. Based on single space composition, transition functions in existing RNNs often have difficulty in capturing complicated long-range dependencies. In this paper, we introduce a new Multi-zone Unit (MZU) for RNNs. The key idea is to design a transition function that is capable of modeling multiple space composition. The MZU consists of three components: zone generation, zone composition, and zone aggregation. Experimental results on multiple datasets of the character-level language modeling task and the aspect-based <font color="#be00be">sentiment</font> analysis task demonstrate the superiority of the MZU. </br></br>

<a href='http://arxiv.org/pdf/1910.13826.pdf'>1910.13826</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.4952баллов, №724</br>
<b>A Framework for Building Closed-Domain Chat Dialogue Systems</b></br>
Authors: , Nakano, Mikio, Komatani, Kazunori</br>
  This paper presents HRIChat, a framework for developing closed-domain chat dialogue systems. Being able to engage in chat dialogues has been found effective for improving communication between humans and dialogue systems. This paper focuses on closed-domain systems because they would be useful when combined with task-oriented dialogue systems in the same domain. HRIChat enables domain-dependent language understanding so that it can deal well with domain-specific utterances. In addition, HRIChat makes it possible to integrate state transition network-based dialogue management and reaction-based dialogue management. FoodChatbot, which is an application in the food and <font color="#be00be">restaurant</font> domain, has been developed and evaluated through a user study. Its results suggest that reasonably good systems can be developed with HRIChat. This paper also reports lessons learned from the development and evaluation of FoodChatbot. </br></br>

<a href='http://arxiv.org/pdf/1911.08626.pdf'>1911.08626</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.4956баллов, №725</br>
<b>Intermittent Connectivity for Exploration in Communication-Constrained\n  Multi-Agent Systems</b></br>
Authors: , Klaesson, Filip, Nilsson, Petter, Ames, Aaron D., Murray, Richard M.</br>
  Motivated by exploration of communication-constrained underground environments using robot teams, we study the problem of planning for intermittent connectivity in multi-agent systems. We propose a novel concept of information-consistency to handle situations where the plan is not initially known by all agents, and suggest an integer linear program for synthesizing information-consistent plans that also achieve auxiliary goals. Furthermore, inspired by network flow problems we propose a novel way to pose connectivity constraints that scales much better than previous methods. In the second part of the paper we apply these results in an exploration setting, and propose a <font color="#be00be">clustering</font> method that separates a large exploration problem into smaller problems that can be solved independently. We demonstrate how the resulting exploration algorithm is able to coordinate a team of ten agents to explore a large environment. </br></br>

<a href='http://arxiv.org/pdf/1911.08891.pdf'>1911.08891</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp -0.4982баллов, №726</br>
<b>Discovering New Intents via Constrained Deep Adaptive Clustering with\n  Cluster Refinement</b></br>
Authors: , Lin, Ting-En, Xu, Hua, Zhang, Hanlei</br>
  Identifying new user intents is an essential task in the dialogue system. However, it is hard to get satisfying <font color="#be00be">clustering</font> results since the definition of intents is strongly guided by prior knowledge. Existing methods incorporate prior knowledge by intensive feature engineering, which not only leads to overfitting but also makes it sensitive to the number of clusters. In this paper, we propose constrained deep adaptive clustering with cluster refinement (CDAC+), an end-to-end clustering method that can naturally incorporate pairwise constraints as prior knowledge to guide the clustering process. Moreover, we refine the clusters by forcing the model to learn from the high confidence assignments. After eliminating low confidence assignments, our approach is surprisingly insensitive to the number of clusters. Experimental results on the three benchmark datasets show that our method can yield significant improvements over strong baselines. </br></br>

<a href='http://arxiv.org/pdf/1911.08711.pdf'>1911.08711</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.5006баллов, №727</br>
<b>Dual Reconstruction with Densely Connected Residual Network for Single\n  Image Super-Resolution</b></br>
Authors: , Hsu, Chih-Chung, Lin, Chia-Hsiang</br>
  Deep learning-based single image <font color="#be00be">super-resolution</font> enables very fast and high-visual-quality reconstruction. Recently, an enhanced super-resolution based on generative adversarial network (ESRGAN) has achieved excellent performance in terms of both qualitative and quantitative quality of the reconstructed high-resolution image. In this paper, we propose to add one more shortcut between two dense-blocks, as well as add shortcut between two convolution layers inside a dense-block. With this simple strategy of adding more shortcuts in the proposed network, it enables a faster learning process as the gradient information can be back-propagated more easily. Based on the improved ESRGAN, the dual reconstruction is proposed to learn different aspects of the super-resolved image for judiciously enhancing the quality of the reconstructed image. In practice, the super-resolution model is pre-trained solely based on pixel distance, followed by fine-tuning the parameters in the model based on adversarial loss and perceptual loss. Finally, we fuse two different models by weighted-summing their parameters to obtain the final super-resolution model. Experimental results demonstrated that the proposed method achieves excellent performance in the <font color="#009600">real-world</font> image super-resolution challenge. We have also verified that the proposed dual reconstruction does further improve the quality of the reconstructed image in terms of both PSNR and SSIM. </br></br>

<a href='http://arxiv.org/pdf/1911.08799.pdf'>1911.08799</a> &nbsp&nbsp (cs:AI, cs:ML) &nbsp&nbsp -0.5023баллов, №728</br>
<b>Solving Online Threat Screening Games using Constrained Action Space\n  Reinforcement Learning</b></br>
Authors: , Shah, Sanket, Sinha, Arunesh, Varakantham, Pradeep, Perrault, Andrew, Tambe, Milind</br>
  Large-scale screening for potential threats with limited resources and capacity for screening is a problem of interest at airports, seaports, and other ports of entry. Adversaries can observe screening procedures and arrive at a time when there will be gaps in screening due to limited resource capacities. To capture this game between ports and adversaries, this problem has been previously represented as a Stackelberg game, referred to as a Threat Screening Game (TSG). Given the significant complexity associated with solving TSGs and uncertainty in arrivals of <font color="#be00be">customer</font>s, existing work has assumed that screenees arrive and are allocated security resources at the beginning of the time window. In practice, screenees such as airport passengers arrive in bursts correlated with flight time and are not bound by fixed time windows. To address this, we propose an online threat screening model in which screening strategy is determined adaptively as a passenger arrives while satisfying a hard bound on acceptable risk of not screening a threat. To solve the online problem with a hard bound on risk, we formulate it as a <font color="#00be00">Reinforcement Learning</font> (RL) problem with constraints on the action space (hard bound on risk). We provide a novel way to efficiently enforce linear inequality constraints on the action output in Deep Reinforcement Learning. We show that our solution allows us to significantly reduce screenee wait time while guaranteeing a bound on risk. </br></br>

<a href='http://arxiv.org/pdf/1911.08730.pdf'>1911.08730</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.5049баллов, №729</br>
<b>Event-based Object Detection and Tracking for Space Situational\n  Awareness</b></br>
Authors: , Afshar, Saeed, Nicholson, Andrew P, van Schaik, Andre, Cohen, Gregory</br>
  In this work, we present optical space imaging using an unconventional yet promising class of imaging devices known as neuromorphic event-based sensors. These devices, which are modeled on the human retina, do not operate with frames, but rather generate asynchronous streams of events in response to changes in log-illumination at each pixel. These devices are therefore extremely fast, do not have fixed exposure times, allow for imaging whilst the device is moving and enable low power space imaging during daytime as well as night without modification of the sensors. Recorded at multiple remote sites, we present the first event-based space imaging dataset including recordings from multiple event-based sensors from multiple providers, greatly lowering the barrier to entry for other researchers given the scarcity of such sensors and the expertise required to operate them. The dataset contains 236 separate recordings and 572 labeled resident space objects. The event-based imaging paradigm presents unique opportunities and challenges motivating the development of specialized event-based algorithms that can perform tasks such as detection and <font color="#be00be">tracking</font> in an event-based manner. Here we examine a range of such event-based algorithms for detection and tracking. The presented methods are designed specifically for space situational awareness applications and are evaluated in terms of accuracy and speed and suitability for implementation in neuromorphic hardware on remote or space-based imaging platforms. </br></br>

<a href='http://arxiv.org/pdf/1911.07429.pdf'>1911.07429</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.5050баллов, №730</br>
<b>Pairwise Interactive Graph Attention Network for Context-Aware\n  Recommendation</b></br>
Authors: , Liu, Yahui, Shen, Furao, Zhao, Jian</br>
  Context-aware recommender systems (CARS), which consider rich side information to improve <font color="blue">recommendat</font>ion performance, have caught more and more attention in both academia and industry. How to predict user preferences from diverse contextual features is the core of CARS. Several recent models pay attention to user behaviors and use specifically designed structures to extract adaptive user interests from history behaviors. However, few works take item history interactions into consideration, which leads to the insufficiency of item feature representation and item attraction extraction. From these observations, we model the user-item interaction as a dynamic interaction graph (DIG) and proposed a GNN-based model called Pairwise Interactive Graph Attention Network (PIGAT) to capture dynamic user interests and item attractions simultaneously. PIGAT introduces the attention mechanism to consider the importance of each interacted user/item to both the user and the item, which captures user interests, item attractions and their influence on the recommendation context. Moreover, confidence embeddings are applied to interactions to distinguish the confidence of interactions occurring at different times. Then more expressive user/item representations and adaptive interaction features are generated, which benefits the recommendation performance especially when involving long-tail items. We conduct experiments on three <font color="#009600">real-world</font> datasets to demonstrate the effectiveness of PIGAT. </br></br>

<a href='http://arxiv.org/pdf/1911.05227.pdf'>1911.05227</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.5053баллов, №731</br>
<b>Understanding Legged Crawling for Soft-Robotics</b></br>
Authors: , Gamus, Benny, Salem, Lior, Gat, Amir D., Or, Yizhar</br>
  Crawling is a common locomotion mechanism in soft robots and nonskeletal animals. In this work we propose modeling soft-robotic legged locomotion by approximating it with an equivalent articulated robot with elastic joints. For concreteness we study our soft robot with two bending actuators via an articulated three-link model. The solution of statically indeterminate systems with stick-slip contact transitions requires for a novel hybrid-quasitatic analysis. Then, we utilize our analysis to investigate the influence of phase-shifted harmonic inputs on performance of crawling gaits, including sensitivity analysis to friction uncertainties and energetic cost of transport. We achieve optimal values of gait parameters. Finally, we fabricate and test a fluid-driven soft robot. The experiments display remarkable agreement with the <font color="blue">theor</font>etical analysis, proving that our simple model correctly captures and explains the fundamental principles of inchworm crawling and can be applied to other soft-robotic legged robots. </br></br>

<a href='http://arxiv.org/pdf/1911.07682.pdf'>1911.07682</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.5088баллов, №732</br>
<b>A New Ensemble Adversarial Attack Powered by Long-term Gradient Memories</b></br>
Authors: , Che, Zhaohui, Borji, Ali, Zhai, Guangtao, Ling, Suiyi, Li, Jing, Callet, Patrick Le</br>
  Deep neural networks are vulnerable to <font color="blue">adversarial att</font>acks. </br></br>

<a href='http://arxiv.org/pdf/1911.08827.pdf'>1911.08827</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp -0.5089баллов, №733</br>
<b>Zero-Shot Semantic Parsing for Instructions</b></br>
Authors: , Givoli, Ofer, Reichart, Roi</br>
  We consider a <font color="#00be00">zero-shot</font> semantic <font color="#be00be">parsing</font> task: parsing instructions into compositional logical forms, in domains that were not seen during training. We present a new dataset with 1,390 examples from 7 application domains (e.g. a calendar or a file manager), each example consisting of a triplet: (a) the application\'s initial state, (b) an instruction, to be carried out in the context of that state, and (c) the state of the application after carrying out the instruction. We introduce a new training algorithm that aims to train a semantic <font color="#be00be">parser</font> on examples from a set of source domains, so that it can effectively parse instructions from an unknown target domain. We integrate our algorithm into the floating parser of Pasupat and Liang (2015), and further augment the parser with features and a logical form candidate filtering logic, to support zero-shot adaptation. Our experiments with various zero-shot adaptation setups demonstrate substantial performance gains over a non-adapted parser. </br></br>

<a href='http://arxiv.org/pdf/1911.08746.pdf'>1911.08746</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.5095баллов, №734</br>
<b>Path tracking control of self-reconfigurable robot hTetro with four\n  differential drive units</b></br>
Authors: , Shi, Yuyao, Elara, Mohan Rajesh, Le, Anh Vu, Prabakaran, Veerajagadheswar, Wood, Kristin L.</br>
  The research interest in robots with more than one steerable wheel has been increasing over recent years due to their high mobility while having a better payload capacity than systems using omnidirectional wheels. However, with more controllable degrees of freedom, almost all of the platforms include redundancy which leads to a modeling method based on the instantaneous center of rotation. The self-reconfigurable the robotic platform, hTetro, is designed for floor cleaning tasks. It also has four differential-drive units which can steer individually. Differing from most other steerable wheeled<font color="#960096"> mobile </font>robots, the wheel arrangement of this robot changes because of its reconfigurability. In this paper, we proposed a robust path <font color="#be00be">tracking</font> controller that can handle discontinuous trajectories and sudden orientation changes. Singularity problems are resolved on both the mechanical aspect and control aspect. The controller is tested experimentally with the self-reconfigurable robotic platform hTetro, and results are discussed. </br></br>

<a href='http://arxiv.org/pdf/1910.04417.pdf'>1910.04417</a> &nbsp&nbsp (cs:ML, cs:AI, cs:RO, stat:ML) &nbsp&nbsp -0.5118баллов, №735</br>
<b>Imitation Learning from Observations by Minimizing Inverse Dynamics\n  Disagreement</b></br>
Authors: , Yang, Chao, Ma, Xiaojian, Huang, Wenbing, Sun, Fuchun, Liu, Huaping, Huang, Junzhou, Gan, Chuang</br>
  This paper studies Learning from Observations (LfO) for imitation learning with access to state-only demonstrations. In contrast to Learning from Demonstration (LfD) that involves both action and state supervision, LfO is more practical in leveraging previously inapplicable resources (e.g. videos), yet more challenging due to the incomplete expert guidance. In this paper, we investigate LfO and its difference with LfD in both <font color="blue">theor</font>etical and practical perspectives. We first prove that the gap between LfD and LfO actually lies in the disagreement of inverse dynamics models between the imitator and the expert, if following the modeling approach of GAIL. More importantly, the upper bound of this gap is revealed by a negative causal entropy which can be minimized in a model-free way. We term our method as Inverse-Dynamics-Disagreement-Minimization (IDDM) which enhances the conventional LfO method through further bridging the gap to LfD. Considerable empirical results on challenging benchmarks indicate that our method attains consistent improvements over other LfO counterparts. </br></br>

<a href='http://arxiv.org/pdf/1911.07805.pdf'>1911.07805</a> &nbsp&nbsp (cs:ML, cs:NE, stat:ML) &nbsp&nbsp -0.5162баллов, №736</br>
<b>Binary Sine Cosine Algorithms for Feature Selection from Medical Data</b></br>
Authors: , Taghian, Shokooh, Nadimi-Shahraki, Mohammad H.</br>
  A well-constructed classification model highly depends on input feature subsets from a dataset, which may contain redundant, irrelevant, or noisy features. This challenge can be worse while dealing with <font color="blue">medic</font>al datasets. The main aim of feature selection as a pre-processing task is to eliminate these features and select the most effective ones. In the literature, metaheuristic algorithms show a successful performance to find optimal feature subsets. In this paper, two binary metaheuristic algorithms named S-shaped binary Sine Cosine Algorithm (SBSCA) and V-shaped binary Sine Cosine Algorithm (VBSCA) are proposed for feature selection from the medical data. In these algorithms, the search space remains continuous, while a binary position vector is generated by two transfer functions S-shaped and V-shaped for each solution. The proposed algorithms are compared with four latest binary optimization algorithms over five medical datasets from the UCI repository. The experimental results confirm that using both bSCA variants enhance the accuracy of classification on these medical datasets compared to four other algorithms. </br></br>

<a href='http://arxiv.org/pdf/1911.00760.pdf'>1911.00760</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.5222баллов, №737</br>
<b>GRAPHENE: A Precise Biomedical Literature Retrieval Engine with Graph\n  Augmented Deep Learning and External Knowledge Empowerment</b></br>
Authors: , Zhao, Sendong, Su, Chang, Sboner, Andrea, Wang, Fei</br>
  Effective bio<font color="blue">medic</font>al literature retrieval (BLR) plays a central role in precision medicine informatics. In this paper, we propose GRAPHENE, which is a deep learning based framework for precise BLR. GRAPHENE consists of three main different modules 1) graph-augmented document representation learning; 2) query expansion and representation learning and 3) learning to rank biomedical articles. The graph-augmented document representation learning module constructs a document-concept graph containing biomedical concept nodes and document nodes so that global biomedical related concept from external knowledge source can be captured, which is further connected to a BiLSTM so both local and global topics can be explored. Query expansion and representation learning module expands the query with abbreviations and different names, and then builds a CNN-based model to convolve the expanded query and obtain a vector representation for each query. Learning to rank minimizes a ranking loss between biomedical articles with the query to learn the retrieval function. Experimental results on applying our system to TREC Precision Medicine track data are provided to demonstrate its effectiveness. </br></br>

<a href='http://arxiv.org/pdf/1911.08554.pdf'>1911.08554</a> &nbsp&nbsp (cs:CL, cs:AI, cs:ML) &nbsp&nbsp -0.5248баллов, №738</br>
<b>Classification as Decoder: Trading Flexibility for Control in Medical\n  Dialogue</b></br>
Authors: , Shleifer, Sam, Chablani, Manish, Kannan, Anitha, Katariya, Namit, Amatriain, Xavier</br>
  Generative seq2seq dialogue systems are trained to predict the next word in dialogues that have already occurred. They can learn from large unlabeled conversation datasets, build a deeper understanding of conversational context, and generate a wide variety of responses. This flexibility comes at the cost of control, a concerning tradeoff in doctor/<font color="blue">patient</font> interactions. Inaccuracies, typos, or undesirable content in the training data will be reproduced by the model at inference time. We trade a small amount of labeling effort and some loss of response variety in exchange for quality control. More specifically, a pretrained language model encodes the conversational context, and we finetune a classification head to map an encoded conversational context to a response class, where each class is a noisily labeled group of interchangeable responses. Experts can update these exemplar responses over time as best practices change without retraining the classifier or invalidating old training data. Expert evaluation of 775 unseen doctor/patient conversations shows that only 12% of the discriminative model\'s responses are worse than the what the doctor ended up writing, compared to 18% for the generative model. </br></br>

<a href='http://arxiv.org/pdf/1911.08860.pdf'>1911.08860</a> &nbsp&nbsp (cs:CV, cs:RO) &nbsp&nbsp -0.5249баллов, №739</br>
<b>Efficient Derivative Computation for Cumulative B-Splines on Lie Groups</b></br>
Authors: , Sommer, Christiane, Usenko, Vladyslav, Schubert, David, Demmel, Nikolaus, Cremers, Daniel</br>
  Continuous-time trajectory representation has recently gained popularity for tasks where the fusion of high-frame-rate sensors and multiple unsynchronized devices is required. Lie group cumulative B-splines are a popular way of representing continuous trajectories without singularities. They have been used in near real-time SLAM and odometry systems with IMU, <font color="blue">LiDAR</font>, regular, RGB-D and event cameras, as well as for offline calibration. These applications require efficient computation of time derivatives (velocity, acceleration), but all prior works rely on a computationally suboptimal formulation. In this work we present an alternative derivation of time derivatives based on recurrence relations that needs $\\mathcal{O}(k)$ instead of $\\mathcal{O}(k^2)$ matrix operations (for a spline of order $k$) and results in simple and elegant expressions. While producing the same result, the proposed approach significantly speeds up the trajectory optimization and allows for computing simple analytic derivatives with respect to spline knots. The results presented in this paper pave the way for incorporating continuous-time trajectory representations into more applications where real-time performance is required. </br></br>

<a href='http://arxiv.org/pdf/1911.06816.pdf'>1911.06816</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.5256баллов, №740</br>
<b>QC-Automator: Deep Learning-based Automated Quality Control for\n  Diffusion MR Images</b></br>
Authors: , Samani, Zahra Riahi, Alappatt, Jacob Antony, Parker, Drew, Ismail, Abdol Aziz Ould, Verma, Ragini</br>
  Quality assessment of diffusion<font color="#be00be"> MRI </font>(dMRI) data is essential prior to any analysis, so that appropriate pre-processing can be used to improve data quality and ensure that the presence of MRI artifacts do not affect the results of subsequent image analysis. Manual quality assessment of the data is subjective, possibly error-prone, and infeasible, especially considering the growing number of consortium-like studies, underlining the need for automation of the process. In this paper, we have developed a deep-learning-based automated quality control (QC) tool, QC-Automator, for dMRI data, that can handle a variety of artifacts such as motion, multiband interleaving, ghosting, susceptibility, herringbone and chemical shifts. QC-Automator uses convolutional neural networks along with transfer learning to train the automated artifact detection on a labeled dataset of ~332000 slices of dMRI data, from 155 unique subjects and 5 scanners with different dMRI acquisitions, achieving a 98% accuracy in detecting artifacts. The method is fast and paves the way for efficient and effective artifact detection in large datasets. It is also demonstrated to be replicable on other datasets with different acquisition parameters. </br></br>

<a href='http://arxiv.org/pdf/1911.07505.pdf'>1911.07505</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.5299баллов, №741</br>
<b>A Hierarchical Framework to Generate Robust Biped Locomotion Based on\n  Divergent Component of Motion</b></br>
Authors: , Kasaei, Mohammadreza, Lau, Nuno, Pereira, Artur</br>
  Keeping the stability can be counted as the essential ability of a humanoid robot to step out of the laboratory to work in our real environment. Since humanoid robots have similar kinematic to a human, humans expect these robots to be robustly capable of stabilizing even in a challenging situation like while a severe push is applied. This paper presents a robust walking framework which not only takes into account the traditional push recovery approaches (e.g., ankle, hip and step strategies) but also uses the concept of Divergent Component of the Motion (DCM) to adjust next step timing and location. The control core of the proposed framework is composed of a Linear-Quadratic-<font color="blue">Gaussi</font>an (LQG) controller and two proportional controllers. In this framework, the LQG controller tries to track the reference trajectories and the proportional controllers are designed to adjust the next step timing and location that allow the robot to recover from a severe push. The robustness and the performance of the proposed framework have been validated by performing a set of simulations, including walking and push recovery using MATLAB. The simulation results verified that the proposed framework is capable of providing a robust walking even in very challenging situations. </br></br>

<a href='http://arxiv.org/pdf/1911.08024.pdf'>1911.08024</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.5308баллов, №742</br>
<b>A Bias Trick for Centered Robust Principal Component Analysis</b></br>
Authors: , He, Baokun, Wan, Guihong, Schweitzer, Haim</br>
  <font color="#be00be">Outlier</font> based Robust Principal Component Analysis (RPCA) requires centering of the non-outliers. We show a &quot;bias trick&quot; that automatically centers these non-outliers. Using this bias trick we obtain the first RPCA algorithm that is optimal with respect to centering. </br></br>

<a href='http://arxiv.org/pdf/1911.07309.pdf'>1911.07309</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.5341баллов, №743</br>
<b>Coverage Testing of Deep Learning Models using Dataset Characterization</b></br>
Authors: , Mani, Senthil, Sankaran, Anush, Tamilselvam, Srikanth, Sethi, Akshay</br>
  Deep Neural Networks (DNNs), with its promising performance, are being increasingly used in safety critical applications such as autonomous driving, <font color="#be00be">cancer</font> detection, and secure authentication. With growing importance in deep learning, there is a requirement for a more standardized framework to evaluate and test deep learning models. The primary challenge involved in automated generation of extensive test cases are: (i) neural networks are difficult to <font color="#be00be">interpret</font> and debug and (ii) availability of human annotators to generate specialized test points. In this research, we explain the necessity to measure the quality of a dataset and propose a test case generation system guided by the dataset properties. From a testing perspective, four different dataset quality dimensions are proposed: (i) equivalence partitioning, (ii) centroid positioning, (iii) boundary conditioning, and (iv) pair-wise boundary conditioning. The proposed system is evaluated on well known image classification datasets such as MNIST, Fashion-MNIST, CIFAR10, CIFAR100, and SVHN against popular deep learning models such as LeNet, ResNet-20, VGG-19. Further, we conduct various experiments to demonstrate the effectiveness of systematic test case generation system for evaluating deep learning models. </br></br>

<a href='http://arxiv.org/pdf/1911.09189.pdf'>1911.09189</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.5365баллов, №744</br>
<b>Information in Infinite Ensembles of Infinitely-Wide Neural Networks</b></br>
Authors: , Schwartz-Ziv, Ravid, Alemi, Alexander A.</br>
  In this preliminary work, we study the generalization properties of infinite ensembles of infinitely-wide neural networks. Amazingly, this model family admits tractable calculations for many information-<font color="blue">theor</font>etic quantities. We report analytical and empirical investigations in the search for signals that correlate with generalization. </br></br>

<a href='http://arxiv.org/pdf/1911.07891.pdf'>1911.07891</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.5369баллов, №745</br>
<b>Basic Principles of Clustering Methods</b></br>
Authors: , Jung, Alexander, Baranov, Ivan</br>
  <font color="#be00be">Clustering</font> methods group a set of data points into a few coherent groups or clusters of similar data points. As an example, consider clustering pixels in an image (or video) if they belong to the same object. Different clustering methods are obtained by using different notions of similarity and different representations of data points. </br></br>

<a href='http://arxiv.org/pdf/1911.07266.pdf'>1911.07266</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.5375баллов, №746</br>
<b>Prescribed Performance Distance-Based Formation Control of Multi-Agent\n  Systems (Extended Version)</b></br>
Authors: , Mehdifar, Farhad, Bechlioulis, Charalampos P., Hashemzadeh, Farzad, Baradarannia, Mahdi</br>
  This paper presents a novel control protocol for robust distance-based formation control with prescribed performance in which agents are subjected to unknown external disturbances. Connectivity maintenance and collision avoidance among neighboring agents are also handled by the appropriate design of certain performance bounds that constrain the inter-agent distance errors. As an extension to the proposed scheme, distance-based formation centroid maneuvering is also studied for disturbance-free agents, in which the formation centroid tracks a desired time-varying velocity. The proposed control laws are decentralized, in the sense that each agent employs local relative information regarding its neighbors to calculate its control signal. Therefore, the control scheme is implementable on the agents\' local coordinate frames. Using rigid graph <font color="blue">theor</font>y, input-to-state stability, and Lyapunov based analysis, the results are established for minimally and infinitesimally rigid formations in 2-D or 3-D space. Furthermore, it is argued that the proposed approach increases formation robustness against shape distortions and can prevent formation convergence to incorrect shapes, which is likely to happen in conventional distance-based formation control methods. Finally, extensive simulation studies clarify and verify the proposed approach. </br></br>

<a href='http://arxiv.org/pdf/1911.08776.pdf'>1911.08776</a> &nbsp&nbsp (cs:CL, cs:AI, cs:ML) &nbsp&nbsp -0.5380баллов, №747</br>
<b>Joint Embedding Learning of Educational Knowledge Graphs</b></br>
Authors: , Yao, Siyu, Wang, Ruijie, Sun, Shen, Bu, Derui, Liu, Jun</br>
  As an efficient model for knowledge organization, the <font color="#960096">knowledge graph</font> has been widely adopted in several fields, e.g., bio<font color="blue">medic</font>ine, sociology, and education. And there is a steady trend of learning embedding representations of knowledge graphs to facilitate knowledge graph construction and downstream tasks. In general, knowledge graph embedding techniques aim to learn vectorized representations which preserve the structural information of the graph. And conventional embedding learning models rely on structural relationships among entities and relations. However, in educational knowledge graphs, structural relationships are not the focus. Instead, rich literals of the graphs are more valuable. In this paper, we focus on this problem and propose a novel model for embedding learning of educational knowledge graphs. Our model considers both structural and literal information and jointly learns embedding representations. Three experimental graphs were constructed based on an educational knowledge graph which has been applied in <font color="#009600">real-world</font> teaching. We conducted two experiments on the three graphs and other common benchmark graphs. The experimental results proved the effectiveness of our model and its superiority over other baselines when processing educational knowledge graphs. </br></br>

<a href='http://arxiv.org/pdf/1911.08756.pdf'>1911.08756</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp -0.5382баллов, №748</br>
<b>Deep Reinforcement Learning with Explicitly Represented Knowledge and\n  Variable State and Action Spaces</b></br>
Authors: , Janisch, Jarom&#xed;r, Pevn&#xfd;, Tom&#xe1;&#x161;, Lis&#xfd;, Viliam</br>
  We focus on a class of <font color="#009600">real-world</font> domains, where gathering <font color="#00be00">hierarchical</font> knowledge is required to accomplish a task. Many problems can be represented in this manner, such as network penetration testing, targeted advertising or <font color="blue">medic</font>al <font color="blue">diagnos</font>is. In our formalization, the task is to sequentially request pieces of information about a sample to build the knowledge hierarchy and terminate when suitable. Any of the learned pieces of information can be further analyzed, resulting in a complex and variable action space. We present a combination of techniques in which the knowledge hierarchy is explicitly represented and given to a deep <font color="#00be00">reinforcement learning</font> algorithm as its input. To process the hierarchical input, we employ Hierarchical Multiple-Instance Learning and to cope with the complex action space, we factor it with hierarchical softmax. Our end-to-end differentiable model is trained with A2C, a standard deep reinforcement learning algorithm. We demonstrate the method in a set of seven classification domains, where the task is to achieve the best accuracy with a set budget on the amount of information retrieved. Compared to baseline algorithms, our method achieves not only better results, but also better generalization. </br></br>

<a href='http://arxiv.org/pdf/1911.03870.pdf'>1911.03870</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.5437баллов, №749</br>
<b>Synthesis of Feedback Controller for Nonlinear Control Systems with\n  Optimal Region of Attraction</b></br>
Authors: , Chakraborty, Ayan, Saha, Indranil</br>
  The problem of computing and characterizing Region of Attraction (ROA) with its many variations have a long tradition in safety-critical systems and control <font color="blue">theor</font>y. By virtue here comes the connections to Lyapunov functions that are considered as the centerpiece of stability theory for a non linear dynamical systems. The agents may be imperfect because of several limitations in the sensors which ultimately restrict to fully observe the potential adversaries in the environment. Therefore while interacting with human life an autonomous robot should safely explore the outdoor environment by avoiding the dangerous states that may cause physical harm both the systems and environment. In this paper we address this problem and propose a framework of learning policies that adapt to the shape of largest safe region in the state space. At the inception the model is trained to learn an accurate safety certificate for non-linear closed loop dynamics system by constructing Lyapunov Neural Network. The current work is also an extension of the previous work of computing ROA under a fixed policy. Specifically we discuss how to design a state feedback controller by using a typical kind of performance objective function to be optimized and demonstrates our method on a simulated inverted pendulum which clearly shows that how this model can be used to resolve issues of trade-offs and extra design freedom. </br></br>

<a href='http://arxiv.org/pdf/1911.08333.pdf'>1911.08333</a> &nbsp&nbsp (cs:RO, cs:CV) &nbsp&nbsp -0.5455баллов, №750</br>
<b>Exactly Sparse Gaussian Variational Inference with Application to\n  Derivative-Free Batch Nonlinear State Estimation</b></br>
Authors: , Barfoot, Timothy D., Forbes, James R., Yoon, David</br>
  We present a <font color="blue">Gaussi</font>an Variational Inference (GVI) technique that can be applied to large-scale nonlinear batch state estimation problems. The main contribution is to show how to fit the best Gaussian to the posterior efficiently by exploiting factorization of the joint likelihood of the state and data, as is common in practical problems. The proposed Exactly Sparse Gaussian Variational Inference (ESGVI) technique stores the inverse covariance matrix, which is typically very sparse (e.g., block-tridiagonal for classic state estimation). We show that the only blocks of the (dense) covariance matrix that are required during the calculations correspond to the non-zero blocks of the inverse covariance matrix, and further show how to calculate these blocks efficiently in the general GVI problem. ESGVI operates iteratively, and while we can use analytical derivatives at each iteration, Gaussian cubature can be substituted, thereby producing an efficient derivative-free batch formulation. ESGVI simplifies to precisely the Rauch-Tung-Striebel (RTS) smoother in the batch linear estimation case, but goes beyond the \'extended\' RTS smoother in the nonlinear case since it finds the best-fit Gaussian, not the Maximum A Posteriori (MAP) point solution. We demonstrate the technique on controlled simulation problems and a batch nonlinear Simultaneous Localization and Mapping (SLAM) problem with an experimental dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.08003.pdf'>1911.08003</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.5486баллов, №751</br>
<b>User-Driven Functional Movement Training with a Wearable Hand Robot\n  after Stroke</b></br>
Authors: , Park, Sangwoo, Fraser, Michaela, Weber, Lynne M., Meeker, Cassie, Bishop, Lauri, Geller, Daniel, Stein, Joel, Ciocarlie, Matei</br>
  We studied the performance of a robotic orthosis designed to assist the paretic hand after stroke. This orthosis is designed to be wearable and fully user-controlled, allowing it to serve two possible roles: as a rehabilitation device, designed to be integrated in device-mediated rehabilitation exercises to improve performance of the affected upper limb without device assistance; or as an assistive device, designed to be integrated into daily wear to improve performance of the affected limb with grasping tasks. We present the <font color="blue">clinic</font>al outcomes of a study designed as a feasibility test for these hypotheses. 11 participants with chronic stroke engaged in a month-long training protocol using the orthosis. Individuals were evaluated using standard outcome measures, both with and without orthosis assistance. Fugl-Meyer scores (unassisted) showed improvement focused specifically at the distal joints of the upper limb, and Action Research Arm Test (ARAT) scores (unassisted) also showed a positive trend. These results suggest the possibility of using our orthosis as a rehabilitative device for the hand. Assisted ARAT and Box and Block Test scores showed that the device can function in an assistive role for participants with minimal functional use of their hand at baseline. We believe these results highlight the potential for wearable and user-driven robotic hand orthoses to extend the use and training of the affected upper limb after stroke. </br></br>

<a href='http://arxiv.org/pdf/1911.06556.pdf'>1911.06556</a> &nbsp&nbsp (cs:ML, cs:NE) &nbsp&nbsp -0.5492баллов, №752</br>
<b>Safe Interactive Model-Based Learning</b></br>
Authors: , Gallieri, Marco, Salehian, Seyed Sina Mirrazavi, Toklu, Nihat Engin, Quaglino, Alessio, Masci, Jonathan, Koutn&#xed;k, Jan, Gomez, Faustino</br>
  Control applications present hard operational constraints. A violation of these can result in unsafe behavior. This paper introduces Safe Interactive Model Based Learning (SiMBL), a framework to refine an existing controller and a system model while operating on the real environment. SiMBL is composed of the following trainable components: a Lyapunov function, which determines a safe set; a safe control policy; and a <font color="blue">Bayes</font>ian RNN forward model. A min-max control framework, based on alternate minimisation and backpropagation through the forward model, is used for the offline computation of the controller and the safe set. Safety is formally verified a-posteriori with a probabilistic method that utilizes the Noise Contrastive Priors (NPC) idea to build a Bayesian RNN forward model with an additive state uncertainty estimate which is large outside the training data distribution. Iterative refinement of the model and the safe set is achieved thanks to a novel loss that conditions the uncertainty estimates of the new model to be close to the current one. The learned safe set and model can also be used for safe exploration, i.e., to collect data within the safe invariant set, for which a simple one-step MPC is proposed. The single components are tested on the simulation of an inverted pendulum with limited torque and stability region, showing that iteratively adding more data can improve the model, the controller and the size of the safe region. </br></br>

<a href='http://arxiv.org/pdf/1910.13511.pdf'>1910.13511</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.5554баллов, №753</br>
<b>A Generalization of Principal Component Analysis</b></br>
Authors: , Battaglino, Samuele, Koyuncu, Erdem</br>
  Conventional principal component analysis (PCA) finds a principal vector that maximizes the sum of second powers of principal components. We consider a generalized PCA that aims at maximizing the sum of an arbitrary convex function of principal components. We present a gradient ascent algorithm to solve the problem. For the <font color="blue">kernel</font> version of generalized PCA, we show that the solutions can be obtained as fixed points of a simple single-layer recurrent neural network. We also evaluate our algorithms on different datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.08128.pdf'>1911.08128</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.5606баллов, №754</br>
<b>Distributed Generative Adversarial Net</b></br>
Authors: , Wang, Xiaoyu, Deng, Ye, Wang, Jinjun</br>
  Recently the Generative Adversarial Network has become a hot topic. Considering the application of GAN in multi-user environment, we propose Distributed-GAN. It enables multiple users to train with their own data locally and generates more diverse samples. Users don\'t need to share data with each other to avoid the leakage of <font color="#be00be">privacy</font>. In recent years, commercial companies have launched cloud platforms based on artificial intelligence to provide model for users who lack computing power. We hope our work can inspire these companies to provide more powerful AI services. </br></br>

<a href='http://arxiv.org/pdf/1911.09313.pdf'>1911.09313</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -0.5615баллов, №755</br>
<b>Magnetic-Assisted Initialization for Infrastructure-free Mobile Robot\n  Localization</b></br>
Authors: , Wu, Zhenyu, Wen, Mingxing, Peng, Guohao, Tang, Xiaoyu, Wang, Danwei</br>
  Most of the existing<font color="#960096"> mobile </font>robot localization solutions are either heavily dependent on pre-installed infrastructures or having difficulty working in highly repetitive environments which do not have sufficient unique features. To address this problem, we propose a magnetic-assisted initialization approach that enhances the performance of infrastructure-free mobile robot localization in repetitive featureless environments. The proposed system adopts a coarse-to-fine structure, which mainly consists of two parts: magnetic field-based matching and laser scan matching. Firstly, the interpolated magnetic field map is built and the initial pose of the mobile robot is partly determined by the k-<font color="#be00be">Nearest Neighbo</font>rs (k-NN) algorithm. Next, with the fusion of prior initial pose information, the robot is localized by laser scan matching more accurately and efficiently. In our experiment, the mobile robot was successfully localized in a featureless rectangular corridor with a success rate of 88% and an average correct localization time of 6.6 seconds. </br></br>

<a href='http://arxiv.org/pdf/1911.07658.pdf'>1911.07658</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.5618баллов, №756</br>
<b>Hacking Neural Networks: A Short Introduction</b></br>
Authors: , Kissner, Michael</br>
  A large chunk of research on the security issues of neural networks is focused on <font color="blue">adversarial att</font>acks. However, there exists a vast sea of simpler attacks one can perform both against and with neural networks. In this article, we give a quick introduction on how deep learning in security works and explore the basic methods of exploitation, but also look at the offensive capabilities deep learning enabled tools provide. All presented attacks, such as backdooring, GPU-based buffer overflows or automated bug hunting, are accompanied by short open-source exercises for anyone to try out. </br></br>

<a href='http://arxiv.org/pdf/1911.07040.pdf'>1911.07040</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.5668баллов, №757</br>
<b>Taming Reasoning in Temporal Probabilistic Relational Models</b></br>
Authors: , Gehrke, Marcel, M&#xf6;ller, Ralf, Braun, Tanya</br>
  Evidence often grounds temporal probabilistic relational models over time, which makes reasoning infeasible. To counteract groundings over time and to keep reasoning polynomial by restoring a lifted representation, we present temporal approximate merging (TAMe), which incorporates (i) <font color="#be00be">clustering</font> for grouping submodels as well as (ii) statistical significance checks to test the fitness of the clustering outcome. In exchange for faster runtimes, TAMe introduces a bounded error that becomes negligible over time. Empirical results show that TAMe significantly improves the runtime performance of inference, while keeping errors small. </br></br>

<a href='http://arxiv.org/pdf/1911.08784.pdf'>1911.08784</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.5690баллов, №758</br>
<b>Deep-seismic-prior-based reconstruction of seismic data using\n  convolutional neural networks</b></br>
Authors: , Liu, Qun, Fu, Lihua, Zhang, Meng</br>
  Reconstruction of seismic data with missing traces is a long-standing issue in seismic data processing. In recent years, rank reduction operations are being commonly utilized to overcome this problem, which require the rank of seismic data to be a prior. However, the rank of field data is unknown; usually it requires much time to manually adjust the rank and just obtain an approximated rank. Methods based on deep learning require very large datasets for training; however acquiring large datasets is difficult owing to physical or <font color="#be00be">financ</font>ial constraints in practice. Therefore, in this work, we developed a novel method based on unsupervised learning using the intrinsic properties of a convolutional neural network known as U-net, without training datasets. Only one undersampled seismic data was needed, and the deep seismic prior of input data could be exploited by the network itself, thus making the reconstruction convenient. Furthermore, this method can handle both irregular and regular seismic data. Synthetic and field data were tested to assess the performance of the proposed algorithm (DSPRecon algorithm); the advantages of using our method were evaluated by comparing it with the singular spectrum analysis (SSA) method for irregular data reconstruction and de-aliased Cadzow method for regular data reconstruction. Experimental results showed that our method provided better reconstruction performance than the SSA or Cadzow methods. The recovered signal-to-noise ratios (SNRs) were 32.68 dB and 19.11 dB for the DSPRecon and SSA algorithms, respectively. Those for the DSPRecon and Cadzow methods were 35.91 dB and 15.32 dB, respectively. </br></br>

<a href='http://arxiv.org/pdf/1911.08577.pdf'>1911.08577</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp -0.5736баллов, №759</br>
<b>Representation Learning with Multisets</b></br>
Authors: , Portilheiro, Vasco</br>
  We study the problem of learning permutation invariant representations that can capture &quot;flexible&quot; notions of containment. We formalize this problem via a measure <font color="blue">theor</font>etic definition of multisets, and obtain a theoretically-motivated learning model. We propose training this model on a novel task: predicting the size of the symmetric difference (or intersection) between pairs of multisets. We demonstrate that our model not only performs very well on predicting containment relations (and more effectively predicts the sizes of symmetric differences and intersections than DeepSets-based approaches with unconstrained object representations), but that it also learns meaningful representations. </br></br>

<a href='http://arxiv.org/pdf/1910.02702.pdf'>1910.02702</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.5738баллов, №760</br>
<b>Noise as Domain Shift: Denoising Medical Images by Unpaired Image\n  Translation</b></br>
Authors: , Manakov, Ilja, Rohm, Markus, Kern, Christoph, Schworm, Benedikt, Kortuem, Karsten, Tresp, Volker</br>
  We cast the problem of <font color="#be00be">image denoising</font> as a domain translation problem between high and low noise domains. By modifying the cycleGAN model, we are able to learn a mapping between these domains on unpaired retinal optical coherence <font color="#be00be">tomography</font> images. In quantitative measurements and a qualitative evaluation by ophthalmologists, we show how this approach <font color="#00be00">outperform</font>s other established methods. The results indicate that the network differentiates subtle changes in the level of noise in the image. Further investigation of the model\'s feature maps reveals that it has learned to distinguish retinal layers and other distinct regions of the images. </br></br>

<a href='http://arxiv.org/pdf/1911.09242.pdf'>1911.09242</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.5746баллов, №761</br>
<b>How Do You #relax When You\'re #stressed? A Content Analysis and\n  Infodemiology Study of Stress-Related Tweets</b></br>
Authors: , Doan, Son, Ritchart, Amanda, Perry, Nicholas, Chaparro, Juan D, Conway, Mike</br>
  Background: Stress is a contributing factor to many major health problems in the United States, such as heart <font color="blue">diseas</font>e, depression, and autoimmune diseases. Relaxation is often recommended in mental health treatment as a frontline strategy to reduce stress, thereby improving health conditions.   Objective: The objective of our study was to understand how people express their feelings of stress and relaxation through Twitter messages.   Methods: We first performed a qualitative content analysis of 1326 and 781 tweets containing the keywords &quot;stress&quot; and &quot;relax&quot;, respectively. We then investigated the use of machine learning algorithms to automatically classify tweets as stress versus non stress and relaxation versus non relaxation. Finally, we applied these classifiers to sample datasets drawn from 4 cities with the goal of evaluating the extent of any correlation between our automatic classification of tweets and results from public stress surveys.   Results: Content analysis showed that the most frequent topic of stress tweets was education, followed by work and social relationships. The most frequent topic of relaxation tweets was rest and vacation, followed by nature and water. When we applied the classifiers to the cities dataset, the proportion of stress tweets in New York and San Diego was substantially higher than that in Los Angeles and San Francisco.   Conclusions: This content analysis and infodemiology study revealed that Twitter, when used in conjunction with natural language processing techniques, is a useful data source for understanding stress and stress management strategies, and can potentially supplement infrequently collected survey-based stress data. </br></br>

<a href='http://arxiv.org/pdf/1911.09554.pdf'>1911.09554</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.5800баллов, №762</br>
<b>Discrete and Continuous Deep Residual Learning Over Graphs</b></br>
Authors: , Avelar, Pedro H. C., Tavares, Anderson R., Gori, Marco, Lamb, Luis C.</br>
  In this paper we propose the use of continuous residual modules for graph <font color="blue">kernel</font>s in Graph Neural Networks. We show the how both discrete and continuous residual layers allow for more robust training, being that continuous residual layers are those which are applied by integrating through an Ordinary Differential Equation (ODE) solver to produce their output. We experimentally show that these residuals achieve better results than the ones with non-residual modules when multiple layers are used, mitigating the low-pass filtering effect of GCN-based models. Finally, we apply and analyse the behaviour of these techniques and give pointers to how this technique can be useful in other domains by allowing more predictable behaviour under dynamic times of computation. </br></br>

<a href='http://arxiv.org/pdf/1911.06893.pdf'>1911.06893</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.5825баллов, №763</br>
<b>Imitation in the Imitation Game</b></br>
Authors: , Kashyap, Ravi</br>
  We discuss the objectives of automation equipped with non-trivial decision making, or creating artificial intelligence, in the <font color="#be00be">financ</font>ial <font color="#be00be">market</font>s and provide a possible alternative. Intelligence might be an unintended consequence of <font color="#00be00">curiosity</font> left to roam free, best exemplified by a frolicking infant. For this unintentional yet welcome aftereffect to set in a foundational list of guiding principles needs to be present. A consideration of these requirements allows us to propose a test of intelligence for trading programs, on the lines of the Turing Test, long the benchmark for intelligent machines. We discuss the application of this methodology to the dilemma in finance, which is whether, when and how much to Buy, Sell or Hold. </br></br>

<a href='http://arxiv.org/pdf/1911.05312.pdf'>1911.05312</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.5830баллов, №764</br>
<b>Topological Stability: a New Algorithm for Selecting The Nearest\n  Neighbors in Non-Linear Dimensionality Reduction Techniques</b></br>
Authors: , Elhenawy, Mohammed, Masoud, Mahmoud, Glaser, Sebastian, Rakotonirainy, Andry</br>
  In the machine learning field, dimensionality reduction is an important task. It mitigates the undesired properties of high-dimensional spaces to facilitate classification, compression, and visualization of high-dimensional data. During the last decade, researchers proposed many new (non-linear) techniques for dimensionality reduction. Most of these techniques are based on the intuition that data lies on or near a complex low-dimensional manifold that is embedded in the high-dimensional space. New techniques for dimensionality reduction aim at identifying and extracting the manifold from the high-dimensional space. Isomap is one of widely-used low-dimensional embedding methods, where geodesic distances on a weighted graph are incorporated with the classical scaling (metric multidimensional scaling). The Isomap chooses the <font color="#be00be">nearest neighbo</font>urs based on the distance only which causes bridges and topological instability. In this paper, we propose a new algorithm to choose the nearest neighbours to reduce the number of short-circuit errors and hence improves the topological stability. Because at any point on the manifold, that point and its nearest neighbours form a vector subspace and the orthogonal to that subspace is orthogonal to all vectors spans the vector subspace. The prposed algorithmuses the point itself and its two nearest neighbours to find the bases of the subspace and the orthogonal to that subspace which belongs to the orthogonal complementary subspace. The proposed algorithm then adds new points to the two nearest neighbours based on the distance and the angle between each new point and the orthogonal to the subspace. The superior performance of the new algorithm in choosing the nearest neighbours is confirmed through experimental work with several datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.07596.pdf'>1911.07596</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.5841баллов, №765</br>
<b>Convergence Analysis of a Momentum Algorithm with Adaptive Step Size for\n  Non Convex Optimization</b></br>
Authors: , Barakat, Anas, Bianchi, Pascal</br>
  Although ADAM is a very popular algorithm for optimizing the weights of neural networks, it has been recently shown that it can diverge even in simple convex optimization examples. Several variants of ADAM have been proposed to circumvent this convergence issue. In this work, we study the ADAM algorithm for smooth nonconvex optimization under a boundedness assumption on the adaptive learning rate. The bound on the adaptive step size depends on the Lipschitz constant of the gradient of the objective function and provides safe <font color="blue">theor</font>etical adaptive step sizes. Under this boundedness assumption, we show a novel first order convergence rate result in both deterministic and stochastic contexts. Furthermore, we establish convergence rates of the function value sequence using the Kurdyka-Lojasiewicz property. </br></br>

<a href='http://arxiv.org/pdf/1911.09214.pdf'>1911.09214</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.5852баллов, №766</br>
<b>Fast Non-Parametric Learning to Accelerate Mixed-Integer Programming for\n  Online Hybrid Model Predictive Control</b></br>
Authors: , Zhu, Jia-Jie, Martius, Georg</br>
  Today\'s fast linear algebra and numerical optimization tools have pushed the frontier of model predictive control (MPC) forward, to the efficient control of highly nonlinear and hybrid systems. The field of hybrid MPC has demonstrated that exact optimal control law can be computed, e.g., by mixed-integer programming (MIP) under piecewise-affine (PWA) system models. Despite the elegant <font color="blue">theor</font>y, online solving hybrid MPC is still out of reach for many applications. We aim to speed up MIP by combining geometric insights from hybrid MPC, a simple-yet-effective learning algorithm, and MIP warm start techniques. Following a line of work in approximate explicit MPC, the proposed learning-control algorithm, LNMS, gains computational advantage over MIP at little cost and is straightforward for practitioners to implement. </br></br>

<a href='http://arxiv.org/pdf/1911.02922.pdf'>1911.02922</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.5868баллов, №767</br>
<b>Persistent Homology as Stopping-Criterion for Natural Neighbor\n  Interpolation</b></br>
Authors: , Melodia, Luciano, Lenz, Richard</br>
  In this study the method of natural neighbours is used to interpolate data that has been drawn from a topological space with higher homology groups on its filtration. A particular difficulty with this algorithm are the boundary points. Its core is based on the Voronoi diagram, which induces a natural dual map to the Delaunay triangulation. Advantage is taken from this fact and the persistent homology is therefore calculated after each iteration to capture the changing topology of the data. The Bottleneck and Wasserstein distance serve as a measure of quality between the original data and the interpolation. If the norm of two distances exceeds a heuristically determined threshold, the algorithm terminates. The <font color="blue">theor</font>etical basis for this procedure is given and the validity of this approach is justified with numerical experiments. </br></br>

<a href='http://arxiv.org/pdf/1911.07107.pdf'>1911.07107</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.5870баллов, №768</br>
<b>SMART: Skeletal Motion Action Recognition aTtack</b></br>
Authors: , Wang, He, He, Feixiang, Peng, Zhexi, Yang, Yongliang, Shao, Tianjia, Zhou, Kun, Hogg, David</br>
  <font color="blue">Adversarial att</font>ack has inspired great interest in computer vision, by showing that classification-based solutions are prone to imperceptible attack in many tasks. In this paper, we propose a method, SMART, to attack action recognizers which rely on 3D skeletal motions. Our method involves an innovative perceptual loss which ensures the imperceptibility of the attack. Empirical studies demonstrate that SMART is effective in both white-box and black-box scenarios. Its generalizability is evidenced on a variety of action recognizers and datasets. Its versatility is shown in different attacking strategies. Its deceitfulness is proven in extensive perceptual studies. Finally, SMART shows that adversarial attack on 3D skeletal motion, one type of time-series data, is significantly different from traditional adversarial attack problems. </br></br>

<a href='http://arxiv.org/pdf/1911.08192.pdf'>1911.08192</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.5882баллов, №769</br>
<b>Information-Theoretic Local Minima Characterization and Regularization</b></br>
Authors: , Jia, Zhiwei, Su, Hao</br>
  Recent advances in deep learning <font color="blue">theor</font>y have evoked the study of generalizability across different local minima of deep neural networks (DNNs). While current work focused on either discovering properties of good local minima or developing regularization techniques to induce good local minima, no approach exists that can tackle both problems. We achieve these two goals successfully in a unified manner. Specifically, based on the Fisher information we propose a metric both strongly indicative of generalizability of local minima and effectively applied as a practical regularizer. We provide theoretical analysis including a generalization bound and empirically demonstrate the success of our approach in both capturing and improving the generalizability of DNNs. Experiments are performed on CIFAR-10 and CIFAR-100 for various network architectures. </br></br>

<a href='http://arxiv.org/pdf/1911.09501.pdf'>1911.09501</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -0.5885баллов, №770</br>
<b>Safe Linear Stochastic Bandits</b></br>
Authors: , Khezeli, Kia, Bitar, Eilyan</br>
  We introduce the safe linear stochastic <font color="blue">bandit</font> framework---a generalization of linear stochastic bandits---where, in each stage, the learner is required to select an arm with an expected reward that is no less than a predetermined (safe) threshold with high probability. We assume that the learner initially has knowledge of an arm that is known to be safe, but not necessarily optimal. Leveraging on this assumption, we introduce a learning algorithm that systematically combines known safe arms with exploratory arms to safely expand the set of safe arms over time, while facilitating safe greedy exploitation in subsequent stages. In addition to ensuring the satisfaction of the safety constraint at every stage of play, the proposed algorithm is shown to exhibit an expected regret that is no more than $O(\\sqrt{T}\\log (T))$ after $T$ stages of play. </br></br>

<a href='http://arxiv.org/pdf/1911.06958.pdf'>1911.06958</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.5899баллов, №771</br>
<b>Regularized Weighted Low Rank Approximation</b></br>
Authors: , Ban, Frank, Woodruff, David, Zhang, Qiuyi</br>
  The classical low rank approximation problem is to find a rank $k$ matrix $UV$ (where $U$ has $k$ columns and $V$ has $k$ rows) that minimizes the Frobenius norm of $A - UV$. Although this problem can be solved efficiently, we study an NP-hard variant of this problem that involves weights and regularization. A previous paper of [Razenshteyn et al. \'16] derived a polynomial time algorithm for weighted low rank approximation with constant rank. We derive provably sharper guarantees for the regularized version by obtaining parameterized complexity bounds in terms of the statistical dimension rather than the rank, allowing for a rank-independent runtime that can be significantly faster. Our improvement comes from applying sharper matrix concentration bounds, using a novel conditioning technique, and proving structural <font color="blue">theor</font>ems for regularized low rank problems. </br></br>

<a href='http://arxiv.org/pdf/1911.08608.pdf'>1911.08608</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.5915баллов, №772</br>
<b>Seq2Seq RNN based Gait Anomaly Detection from Smartphone Acquired\n  Multimodal Motion Data</b></br>
Authors: , Bonetto, Riccardo, Soldan, Mattia, Lanaro, Alberto, Milani, Simone, Rossi, Michele</br>
  Smartphones and wearable devices are fast growing technologies that, in conjunction with advances in wireless sensor hardware, are enabling ubiquitous sensing applications. Wearables are suitable for indoor and outdoor scenarios, can be placed on many parts of the human body and can integrate a large number of sensors capable of gathering physiological and behavioral biometric information. Here, we are concerned with gait analysis systems that extract meaningful information from a user\'s movements to identify <font color="#be00be">anomal</font>ies and changes in their walking <font color="#be00be">style</font>. The solution that is put forward is subject-specific, as the designed feature extraction and classification tools are trained on the subject under observation. A smartphone mounted on an ad-hoc made chest support is utilized to gather inertial data and video signals from its built-in sensors and rear-facing camera. The collected video and inertial data are preprocessed, combined and then classified by means of a Recurrent Neural Network (RNN) based Sequence-to-Sequence (Seq2Seq) model, which is used as a feature extractor, and a following Convolutional Neural Network (CNN) classifier. This architecture provides excellent results, being able to correctly assess anomalies in 100% of the cases, for the considered tests, surpassing the performance of support vector machine classifiers. </br></br>

<a href='http://arxiv.org/pdf/1911.02360.pdf'>1911.02360</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.5918баллов, №773</br>
<b>Reversible Adversarial Examples based on Reversible Image Transformation</b></br>
Authors: , Wang, Hua, Yin, Zhaoxia</br>
  Recent studies show that widely used deep neural networks (DNNs) are vulnerable to carefully crafted adversarial examples, it inevitably brings some security challenges. However, the attack characteristic of adversarial examples can be taken advantage to do <font color="#be00be">privacy</font>-preserving image research. In this paper, we make use of Reversible Image Transformation to construct reversible adversarial examples, which are still misclassified by DNNs that are utilized by illegal organizations to steal privacy of image content that we upload to the cloud or social platforms. Most importantly, the proposed method can recover original images from downloaded reversible adversarial examples with no distortion. The experimental results show that the attack success rate of the reversible adversarial examples obtained by this method can reach more than 95 % on MNIST and more than 60 % on ImageNet. </br></br>

<a href='http://arxiv.org/pdf/1911.05828.pdf'>1911.05828</a> &nbsp&nbsp (cs:ET) &nbsp&nbsp -0.5944баллов, №774</br>
<b>All-Spin Bayesian Neural Networks</b></br>
Authors: , Yang, Kezhou, Malhotra, Akul, Lu, Sen, Sengupta, Abhronil</br>
  Probabilistic machine learning enabled by the <font color="blue">Bayes</font>ian formulation has recently gained significant attention in the domain of automated reasoning and decision-making. While impressive strides have been made recently to scale up the performance of deep Bayesian neural networks, they have been primarily standalone software efforts without any regard to the underlying hardware implementation. In this paper, we propose an &quot;All-Spin&quot; Bayesian Neural Network where the underlying spintronic hardware provides a better match to the Bayesian computing models. To the best of our knowledge, this is the first exploration of a Bayesian neural hardware accelerator enabled by emerging post-CMOS technologies. We develop an experimentally calibrated device-circuit-algorithm co-simulation framework and demonstrate $23.6\\times$ reduction in energy consumption against an iso-network CMOS baseline implementation. </br></br>

<a href='http://arxiv.org/pdf/1911.06962.pdf'>1911.06962</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp -0.5961баллов, №775</br>
<b>Inductive Relation Prediction on Knowledge Graphs</b></br>
Authors: , Teru, Komal K., Hamilton, William L.</br>
  Inferring missing edges in multi-relational <font color="#960096">knowledge graph</font>s is a fundamental task in statistical relational learning. However, previous work has largely focused on the transductive relation prediction problem, where missing edges must be predicted for a single, fixed graph. In contrast, many <font color="#009600">real-world</font> situations require relation prediction on dynamic or previously unseen knowledge graphs (e.g., for question answering, dialogue, or <font color="#be00be">e-commerce</font> applications). Here, we develop a novel graph neural network (GNN) architecture to perform inductive relation prediction and provide a systematic comparison between this GNN approach and a strong, rule-based baseline. Our results highlight the significant difficulty of inductive relational learning, compared to the transductive case, and offer a new challenging set of inductive benchmarks for knowledge graph completion. </br></br>

<a href='http://arxiv.org/pdf/1911.07571.pdf'>1911.07571</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.5964баллов, №776</br>
<b>Casimir effect with machine learning</b></br>
Authors: , Chernodub, M. N., Erbin, Harold, Grishmanovskii, I. V., Goy, V. A., Molochkov, A. V.</br>
  Vacuum fluctuations of quantum fields between physical objects depend on the shapes, positions, and internal composition of the latter. For objects of arbitrary shapes, even made from idealized materials, the calculation of the associated zero-point (Casimir) energy is an analytically intractable challenge. We propose a new numerical approach to this problem based on machine-learning techniques and illustrate the effectiveness of the method in a (2+1) dimensional scalar field <font color="blue">theor</font>y. The Casimir energy is first calculated numerically using a Monte-Carlo algorithm for a set of the Dirichlet boundaries of various shapes. Then, a neural network is trained to compute this energy given the Dirichlet domain, treating the latter as black-and-white pixelated images. We show that after the learning phase, the neural network is able to quickly predict the Casimir energy for new boundaries of general shapes with reasonable accuracy. </br></br>

<a href='http://arxiv.org/pdf/1911.03952.pdf'>1911.03952</a> &nbsp&nbsp (cs:SD) &nbsp&nbsp -0.5970баллов, №777</br>
<b>Transformation of low-quality device-recorded speech to high-quality\n  speech using improved SEGAN model</b></br>
Authors: , Sarfjoo, Seyyed Saeed, Wang, Xin, Henter, Gustav Eje, Lorenzo-Trueba, Jaime, Takaki, Shinji, Yamagishi, Junichi</br>
  Nowadays vast amounts of speech data are recorded from low-quality recorder devices such as smartphones, tablets, laptops, and medium-quality microphones. The objective of this research was to study the automatic generation of high-quality speech from such low-quality device-recorded speech, which could then be applied to many speech-generation tasks. In this paper, we first introduce our new device-recorded speech dataset then propose an improved end-to-end method for automatically transforming the low-quality device-recorded speech into professional high-quality speech. Our method is an extension of a generative adversarial network (GAN)-based <font color="#be00be">speech enhancement</font> model called speech enhancement GAN (SEGAN), and we present two modifications to make model training more robust and stable. Finally, from a large-scale listening test, we show that our method can significantly enhance the quality of device-recorded speech signals. </br></br>

<a href='http://arxiv.org/pdf/1911.09459.pdf'>1911.09459</a> &nbsp&nbsp (cs:SD) &nbsp&nbsp -0.5971баллов, №778</br>
<b>Designing Virtual Soundscapes for Alzheimer\'s Disease Care</b></br>
Authors: , Voisin, Fr&#xe9;d&#xe9;ric</br>
  Sound environment is a prime source of conscious and unconscious information which allows listeners to place themselves, to communicate, to feel, to remember. The author describes the process of designing a new audio interactive apparatus for Alzheimer\'s care, in the context of an active multidisciplinary research project led by the author in collaboration with a longterm care centre (EHPAD) in Burgundy (France), a geriatrician, a gerontologist, psychologists and caregivers. The apparatus, named Madeleines Sonores in reference to Proust\'s madeleine, have provided virtual soundscapes sounding for a year for 14 elderly people hosted in the dedicated Alzheimer\'s unit of the care centre, 24/7. Empiric aspects of sonic interactivity are discussed in relation to dementia and to the activity of caring. Scientific studies are initiated to evaluate the benefits of such a disposal in Alzheimer\'s <font color="blue">diseas</font>e therapy and in caring dementia. </br></br>

<a href='http://arxiv.org/pdf/1911.09291.pdf'>1911.09291</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp -0.5979баллов, №779</br>
<b>Scalable methods for computing state similarity in deterministic Markov\n  Decision Processes</b></br>
Authors: , Castro, Pablo Samuel</br>
  We present new algorithms for computing and approximating bisimulation metrics in Markov Decision Processes (MDPs). Bisimulation metrics are an elegant formalism that capture behavioral equivalence between states and provide strong <font color="blue">theor</font>etical guarantees on differences in optimal behaviour. Unfortunately, their computation is expensive and requires a tabular representation of the states, which has thus far rendered them impractical for large problems. In this paper we present a new version of the metric that is tied to a behavior policy in an MDP, along with an analysis of its theoretical properties. We then present two new algorithms for approximating bisimulation metrics in large, deterministic MDPs. The first does so via sampling and is guaranteed to converge to the true metric. The second is a differentiable loss which allows us to learn an approximation even for continuous state MDPs, which prior to this work had not been possible. </br></br>

<a href='http://arxiv.org/pdf/1911.07930.pdf'>1911.07930</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.5982баллов, №780</br>
<b>BiNet: Degraded-Manuscript Binarization in Diverse Document Textures and\n  Layouts using Deep Encoder-Decoder Networks</b></br>
Authors: , Dhali, Maruf A., de Wit, Jan Willem, Schomaker, Lambert</br>
  Handwritten document-image binarization is a semantic <font color="#be00be">segmentation</font> process to differentiate ink pixels from background pixels. It is one of the essential steps towards character recognition, writer identification, and script-<font color="#be00be">style</font> evolution analysis. The binarization task itself is challenging due to the vast diversity of writing styles, inks, and paper materials. It is even more difficult for historical manuscripts due to the aging and degradation of the documents over time. One of such manuscripts is the Dead Sea Scrolls (DSS) image collection, which poses extreme challenges for the existing binarization techniques. This article proposes a new binarization technique for the DSS images using the deep encoder-decoder networks. Although the artificial neural network proposed here is primarily designed to binarize the DSS images, it can be trained on different manuscript collections as well. Additionally, the use of transfer learning makes the network already utilizable for a wide range of handwritten documents, making it a unique multi-purpose tool for binarization. Qualitative results and several quantitative comparisons using both historical manuscripts and datasets from handwritten document image binarization competition (H-DIBCO and DIBCO) exhibit the robustness and the effectiveness of the system. The best performing network architecture proposed here is a variant of the U-Net encoder-decoders. </br></br>

<a href='http://arxiv.org/pdf/1911.08216.pdf'>1911.08216</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.5986баллов, №781</br>
<b>On the Impact of Object and Sub-component Level Segmentation Strategies\n  for Supervised Anomaly Detection within X-ray Security Imagery</b></br>
Authors: , Bhowmik, Neelanjan, Gaus, Yona Falinie A., Akcay, Samet, Barker, Jack W., Breckon, Toby P.</br>
  X-ray security screening is in widespread use to maintain transportation security against a wide range of potential threat profiles. Of particular interest is the recent focus on the use of automated screening approaches, including the potential <font color="#be00be">anomal</font>y detection as a methodology for concealment detection within complex electronic items. Here we address this problem considering varying <font color="#be00be">segmentation</font> strategies to enable the use of both object level and sub-component level anomaly detection via the use of secondary convolutional neural network (CNN) architectures. Relative performance is evaluated over an extensive dataset of exemplar cluttered X-ray imagery, with a focus on consumer electronics items. We find that sub-component level segmentation produces marginally superior performance in the secondary anomaly detection via classification stage, with true positive of ~98% of anomalies, with a ~3% false positive. </br></br>

<a href='http://arxiv.org/pdf/1911.08856.pdf'>1911.08856</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.6009баллов, №782</br>
<b>Learning Generalized Quasi-Geostrophic Models Using Deep Neural\n  Numerical Models</b></br>
Authors: , Lguensat, Redouane, Sommer, Julien Le, Metref, Sammy, Cosme, Emmanuel, Fablet, Ronan</br>
  We introduce a new strategy designed to help physicists discover hidden laws governing dynamical systems. We propose to use machine learning automatic differentiation libraries to develop hybrid numerical models that combine components based on prior physical knowledge with components based on neural networks. In these architectures, named Deep Neural Numerical Models (DNNMs), the neural network components are used as building-blocks then deployed for learning hidden variables of underlying physical laws governing dynamical systems. In this paper, we illustrate an application of DNNMs to upper ocean dynamics, more precisely the dynamics of a sea surface tracer, the Sea Surface Height (SSH). We develop an advection-based fully differentiable numerical scheme, where parts of the computations can be replaced with learnable ConvNets, and make connections with the single-layer Quasi-Geostrophic (QG) model, a baseline <font color="blue">theor</font>y in physical oceanography developed decades ago. </br></br>

<a href='http://arxiv.org/pdf/1911.09162.pdf'>1911.09162</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.6012баллов, №783</br>
<b>Deep Active Learning: Unified and Principled Method for Query and\n  Training</b></br>
Authors: , Shui, Changjian, Zhou, Fan, Gagn&#xe9;, Christian, Wang, Boyu</br>
  In this paper, we proposed a unified and principled method for both querying and training process in deep batch active learning. We provided the <font color="blue">theor</font>etical insights from the intuition of modeling the interactive procedure in active learning as distribution matching, by adopting Wasserstein distance. As a consequence, we derived a new training loss from the theoretical analysis, which is decomposed into optimizing deep neural network parameter and batch query selection through alternative optimization. In addition, the loss for training deep neural network is naturally formulated as a min-max optimization problem through leveraging the unlabeled data information. Moreover, the proposed principles also indicate an explicit uncertainty-diversity trade-off in the query batch selection. Finally we evaluated our proposed method for different benchmarks, showing consistently better empirical performances and more time efficient query strategy, comparing to several baselines. </br></br>

<a href='http://arxiv.org/pdf/1910.09721.pdf'>1910.09721</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -0.6019баллов, №784</br>
<b>Intelligence via ultrafilters: structural properties of some\n  intelligence comparators of deterministic Legg-Hutter agents</b></br>
Authors: , Alexander, Samuel Allen</br>
  Legg and Hutter, as well as subsequent authors, considered intelligent agents through the lens of interaction with reward-giving environments, attempting to assign numeric intelligence measures to such agents, with the guiding principle that a more intelligent agent should gain higher rewards from environments in some aggregate sense. In this paper, we consider a related question: rather than measure numeric intelligence of one Legg- Hutter agent, how can we compare the relative intelligence of two Legg-Hutter agents? We propose an elegant answer based on the following insight: we can view Legg-Hutter agents as candidates in an election, whose voters are environments, letting each environment vote (via its rewards) which agent (if either) is more intelligent. This leads to an abstract family of comparators simple enough that we can prove some structural <font color="blue">theor</font>ems about them. It is an open question whether these structural theorems apply to more practical intelligence measures. </br></br>

<a href='http://arxiv.org/pdf/1911.08764.pdf'>1911.08764</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.6025баллов, №785</br>
<b>Learning mappings onto regularized latent spaces for biometric\n  authentication</b></br>
Authors: , Testa, Matteo, Ali, Arslan, Bianchi, Tiziano, Magli, Enrico</br>
  We propose a novel architecture for generic biometric authentication based on deep neural networks: RegNet. Differently from other methods, RegNet learns a mapping of the input biometric traits onto a target distribution in a well-behaved space in which users can be separated by means of simple and tunable boundaries. More specifically, authorized and unauthorized users are mapped onto two different and well behaved <font color="blue">Gaussi</font>an distributions. The novel approach of learning the mapping instead of the boundaries further avoids the problem encountered in typical classifiers for which the learnt boundaries may be complex and difficult to analyze. RegNet achieves high performance in terms of security metrics such as Equal Error Rate (EER), False Acceptance Rate (FAR) and Genuine Acceptance Rate (GAR). The experiments we conducted on <font color="#00be00">publicly available</font> datasets of<font color="#be00be"> face </font>and fingerprint confirm the effectiveness of the proposed system. </br></br>

<a href='http://arxiv.org/pdf/1911.07072.pdf'>1911.07072</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.6026баллов, №786</br>
<b>Unsupervised Deep Metric Learning via Auxiliary Rotation Loss</b></br>
Authors: , Cao, Xuefei, Chen, Bor-Chun, Lim, Ser-Nam</br>
  Deep metric learning is an important area due to its applicability to many domains such as image retrieval and person <font color="blue">re-identification</font>. The main drawback of such models is the necessity for labeled data. In this work, we propose to generate pseudo-labels for deep metric learning directly from <font color="#be00be">clustering</font> assignment and we introduce unsupervised deep metric learning (UDML) regularized by a self-supervision (SS) task. In particular, we propose to regularize the training process by predicting image rotations. Our method (UDML-SS) jointly learns discriminative embeddings, unsupervised clustering assignments of the embeddings, as well as a self-supervised pretext task. UDML-SS iteratively cluster embeddings using traditional clustering algorithm (e.g., <font color="blue">k-mean</font>s), and sampling training pairs based on the cluster assignment for metric learning, while optimizing self-supervised pretext task in a multi-task fashion. The role of self-supervision is to stabilize the training process and encourages the model to learn meaningful feature representations that are not distorted due to unreliable clustering assignments. The proposed method performs well on standard benchmarks for metric learning, where it <font color="#00be00">outperform</font>s current <font color="red">state-of-the-art</font> approaches by a large margin and it also shows <font color="#960096">competitive</font> performance with various metric learning loss functions. </br></br>

<a href='http://arxiv.org/pdf/1911.08934.pdf'>1911.08934</a> &nbsp&nbsp (cs:SD, cs:ML, stat:ML) &nbsp&nbsp -0.6060баллов, №787</br>
<b>Joint DNN-Based Multichannel Reduction of Acoustic Echo, Reverberation\n  and Noise</b></br>
Authors: , Carbajal, Guillaume, Serizel, Romain, Vincent, Emmanuel, Humbert, Eric</br>
  We consider the problem of simultaneous reduction of acoustic echo, <font color="#be00be">reverberat</font>ion and noise. In real scenarios, these distortion sources may occur simultaneously and reducing them implies combining the corresponding distortion-specific filters. As these filters interact with each other, they must be jointly optimized. We propose to model the target and residual signals after linear echo cancellation and dereverberation using a multichannel <font color="blue">Gaussi</font>an modeling framework and to jointly represent their spectra by means of a neural network. We develop an iterative block-coordinate ascent algorithm to update all the filters. We evaluate our system on real recordings of acoustic echo, reverberation and noise acquired with a smart speaker in various situations. The proposed approach <font color="#00be00">outperform</font>s in terms of overall distortion a cascade of the individual approaches and a joint reduction approach which does not rely on a spectral model of the target and residual signals. </br></br>

<a href='http://arxiv.org/pdf/1911.06935.pdf'>1911.06935</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.6071баллов, №788</br>
<b>Fairness With Minimal Harm: A Pareto-Optimal Approach For Healthcare</b></br>
Authors: , Martinez, Natalia, Bertran, Martin, Sapiro, Guillermo</br>
  Common fairness definitions in machine learning focus on balancing notions of disparity and utility. In this work, we study fairness in the context of risk disparity among sub-populations. We are interested in learning models that minimize performance discrepancies across sensitive groups without causing unnecessary harm. This is relevant to high-stakes domains such as healthcare, where non-maleficence is a core principle. We formalize this objective using Pareto frontiers, and provide analysis, based on recent works in fairness, to exemplify scenarios were perfect fairness might not be feasible without doing unnecessary harm. We present a methodology for training neural networks that achieve our goal by dynamically re-balancing subgroups risks. We argue that even in domains where fairness at cost is required, finding a non-unnecessary-harm fairness model is the optimal initial step. We demonstrate this methodology on real case-studies of predicting ICU <font color="blue">patient</font> mortality, and classifying skin lesions from dermatoscopic images. </br></br>

<a href='http://arxiv.org/pdf/1911.08339.pdf'>1911.08339</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.6074баллов, №789</br>
<b>The Power of Factorization Mechanisms in Local and Central Differential\n  Privacy</b></br>
Authors: , Edmonds, Alexander, Nikolov, Aleksandar, Ullman, Jonathan</br>
  We give new characterizations of the sample complexity of answering linear queries (statistical queries) in the local and central models of differential <font color="#be00be">privacy</font>:   *In the non-interactive local model, we give the first approximate characterization of the sample complexity. Informally our bounds are tight to within polylogarithmic factors in the number of queries and desired accuracy. Our characterization extends to agnostic learning in the local model.   *In the central model, we give a characterization of the sample complexity in the high-accuracy regime that is analogous to that of Nikolov, Talwar, and Zhang (STOC 2013), but is both quantitatively tighter and has a dramatically simpler proof.   Our lower bounds apply equally to the empirical and population estimation problems. In both cases, our characterizations show that a particular factorization mechanism is approximately optimal, and the optimal sample complexity is bounded from above and below by well studied factorization norms of a matrix associated with the queries. </br></br>

<a href='http://arxiv.org/pdf/1911.09279.pdf'>1911.09279</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.6087баллов, №790</br>
<b>NaMemo: Enhancing Lecturers\' Interpersonal Competence of Remembering\n  Students\' Names</b></br>
Authors: , Jiang, Guang, Shi, Mengzhen, Su, Ying, An, Pengcheng, Wang, Yunlong</br>
  Addressing students by their names helps a teacher to start building rapport with students and thus facilitate their classroom participation. However, this basic yet effective skill has become rather challenging for university lecturers (especially in Asian universities), who have to handle large-sized (sometimes exceeding 100) groups in their daily teaching. To enhance lecturers\' competence in delivering interpersonal interaction, we develop NaMemo, a real-time name-indicating system based on a dedicated computer vision algorithm. This paper presents its design and feasibility study, which showed a plausible acceptance level from the participating teachers and students. We also reveal students\' concerns on the abuse or misuse of this system: e.g., for checking attendance. Taken together, we discuss the opportunities and risks in design, and elaborate on the plan of a follow-up, in-depth implementation to further evaluate NaMemo\'s impacts on learning and teaching, as well as to probe design implications including <font color="#be00be">privacy</font> considerations. </br></br>

<a href='http://arxiv.org/pdf/1911.04933.pdf'>1911.04933</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.6090баллов, №791</br>
<b>Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep\n  Networks</b></br>
Authors: , Golatkar, Aditya, Achille, Alessandro, Soatto, Stefano</br>
  We explore the problem of selectively forgetting a particular set of data used for training a deep neural network. While the effects of the data to be forgotten can be hidden from the output of the network, insights may still be gleaned by probing deep into its weights. We propose a method for &quot;scrubbing&quot; the weights clean of information about a particular set of training data. The method does not require retraining from scratch, nor access to the data originally used for training. Instead, the weights are modified so that any probing function of the weights, computed with no knowledge of the random seed used for training, is indistinguishable from the same function applied to the weights of a network trained without the data to be forgotten. This condition is a generalized and weaker form of Differential <font color="#be00be">Privacy</font>. Exploiting ideas related to the stability of stochastic gradient descent, we introduce an upper-bound on the amount of information remaining in the weights, which can be estimated efficiently even for deep neural networks. </br></br>

<a href='http://arxiv.org/pdf/1911.07953.pdf'>1911.07953</a> &nbsp&nbsp (cs:SD, cs:ML, stat:ML) &nbsp&nbsp -0.6104баллов, №792</br>
<b>Alternating Between Spectral and Spatial Estimation for Speech\n  Separation and Enhancement</b></br>
Authors: , Wang, Zhong-Qiu, Wisdom, Scott, Wilson, Kevin, Hershey, John R.</br>
  This work investigates alternation between spectral separation using masking-based networks and spatial separation using multichannel beamforming. In this framework, the spectral separation is performed using a mask-based deep network. The result of mask-based separation is used, in turn, to estimate a spatial beamformer. The output of the beamformer is fed back into another mask-based separation network. We explore multiple ways of computing time-varying covariance matrices to improve beamforming, including factorizing the spatial covariance into a time-varying amplitude component and time-invariant spatial component. For the subsequent mask-based filtering, we consider different modes, including masking the noisy input, masking the beamformer output, and a hybrid approach combining both. Our best method first uses spectral separation, then spatial beamforming, and finally a spectral post-filter, and demonstrates an average improvement of 2.8 dB over baseline mask-based separation, across four different reverberant <font color="#be00be">speech enhancement</font> and separation tasks. </br></br>

<a href='http://arxiv.org/pdf/1911.08250.pdf'>1911.08250</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.6105баллов, №793</br>
<b>On the Discrepancy between the Theoretical Analysis and Practical\n  Implementations of Compressed Communication for Distributed Deep Learning</b></br>
Authors: , Dutta, Aritra, Bergou, El Houcine, Abdelmoniem, Ahmed M., Ho, Chen-Yu, Sahu, Atal Narayan, Canini, Marco, Kalnis, Panos</br>
  Compressed communication, in the form of sparsification or quantization of stochastic gradients, is employed to reduce communication costs in distributed data-parallel training of deep neural networks. However, there exists a discrepancy between <font color="blue">theor</font>y and practice: while theoretical analysis of most existing compression methods assumes compression is applied to the gradients of the entire model, many practical implementations operate individually on the gradients of each layer of the model. In this paper, we prove that layer-wise compression is, in theory, better, because the convergence rate is upper bounded by that of entire-model compression for a wide range of biased and unbiased compression methods. However, despite the theoretical bound, our experimental study of six well-known methods shows that convergence, in practice, may or may not be better, depending on the actual trained model and compression ratio. Our findings suggest that it would be advantageous for deep learning frameworks to include support for both layer-wise and entire-model compression. </br></br>

<a href='http://arxiv.org/pdf/1911.08553.pdf'>1911.08553</a> &nbsp&nbsp (cs:AI, cs:ML) &nbsp&nbsp -0.6135баллов, №794</br>
<b>Six Degree-of-Freedom Hovering using LIDAR Altimetry via Reinforcement\n  Meta-Learning</b></br>
Authors: , Gaudet, Brian, Linares, Richard, Furfaro, Roberto</br>
  We optimize a six degrees of freedom hovering policy using reinforcement meta-learning. The policy maps flash <font color="blue">LIDAR</font> measurements directly to on/off spacecraft body-frame thrust commands, allowing hovering at a fixed position and attitude in the asteroid body-fixed reference frame. Importantly, the policy does not require position and velocity estimates, and can operate in environments with unknown dynamics, and without an asteroid shape model or navigation aids. Indeed, during optimization the agent is confronted with a new randomly generated asteroid for each episode, insuring that it does not learn an asteroid\'s shape, texture, or environmental dynamics. This allows the deployed policy to generalize well to novel asteroid characteristics, which we demonstrate in our experiments. The hovering controller has the potential to simplify mission planning by allowing asteroid body-fixed hovering immediately upon the spacecraft\'s arrival to an asteroid. This in turn simplifies shape model generation and allows resource mapping via remote sensing immediately upon arrival at the target asteroid. </br></br>

<a href='http://arxiv.org/pdf/1911.05443.pdf'>1911.05443</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp -0.6135баллов, №795</br>
<b>Dynamic Connected Neural Decision Classifier and Regressor with Dynamic\n  Softing Pruning</b></br>
Authors: , Zhang, Faen, Fan, Xinyu, Xu, Hui, Zhou, Pengcheng, He, Yujian, Liu, Junlong</br>
  To deal with datasets of different complexity, this paper presents an efficient learning model that combines the proposed Dynamic Connected Neural Decision Networks (DNDN) and a new pruning method--Dynamic Soft Pruning (DSP). DNDN is a combination of <font color="blue">random forest</font>s and deep neural networks thereby it enjoys both the properties of powerful classification capability and representation learning functionality. Different from Deep Neural Decision Forests (DNDF), this paper adopts an end-to-end training approach by representing the classification distribution with multiple randomly initialized softmax layers, which enables the placement of the forest trees after each layer in the neural network and greatly improves the training speed and stability. Furthermore, DSP is proposed to reduce the redundant connections of the network in a soft fashion which has high flexibility but demonstrates no performance loss compared with previous approaches. Extensive experiments on different datasets demonstrate the superiority of the proposed model over other popular algorithms in solving classification tasks. </br></br>

<a href='http://arxiv.org/pdf/1911.08011.pdf'>1911.08011</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.6140баллов, №796</br>
<b>Adversarial Attacks on Grid Events Classification: An Adversarial\n  Machine Learning Approach</b></br>
Authors: , Niazazari, Iman, Livani, Hanif</br>
  With the ever-increasing reliance on data for data-driven applications in power grids, such as event cause analysis, the authenticity of data streams has become crucially important. The data can be prone to adversarial stealthy attacks aiming to manipulate the data such that residual-based bad data detectors cannot detect them, and the perception of system operators or event classifiers changes about the actual event. This paper investigates the impact of <font color="blue">adversarial att</font>acks on convolutional neural network-based event cause analysis frameworks. We have successfully verified the ability of adversaries to maliciously misclassify events through stealthy data manipulations. The vulnerability assessment is studied with respect to the number of compromised measurements. Furthermore, a defense mechanism to robustify the performance of the event cause analysis is proposed. The effectiveness of adversarial attacks on changing the output of the framework is studied using the data generated by real-time digital simulator (RTDS) under different scenarios such as type of attacks and level of access to data. </br></br>

<a href='http://arxiv.org/pdf/1911.07590.pdf'>1911.07590</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.6182баллов, №797</br>
<b>Signal Clustering with Class-independent Segmentation</b></br>
Authors: , Gasperini, Stefano, Paschali, Magdalini, Hopke, Carsten, Wittmann, David, Navab, Nassir</br>
  Radar signals have been dramatically increasing in complexity, limiting the source separation ability of traditional approaches. In this paper we propose a Deep Learning-based <font color="#be00be">clustering</font> method, which encodes concurrent signals into images, and, for the first time, tackles clustering with image <font color="#be00be">segmentation</font>. Novel loss functions are introduced to optimize a Neural Network to separate the input pulses into pure and non-fragmented clusters. <font color="#00be00">Outperform</font>ing a variety of baselines, the proposed approach is capable of clustering inputs directly with a Neural Network, in an end-to-end fashion. </br></br>

<a href='http://arxiv.org/pdf/1911.08871.pdf'>1911.08871</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.6197баллов, №798</br>
<b>CNAK : Cluster Number Assisted K-means</b></br>
Authors: , Saha, Jayasree, Mukherjee, Jayanta</br>
  Determining the number of clusters present in a dataset is an important problem in cluster analysis. Conventional <font color="#be00be">clustering</font> techniques generally assume this parameter to be provided up front. %user supplied. %Recently, robustness of any given clustering algorithm is analyzed to measure cluster stability/instability which in turn determines the cluster number. In this paper, we propose a method which analyzes cluster stability for predicting the cluster number. Under the same computational framework, the technique also finds representatives of the clusters. The method is apt for handling big data, as we design the algorithm using \\emph{Monte-Carlo} simulation. Also, we explore a few pertinent issues found to be of also clustering. Experiments reveal that the proposed method is capable of identifying a single cluster. It is robust in handling high dimensional dataset and performs reasonably well over datasets having cluster imbalance. Moreover, it can indicate cluster hierarchy, if present. Overall we have observed significant improvement in speed and quality for predicting cluster numbers as well as the composition of clusters in a large dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.09428.pdf'>1911.09428</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.6228баллов, №799</br>
<b>Single Image Super Resolution based on a Modified U-net with Mixed\n  Gradient Loss</b></br>
Authors: , Lu, Zhengyang, Chen, Ying</br>
  Single image <font color="#be00be">super-resolution</font> (SISR) is the task of inferring a high-resolution image from a single low-resolution image. Recent research on super-resolution has achieved great progress due to the development of deep convolutional neural networks in the field of computer vision. Existing super-resolution reconstruction methods have high performances in the criterion of Mean Square Error (MSE) but most methods fail to reconstruct an image with shape edges. To solve this problem, the mixed gradient error, which is composed by MSE and a weighted mean gradient error, is proposed in this work and applied to a modified U-net network as the loss function. The modified U-net removes all batch normalization layers and one of the convolution layers in each block. The operation reduces the number of parameters, and therefore accelerates the reconstruction. Compared with the existing image super-resolution algorithms, the proposed reconstruction method has better performance and time consumption. The experiments demonstrate that modified U-net network architecture with mixed gradient loss yields high-level results on three image datasets: SET14, BSD300, ICDAR2003. Code is available online. </br></br>

<a href='http://arxiv.org/pdf/1911.08530.pdf'>1911.08530</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.6228баллов, №800</br>
<b>Gromov-Wasserstein Factorization Models for Graph Clustering</b></br>
Authors: , Xu, Hongteng</br>
  We propose a new nonlinear factorization model for graphs that are with topological structures, and optionally, node attributes. This model is based on a pseudometric called Gromov-Wasserstein (GW) discrepancy, which compares graphs in a relational way. It estimates observed graphs as GW barycenters constructed by a set of atoms with different weights. By minimizing the GW discrepancy between each observed graph and its GW barycenter-based estimation, we learn the atoms and their weights associated with the observed graphs. The model achieves a novel and flexible factorization mechanism under GW discrepancy, in which both the observed graphs and the learnable atoms can be unaligned and with different sizes. We design an effective approximate algorithm for learning this Gromov-Wasserstein factorization (GWF) model, unrolling loopy computations as stacked modules and computing gradients with backpropagation. The stacked modules can be with two different architectures, which correspond to the proximal point algorithm (PPA) and Bregman alternating direction method of multipliers (BADMM), respectively. Experiments show that our model obtains encouraging results on <font color="#be00be">clustering</font> graphs. </br></br>

<a href='http://arxiv.org/pdf/1911.08644.pdf'>1911.08644</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.6229баллов, №801</br>
<b>Generate (non-software) Bugs to Fool Classifiers</b></br>
Authors: , Yakura, Hiromu, Akimoto, Youhei, Sakuma, Jun</br>
  In <font color="blue">adversarial att</font>acks intended to confound deep learning models, most studies have focused on limiting the magnitude of the modification so that humans do not notice the attack. On the other hand, during an attack against autonomous cars, for example, most drivers would not find it strange if a small insect image were placed on a stop sign, or they may overlook it. In this paper, we present a systematic approach to generate natural adversarial examples against classification models by employing such natural-appearing perturbations that imitate a certain object or signal. We first show the feasibility of this approach in an attack against an image classifier by employing generative adversarial networks that produce image patches that have the appearance of a natural object to fool the target model. We also introduce an algorithm to optimize placement of the perturbation in accordance with the input image, which makes the generation of adversarial examples fast and likely to succeed. Moreover, we experimentally show that the proposed approach can be extended to the audio domain, for example, to generate perturbations that sound like the chirping of birds to fool a speech classifier. </br></br>

<a href='http://arxiv.org/pdf/1911.07625.pdf'>1911.07625</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.6263баллов, №802</br>
<b>Deep-Gap: A deep learning framework for forecasting crowdsourcing\n  supply-demand gap based on imaging time series and residual learning</b></br>
Authors: , Said, Ahmed Ben, Erradi, Abdelkarim</br>
 <font color="#960096"> Mobile </font>crowdsourcing has become easier thanks to the widespread of smartphones capable of seamlessly collecting and pushing the desired data to cloud services. However, the success of mobile crowdsourcing relies on balancing the supply and demand by first accurately forecasting spatially and temporally the supply-demand gap, and then providing efficient incentives to encourage participant movements to maintain the desired balance. In this paper, we propose Deep-Gap, a deep learning approach based on residual learning to predict the gap between mobile crowdsourced service supply and demand at a given time and space. The prediction can drive the incentive model to achieve a geographically balanced service coverage in order to avoid the case where some areas are over-supplied while other areas are under-supplied. This allows anticipating the supply-demand gap and redirecting crowdsourced service providers towards target areas. Deep-Gap relies on historical supply-demand time series data as well as available external data such as <font color="#be00be">weather</font> conditions and day type (e.g., weekday, weekend, holiday). First, we roll and encode the time series of supply-demand as images using the Gramian Angular Summation Field (GASF), Gramian Angular Difference Field (GADF) and the Recurrence Plot (REC). These images are then used to train deep Convolutional Neural Networks (CNN) to extract the low and high-level features and forecast the crowdsourced services gap. We conduct comprehensive comparative study by establishing two supply-demand gap forecasting scenarios: with and without external data. Compared to state-of-art approaches, Deep-Gap achieves the lowest forecasting errors in both scenarios. </br></br>

<a href='http://arxiv.org/pdf/1911.07460.pdf'>1911.07460</a> &nbsp&nbsp (cs:ML, cs:CV) &nbsp&nbsp -0.6264баллов, №803</br>
<b>Walking the Tightrope: An Investigation of the Convolutional Autoencoder\n  Bottleneck</b></br>
Authors: , Manakov, Ilja, Rohm, Markus, Tresp, Volker</br>
  In this paper, we present an in-depth investigation of the convolutional autoencoder (CAE) bottleneck. Autoencoders (AE), and especially their convolutional variants, play a vital role in the current deep learning toolbox. Researchers and practitioners employ CAEs for a variety of tasks, ranging from <font color="#be00be">outlier</font> detection and compression to transfer and representation learning. Despite their widespread adoption, we have limited insight into how the bottleneck shape impacts the emergent properties of the CAE. We demonstrate that increased height and width of the bottleneck drastically improves generalization, which in turn leads to better performance of the latent codes in downstream transfer learning tasks. The number of channels in the bottleneck, on the other hand, is secondary in importance. Furthermore, we show empirically that, contrary to popular belief, CAEs do not learn to copy their input, even when the bottleneck has the same number of neurons as there are pixels in the input. Copying does not occur, despite training the CAE for 1,000 epochs on a tiny ($\\approx$ 600 images) dataset. We believe that the findings in this paper are directly applicable and will lead to improvements in models that rely on CAEs. </br></br>

<a href='http://arxiv.org/pdf/1911.07125.pdf'>1911.07125</a> &nbsp&nbsp (cs:AI, cs:ML) &nbsp&nbsp -0.6286баллов, №804</br>
<b>Opportunities for artificial intelligence in advancing precision\n  medicine</b></br>
Authors: , Filipp, Fabian V.</br>
  Machine learning (ML), deep learning (DL), and artificial intelligence (AI) are of increasing importance in bio<font color="blue">medic</font>ine. The goal of this work is to show progress in ML in digital health, to exemplify future needs and trends, and to identify any essential prerequisites of AI and ML for precision health. High-throughput technologies are delivering growing volumes of biomedical data, such as large-scale genome-wide sequencing assays, libraries of medical images, or drug perturbation screens of healthy, developing, and <font color="blue">diseas</font>ed tissue. Multi-omics data in biomedicine is deep and complex, offering an opportunity for data-driven insights and automated disease classification. Learning from these data will open our understanding and definition of healthy baselines and disease signatures. <font color="red">State-of-the-art</font> applications of deep neural networks include digital image recognition, single cell <font color="#be00be">clustering</font>, and virtual drug screens, demonstrating breadths and power of ML in biomedicine. Significantly, AI and systems biology have embraced big data challenges and may enable novel biotechnology-derived therapies to facilitate the implementation of precision medicine approaches. </br></br>

<a href='http://arxiv.org/pdf/1911.09508.pdf'>1911.09508</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.6288баллов, №805</br>
<b>Automatic Driver Identification from In-Vehicle Network Logs</b></br>
Authors: , Remeli, Mina, Lestyan, Szilvia, Acs, Gergely, Biczok, Gergely</br>
  Data generated by cars is growing at an unprecedented scale. As cars gradually become part of the Internet of Things (IoT) ecosystem, several stakeholders discover the value of in-vehicle network logs containing the measurements of the multitude of sensors deployed within the car. This wealth of data is also expected to be exploitable by third parties for the purpose of profiling drivers in order to provide personalized, valueadded services. Although several prior works have successfully demonstrated the feasibility of driver <font color="blue">re-identification</font> using the in-vehicle network data captured on the vehicle\'s CAN (Controller Area Network) bus, they inferred the identity of the driver only from known sensor signals (such as the vehicle\'s speed, brake pedal position, steering wheel angle, etc.) extracted from the CAN messages. However, car manufacturers intentionally do not reveal exact signal location and semantics within CAN logs. We show that the inference of driver identity is possible even with off-the-shelf machine learning techniques without reverse-engineering the CAN protocol. We demonstrate our approach on a dataset of 33 drivers and show that a driver can be re-identified and distinguished from other drivers with an accuracy of 75-85%. </br></br>

<a href='http://arxiv.org/pdf/1911.08895.pdf'>1911.08895</a> &nbsp&nbsp (cs:SD, cs:CL) &nbsp&nbsp -0.6292баллов, №806</br>
<b>Demystifying TasNet: A Dissecting Approach</b></br>
Authors: , Heitkaemper, Jens, Jakobeit, Darius, Boeddeker, Christoph, Drude, Lukas, Haeb-Umbach, Reinhold</br>
  In recent years time domain <font color="#be00be">speech separation</font> has excelled over frequency domain separation in single channel scenarios and noise-free environments. In this paper we dissect the gains of the time-domain audio separation network (TasNet) approach by gradually replacing components of an utterance-level permutation invariant training (u-PIT) based separation system in the frequency domain until the TasNet system is reached, thus blending components of frequency domain approaches with those of time domain approaches. Some of the intermediate variants achieve comparable signal-to-distortion ratio (SDR) gains to TasNet, but retain the advantage of frequency domain processing: compatibility with classic signal processing tools such as frequency-domain beamforming and the human <font color="#be00be">interpret</font>ability of the masks. Furthermore, we show that the scale invariant signal-to-distortion ratio (si-SDR) criterion used as loss function in TasNet is related to a logarithmic mean square error criterion and that it is this criterion which contributes most reliable to the performance advantage of TasNet. Finally, we critically assess which gains in a noise-free single channel environment generalize to more realistic reverberant conditions. </br></br>

<a href='http://arxiv.org/pdf/1911.06848.pdf'>1911.06848</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp -0.6326баллов, №807</br>
<b>Assigning Medical Codes at the Encounter Level by Paying Attention to\n  Documents</b></br>
Authors: , Shing, Han-Chin, Wang, Guoli, Resnik, Philip</br>
  The vast majority of research in computer assisted <font color="blue">medic</font>al coding focuses on coding at the document level, but a substantial proportion of medical coding in the <font color="#009600">real world</font> involves coding at the level of <font color="blue">clinic</font>al encounters, each of which is typically represented by a potentially large set of documents. We introduce encounter-level document attention networks, which use <font color="#00be00">hierarchical</font> attention to explicitly take the hierarchical structure of encounter documentation into account. Experimental evaluation demonstrates improvements in coding accuracy as well as facilitation of human reviewers in their ability to identify which documents within an encounter play a role in determining the encounter level codes. </br></br>

<a href='http://arxiv.org/pdf/1911.09450.pdf'>1911.09450</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp -0.6361баллов, №808</br>
<b>Few Shot Network Compression via Cross Distillation</b></br>
Authors: , Bai, Haoli, Wu, Jiaxiang, King, Irwin, Lyu, Michael</br>
  Model compression has been widely adopted to obtain light-weighted deep neural networks. Most prevalent methods, however, require fine-tuning with sufficient training data to ensure accuracy, which could be challenged by <font color="#be00be">privacy</font> and security issues. As a compromise between privacy and performance, in this paper we investigate few shot network compression: given few samples per class, how can we effectively compress the network with negligible performance drop? The core challenge of few shot network compression lies in high estimation errors from the original network during inference, since the compressed network can easily over-fits on the few training instances. The estimation errors could propagate and accumulate layer-wisely and finally deteriorate the network output. To address the problem, we propose cross distillation, a novel layer-wise knowledge distillation approach. By interweaving hidden layers of teacher and student network, layer-wisely accumulated estimation errors can be effectively reduced.The proposed method offers a general framework compatible with prevalent network compression techniques such as pruning. Extensive experiments on benchmark datasets demonstrate that cross distillation can significantly improve the student network\'s accuracy when only a few training instances are available. </br></br>

<a href='http://arxiv.org/pdf/1911.08790.pdf'>1911.08790</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.6390баллов, №809</br>
<b>Analysis of Deep Networks for Monocular Depth Estimation Through\n  Adversarial Attacks with Proposal of a Defense Method</b></br>
Authors: , Hu, Junjie, Okatani, Takayuki</br>
  In this paper, we consider <font color="blue">adversarial att</font>acks against a system of monocular depth estimation (MDE) based on convolutional neural networks (CNNs). The motivation is two-fold. One is to study the security of MDE systems, which has not been actively considered in the community. The other is to improve our understanding of the computational mechanism of CNNs performing MDE. Toward this end, we apply the method recently proposed for visualization of MDE to defending attacks. It trains another CNN to predict a saliency map from an input image, such that the CNN for MDE continues to accurately estimate the depth map from the image with its non-salient part masked out. We report the following findings. First, unsurprisingly, attacks by IFGSM (or equivalently PGD) succeed in making the CNNs yield inaccurate depth estimates. Second, the attacks can be defended by masking out non-salient pixels, indicating that the attacks function by perturbing mostly non-salient pixels. However, the prediction of saliency maps is itself vulnerable to the attacks, even though it is not the direct target of the attacks. We show that the attacks can be defended by using a saliency map predicted by a CNN trained to be robust to the attacks. These results provide an effective defense method as well as a clue to understanding the computational mechanism of CNNs for MDE. </br></br>

<a href='http://arxiv.org/pdf/1911.07268.pdf'>1911.07268</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.6438баллов, №810</br>
<b>On the well-posedness of uncalibrated photometric stereo under general\n  lighting</b></br>
Authors: , Brahimi, Mohammed, Qu&#xe9;au, Yvain, Haefner, Bjoern, Cremers, Daniel</br>
  Uncalibrated photometric stereo aims at estimating the 3D-shape of a surface, given a set of images captured from the same viewing angle, but under unknown, varying illumination. While the <font color="blue">theor</font>etical foundations of this inverse problem under directional lighting are well-established, there is a lack of mathematical evidence for the uniqueness of a solution under general lighting. On the other hand, stable and accurate heuristical solutions of uncalibrated photometric stereo under such general lighting have recently been proposed. The quality of the results demonstrated therein tends to indicate that the problem may actually be well-posed, but this still has to be established. The present paper addresses this theoretical issue, considering first-order spherical harmonics approximation of general lighting. Two important theoretical results are established. First, the orthographic integrability constraint ensures uniqueness of a solution up to a global concave-convex ambiguity, which had already been conjectured, yet not proven. Second, the perspective integrability constraint makes the problem well-posed, which generalizes a previous result limited to directional lighting. Eventually, a closed-form expression for the unique least-squares solution of the problem under perspective projection is provided, allowing numerical simulations on synthetic data to empirically validate our findings. </br></br>

<a href='http://arxiv.org/pdf/1911.08019.pdf'>1911.08019</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp -0.6503баллов, №811</br>
<b>Online Learned Continual Compression with Stacked Quantization Module</b></br>
Authors: , Caccia, Lucas, Belilovsky, Eugene, Caccia, Massimo, Pineau, Joelle</br>
  We introduce and study the problem of Online Continual Compression, where one attempts to learn to compress and store a representative dataset from a non i.i.d data stream, while only observing each sample once. This problem is highly relevant for downstream online continual learning tasks, as well as standard learning methods under resource constrained data collection. To address this we propose a new architecture which Stacks Quantization Modules (SQM), consisting of a series of discrete autoencoders, each equipped with their own memory. Every added module is trained to reconstruct the latent space of the previous module using fewer bits, allowing the learned representation to become more compact as training progresses. This modularity has several advantages: 1) moderate compressions are quickly available early in training, which is crucial for remembering the early tasks, 2) as more data needs to be stored, earlier data becomes more compressed, freeing memory, 3) unlike previous methods, our approach does not require pretraining, even on challenging datasets. We show several potential applications of this method. We first replace the episodic memory used in Experience Replay with SQM, leading to significant gains on standard continual learning benchmarks using a fixed memory budget. We then apply our method to online compression of larger images like those from Imagenet, and show that it is also effective with other modalities, such as <font color="blue">LiDAR</font> data. </br></br>

<a href='http://arxiv.org/pdf/1910.07454.pdf'>1910.07454</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.6543баллов, №812</br>
<b>An Exponential Learning Rate Schedule for Deep Learning</b></br>
Authors: , Li, Zhiyuan, Arora, Sanjeev</br>
  Intriguing empirical evidence exists that deep learning can work well with exoticschedules for varying the learning rate. This paper suggests that the phenomenon may be due to Batch Normalization or BN, which is ubiquitous and provides benefits in optimization and generalization across all standard architectures. The following new results are shown about BN with weight decay and momentum (in other words, the typical use case which was not considered in earlier <font color="blue">theor</font>etical analyses of stand-alone BN.   1. Training can be done using SGD with momentum and an exponentially increasing learning rate schedule, i.e., learning rate increases by some $(1 +\\alpha)$ factor in every epoch for some $\\alpha &gt;0$. (Precise statement in the paper.) To the best of our knowledge this is the first time such a rate schedule has been successfully used, let alone for highly successful architectures. As expected, such training rapidly blows up network weights, but the net stays well-behaved due to normalization.   2. Mathematical explanation of the success of the above rate schedule: a rigorous proof that it is equivalent to the standard setting of BN + SGD + StandardRate Tuning + Weight Decay + Momentum. This equivalence holds for other normalization layers as well, Group Normalization, LayerNormalization, Instance Norm, etc.   3. A worked-out toy example illustrating the above linkage of hyper-parameters. Using either weight decay or BN alone reaches global minimum, but convergence fails when both are used. </br></br>

<a href='http://arxiv.org/pdf/1911.08674.pdf'>1911.08674</a> &nbsp&nbsp (cs:ET) &nbsp&nbsp -0.6564баллов, №813</br>
<b>Toward a Wired Ad Hoc Nanonetwork</b></br>
Authors: , Dambri, Oussama Abderrahmane, Cherkaoui, Soumaya</br>
  Nanomachines promise to enable new <font color="blue">medic</font>al applications, including drug delivery and real time chemical reactions\' detection inside the human body. Such complex tasks need cooperation between nanomachines using a communication network. Wireless Ad hoc networks, using molecular or electromagnetic-based communication have been proposed in the literature to create flexible nanonetworks between nanomachines. In this paper, we propose a Wired Ad hoc NanoNETwork (WANNET) model design using actin-based nano-communication. In the proposed model, actin filaments self-assembly and disassembly is used to create flexible nanowires between nanomachines, and electrons are used as carriers of information. We give a general overview of the application layer, Medium Access Control (MAC) layer and a physical layer of the model. We also detail the analytical model of the physical layer using actin nanowire equivalent circuits, and we present an estimation of the circuit component\'s values. Numerical results of the derived model are provided in terms of attenuation, phase and delay as a function of the frequency and distances between nanomachines. The maximum throughput of the actin-based nanowire is also provided, and a comparison between the maximum throughput of the proposed WANNET, vs other proposed approaches is presented. The obtained results prove that the proposed wired ad hoc nanonetwork can give a very high achievable throughput with a smaller delay compared to other proposed wireless molecular communication networks. </br></br>

<a href='http://arxiv.org/pdf/1910.08962.pdf'>1910.08962</a> &nbsp&nbsp (cs:CL, stat:ML) &nbsp&nbsp -0.6566баллов, №814</br>
<b>Byte-Pair Encoding for Text-to-SQL Generation</b></br>
Authors: , M&#xfc;ller, Samuel, Vlachos, Andreas</br>
  Neural sequence-to-sequence models provide a <font color="#960096">competitive</font> approach to the task of mapping a question in natural language to an <font color="#be00be">SQL</font> query, also referred to as text-to-SQL generation. The Byte-Pair Encoding algorithm (BPE) has previously been used to improve machine translation (MT) between natural languages. In this work, we adapt BPE for text-to-SQL generation. As the datasets for this task are rather small compared to MT, we present a novel stopping criterion that prevents overfitting the BPE encoding to the training set. Additionally, we present AST BPE, which is a version of BPE that uses the Abstract Syntax Tree (AST) of the SQL statement to guide BPE merges and therefore produce BPE encodings that generalize better. We improved the accuracy of a strong attentive seq2seq baseline on five out of six English text-to-SQL tasks while reducing training time by more than 50% on four of them due to the shortened targets. Finally, on two of these tasks we exceeded previously reported accuracies. </br></br>

<a href='http://arxiv.org/pdf/1911.08054.pdf'>1911.08054</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.6566баллов, №815</br>
<b>Fair Learning-to-Rank from Implicit Feedback</b></br>
Authors: , Yadav, Himank, Du, Zhengxiao, Joachims, Thorsten</br>
  Addressing unfairness in rankings has become an increasingly important problem due to the growing influence of rankings in critical decision making, yet existing learning-to-rank algorithms suffer from multiple drawbacks when learning fair ranking policies from implicit feedback. Some algorithms suffer from extrinsic reasons of unfairness due to inherent selection biases in implicit feedback leading to rich-get-richer dynamics. While those that address the biased nature of implicit feedback suffer from intrinsic reasons of unfairness due to the lack of explicit control over the allocation of exposure based on merit (i.e, relevance). In both cases, the learned ranking policy can be unfair and lead to suboptimal results. To this end, we propose a novel learning-to-rank framework, FULTR, that is the first to address both intrinsic and extrinsic reasons of unfairness when learning ranking policies from logged implicit feedback. Considering the needs of various applications, we define a class of amortized fairness of exposure constraints with respect to items based on their merit, and propose corresponding counterfactual estimators of disparity (aka unfairness) and utility that are also robust to click noise. Furthermore, we provide an efficient algorithm that optimizes both utility and fairness via a policy-gradient approach. To show that our proposed algorithm learns accurate and fair ranking policies from biased and noisy feedback, we provide empirical results beyond the <font color="blue">theor</font>etical justification of the framework. </br></br>

<a href='http://arxiv.org/pdf/1911.09018.pdf'>1911.09018</a> &nbsp&nbsp (cs:SD) &nbsp&nbsp -0.6581баллов, №816</br>
<b>Moving to Communicate, Moving to Interact: Patterns of Body Motion in\n  Musical Duo Performance</b></br>
Authors: , Bishop, Laura, Cancino-Chac&#xf3;n, Carlos, Goebl, Werner</br>
  Skilled ensemble musicians coordinate with high precision, even when improvising or <font color="#be00be">interpret</font>ing loosely-defined notation. Successful coordination is supported primarily through shared attention to the musical output; however, musicians also interact visually, particularly when the musical timing is irregular. This study investigated the performance conditions that encourage visual signalling and interaction between ensemble members. Piano and clarinet duos rehearsed a new piece as their body motion was recorded. Analyses of head movement showed that performers communicated gesturally following held notes. Gesture patterns became more consistent as duos rehearsed, though consistency dropped again during a final performance given under no-visual-contact conditions. Movements were smoother and interperformer coordination was stronger during irregularly-timed passages than elsewhere in the piece, suggesting heightened visual interaction. Performers moved more after rehearsing than before, and more when they could see each other than when visual contact was occluded. Periods of temporal instability and increased familiarity with the<font color="#be00be"> music </font>and co-performer seem to encourage visual interaction, while specific communicative gestures are integrated into performance routines through rehearsal. We propose that visual interaction may support successful ensemble performance by affirming coordination throughout periods of temporal instability and serving as a social motivator to promote creative risk-taking. </br></br>

<a href='http://arxiv.org/pdf/1911.07661.pdf'>1911.07661</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.6584баллов, №817</br>
<b>Domain Generalization Using a Mixture of Multiple Latent Domains</b></br>
Authors: , Matsuura, Toshihiko, Harada, Tatsuya</br>
  When domains, which represent underlying data distributions, vary during training and testing processes, deep neural networks suffer a drop in their performance. Domain generalization allows improvements in the generalization performance for unseen target domains by using multiple source domains. Conventional methods assume that the domain to which each sample belongs is known in training. However, many datasets, such as those collected via web crawling, contain a mixture of multiple latent domains, in which the domain of each sample is unknown. This paper introduces domain generalization using a mixture of multiple latent domains as a novel and more realistic scenario, where we try to train a domain-generalized model without using domain labels. To address this scenario, we propose a method that iteratively divides samples into latent domains via <font color="#be00be">clustering</font>, and which trains the domain-invariant feature extractor shared among the divided latent domains via adversarial learning. We assume that the latent domain of images is reflected in their <font color="#be00be">style</font>, and thus, utilize style features for clustering. By using these features, our proposed method successfully discovers latent domains and achieves domain generalization even if the domain labels are not given. Experiments show that our proposed method can train a domain-generalized model without using domain labels. Moreover, it <font color="#00be00">outperform</font>s conventional domain generalization methods, including those that utilize domain labels. </br></br>

<a href='http://arxiv.org/pdf/1911.08348.pdf'>1911.08348</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp -0.6653баллов, №818</br>
<b>Live Face De-Identification in Video</b></br>
Authors: , Gafni, Oran, Wolf, Lior, Taigman, Yaniv</br>
  We propose a method for<font color="#be00be"> face </font>de-identification that enables fully automatic video modification at high frame rates. The goal is to maximally decorrelate the identity, while having the perception (pose, illumination and expression) fixed. We achieve this by a novel feed-forward encoder-decoder network architecture that is conditioned on the high-level representation of a person\'s<font color="#be00be"> facial </font>image. The network is global, in the sense that it does not need to be retrained for a given video or for a given identity, and it creates natural looking image sequences with little distortion in time. </br></br>

<a href='http://arxiv.org/pdf/1911.08793.pdf'>1911.08793</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.6702баллов, №819</br>
<b>A Framework for End-to-End Deep Learning-Based Anomaly Detection in\n  Transportation Networks</b></br>
Authors: , Davis, Neema, Raina, Gaurav, Jagannathan, Krishna</br>
  We develop an end-to-end deep learning-based <font color="#be00be">anomal</font>y detection model for temporal data in transportation networks. The proposed EVT-LSTM model is derived from the popular LSTM (Long Short-Term Memory) network and adopts an objective function that is based on fundamental results from EVT (Extreme Value <font color="blue">Theor</font>y). We compare the EVT-LSTM model with some established statistical, machine learning, and hybrid deep learning baselines. Experiments on seven diverse <font color="#009600">real-world</font> data sets demonstrate the superior anomaly detection performance of our proposed model over the other models considered in the comparison study. </br></br>

<a href='http://arxiv.org/pdf/1911.07292.pdf'>1911.07292</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.6703баллов, №820</br>
<b>Efficient Ridge Solutions for the Incremental Broad Learning System on\n  Added Inputs by Updating the Inverse or the Inverse Cholesky Factor of the\n  Hermitian matrix in the Ridge Inverse</b></br>
Authors: , Zhu, Hufei, Wei, Chenghao</br>
  This brief proposes two BLS algorithms to improve the existing BLS for new added inputs in [7]. The proposed BLS algorithms avoid computing the ridge inverse, by computing the ridge solution (i.e., the output weights) from the inverse or the inverse Cholesky factor of the Hermitian matrix in the ridge inverse. The proposed BLS algorithm 1 updates the inverse of the Hermitian matrix by the matrix inversion lemma [12]. To update the upper-triangular inverse Cholesky factor of the Hermitian matrix, the proposed BLS algorithm 2 multiplies the inverse Cholesky factor with an upper-triangular intermediate matrix, which is computed by a Cholesky factorization or an inverse Cholesky factorization. Assume that the newly added input matrix corresponding to the added inputs is p * k, where p and k are the number of added training samples and the total node number, respectively. When p &gt; k, the inverse of a sum of matrices [11] is utilized to compute the intermediate variables by a smaller matrix inverse in the proposed algorithm 1, or by a smaller inverse Cholesky factorization in the proposed algorithm 2. Usually the Hermitian matrix in the ridge inverse is smaller than the ridge inverse. Thus the proposed algorithms 1 and 2 require less flops (floating-point operations) than the existing BLS algorithm, which is verified by the <font color="blue">theor</font>etical flops calculation. In numerical experiments, the speedups for the case of p &gt; k in each additional training time of the proposed BLS algorithms 1 and 2 over the existing algorithm are 1.95 - 5.43 and 2.29 - 6.34, respectively, and the speedups for the case of p &lt; k are 8.83 - 10.21 and 2.28 - 2.58, respectively. </br></br>

<a href='http://arxiv.org/pdf/1911.08739.pdf'>1911.08739</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.6716баллов, №821</br>
<b>Vision: A Deep Learning Approach to provide walking assistance to the\n  visually impaired</b></br>
Authors: , Thakurdesai, Nikhil, Tripathi, Anupam, Butani, Dheeraj, Sankhe, Smita</br>
  Blind people<font color="#be00be"> face </font>a lot of problems in their daily routines. They have to struggle a lot just to do their day-to-day chores. In this paper, we have proposed a system with the objective to help the visually impaired by providing audio aid guiding them to avoid obstacles, which will assist them to move in their surroundings. <font color="#be00be">Object Detection</font> using YOLO will help them detect the nearby objects and Depth Estimation using monocular vision will tell the approximate distance of the detected objects from the user. Despite a higher accuracy, stereo vision has many hardware constraints, which makes monocular vision the preferred choice for this application. </br></br>

<a href='http://arxiv.org/pdf/1911.08678.pdf'>1911.08678</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.6738баллов, №822</br>
<b>Robust Triple-Matrix-Recovery-Based Auto-Weighted Label Propagation for\n  Classification</b></br>
Authors: , Zhang, Huan, Zhang, Zhao, Zhao, Mingbo, Ye, Qiaolin, Zhang, Min, Wang, Meng</br>
  The graph-based semi-supervised label propagation algorithm has delivered impressive classification results. However, the estimated soft labels typically contain mixed signs and noise, which cause inaccurate predictions due to the lack of suitable constraints. Moreover, available methods typically calculate the weights and estimate the labels in the original input space, which typically contains noise and corruption. Thus, the en-coded similarities and manifold smoothness may be inaccurate for label estimation. In this paper, we present effective schemes for resolving these issues and propose a novel and robust semi-supervised classification algorithm, namely, the tri-ple-matrix-recovery-based robust auto-weighted label propa-gation framework (ALP-TMR). Our ALP-TMR introduces a triple matrix recovery mechanism to remove noise or mixed signs from the estimated soft labels and improve the robustness to noise and <font color="#be00be">outlier</font>s in the steps of assigning weights and pre-dicting the labels simultaneously. Our method can jointly re-cover the underlying clean data, clean labels and clean weighting spaces by decomposing the original data, predicted soft labels or weights into a clean part plus an error part by fitting noise. In addition, ALP-TMR integrates the au-to-weighting process by minimizing reconstruction errors over the recovered clean data and clean soft labels, which can en-code the weights more accurately to improve both data rep-resentation and classification. By classifying samples in the recovered clean label and weight spaces, one can potentially improve the label prediction results. The results of extensive experiments demonstrated the satisfactory performance of our ALP-TMR. </br></br>

<a href='http://arxiv.org/pdf/1910.09359.pdf'>1910.09359</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.6767баллов, №823</br>
<b>Separable Convolutional Eigen-Filters (SCEF): Building Efficient CNNs\n  Using Redundancy Analysis</b></br>
Authors: , Scheidegger, Samuel, Yu, Yinan, McKelvey, Tomas</br>
  Deep Convolutional Neural Networks (CNNs) have been widely used in computer vision due to its effectiveness. While the high model complexity of CNN enables remarkable learning capacity, the large number of trainable parameters comes with a high cost. In addition to the demand of a large amount of resources, the high complexity of the network can result in a high variance in its generalization performance from a statistical learning <font color="blue">theor</font>y perspective. One way to reduce the complexity of a network without sacrificing its accuracy is to define and identify redundancies in order to remove them. In this work, we propose a method to observe and analyze redundancies in the weights of 2D convolutional (Conv2D) filters. From our experiments, we observe that 1) the vectorized Conv2D filters exhibit low rank behaviors; 2) the effective ranks of these filters typically decrease when the network goes deeper, and 3) these effective ranks are converging over training steps. Inspired by these observations, we propose a new layer called Separable Convolutional Eigen-Filters (SCEF) as an alternative parameterization to Conv2D filters. A SCEF layer can be easily implemented using the depthwise separable convolutions trained with our proposed training strategy. In addition to the decreased number of trainable parameters by using SCEF, depthwise separable convolutions are known to be more computationally efficient compared to Conv2D operations, which reduces the runtime FLOPs as well. Experiments are conducted on the CIFAR-10 and ImageNet datasets by replacing the Conv2D layers with SCEF. The results have shown an increased accuracy using about 2/3 of the original parameters and reduce the number of FLOPs to 2/3 of the base net. </br></br>

<a href='http://arxiv.org/pdf/1911.07575.pdf'>1911.07575</a> &nbsp&nbsp (cs:SD) &nbsp&nbsp -0.6803баллов, №824</br>
<b>A Spatial Sampling Approach to Wave Field Synthesis: PBAP and Huygens\n  Arrays</b></br>
Authors: , Smith III, Julius O.</br>
  A simple approach to microphone- and speaker-arrays is described in which the microphone array is regarded as a sampling grid for the acoustic field, and the corresponding speaker-array is treated as a &quot;spatial digital to analog converter&quot; that reconstructs the acoustic field from its spatial samples. Advantages of this approach include ease of understanding and teaching, ease of deployment, effective practical guidelines for deployment, and significant computational savings in special cases. In particular, in the far-field case (acoustic sources many wavelengths away from a linear array of speakers) it is possible to quantize source angles slightly so that no processing per speaker is required beyond pure integer delay. Smoothly moving sources are obtained using well known delay-line interpolation techniques such as linear (cross-fading) and Lagrange (polynomial) interpolation between/among speakers. We call the far-field line-array case Planewave-Based Angle Panning (PBAP), in reference to the well-known Vector-Based Amplitude Panning (VBAP) family of techniques, some of which are derived here as special cases: When speakers undersample the acoustic field, the result may be considered a form of VBAP, and VBAP is also obtained as a limiting case of polygonal PBAP arrays truncated to the polygon perimeter. Spatial samples need not be on a linear array, leading to a simple spatial audio system we call Huygens Arrays (HA). HAs are quite general for sources located behind the speaker array, which no longer needs to be linear, and the sources are no longer restricted to the far field. Multiband and hybrid arrays employing VBAP (or stereo) and subwoofer(s) are discussed, using sampling <font color="blue">theor</font>y to inform the choices of crossover frequencies. </br></br>

<a href='http://arxiv.org/pdf/1911.09026.pdf'>1911.09026</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.6809баллов, №825</br>
<b>Weak Supervision for Generating Pixel-Level Annotations in Scene Text\n  Segmentation</b></br>
Authors: , Bonechi, Simone, Andreini, Paolo, Bianchini, Monica, Scarselli, Franco</br>
  Providing pixel-level supervisions for scene text <font color="#be00be">segmentation</font> is inherently difficult and costly, so that only few small datasets are available for this task. To<font color="#be00be"> face </font>the scarcity of training data, previous approaches based on Convolutional Neural Networks (CNNs) rely on the use of a synthetic dataset for pre-training. However, synthetic data cannot reproduce the complexity and variability of natural images. In this work, we propose to use a weakly supervised learning approach to reduce the domain-shift between synthetic and real data. Leveraging the bounding-box supervision of the COCO-Text and the MLT datasets, we generate weak pixel-level supervisions of real images. In particular, the COCO-Text-Segmentation (COCO_TS) and the MLT-Segmentation (MLT_S) datasets are created and released. These two datasets are used to train a CNN, the Segmentation Multiscale Attention Network (SMANet), which is specifically designed to face some peculiarities of the scene text segmentation task. The SMANet is trained end-to-end on the proposed datasets, and the experiments show that COCO_TS and MLT_S are a valid alternative to synthetic images, allowing to use only a fraction of the training samples and improving significantly the performances. </br></br>

<a href='http://arxiv.org/pdf/1911.08141.pdf'>1911.08141</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.6882баллов, №826</br>
<b>Tell Me What They\'re Holding: Weakly-supervised Object Detection with\n  Transferable Knowledge from Human-object Interaction</b></br>
Authors: , Kim, Daesik, Lee, Gyujeong, Jeong, Jisoo, Kwak, Nojun</br>
  In this work, we introduce a novel weakly supervised <font color="#be00be">object detection</font> (WSOD) paradigm to detect objects belonging to rare classes that have not many examples using transferable knowledge from human-object interactions (HOI). While WSOD shows lower performance than full supervision, we mainly focus on HOI as the main context which can strongly supervise complex semantics in images. Therefore, we propose a novel module called RRPN (relational region proposal network) which outputs an object-localizing attention map only with human poses and action verbs. In the source domain, we fully train an object detector and the RRPN with full supervision of HOI. With transferred knowledge about localization map from the trained RRPN, a new object detector can learn unseen objects with weak verbal supervision of HOI without bounding box annotations in the target domain. Because the RRPN is designed as an add-on type, we can apply it not only to the object detection but also to other domains such as semantic <font color="#be00be">segmentation</font>. The experimental results on HICO-DET dataset show the possibility that the proposed method can be a cheap alternative for the current supervised object detection paradigm. Moreover, qualitative results demonstrate that our model can properly localize unseen objects on HICO-DET and V-COCO datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.05781.pdf'>1911.05781</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.6888баллов, №827</br>
<b>Learning internal representations</b></br>
Authors: , Baxter, Jonathan</br>
  Probably the most important problem in machine learning is the preliminary biasing of a learner\'s hypothesis space so that it is small enough to ensure good generalisation from reasonable training sets, yet large enough that it contains a good solution to the problem being learnt. In this paper a mechanism for {\\em automatically} learning or biasing the learner\'s hypothesis space is introduced. It works by first learning an appropriate {\\em internal representation} for a learning environment and then using that representation to bias the learner\'s hypothesis space for the learning of future tasks drawn from the same environment.   An internal representation must be learnt by sampling from {\\em many similar tasks}, not just a single task as occurs in ordinary machine learning. It is proved that the number of examples $m$ {\\em per task} required to ensure good generalisation from a representation learner obeys $m = O(a+b/n)$ where $n$ is the number of tasks being learnt and $a$ and $b$ are constants. If the tasks are learnt independently ({\\em i.e.} without a common representation) then $m=O(a+b)$. It is argued that for learning environments such as speech and character recognition $b\\gg a$ and hence representation learning in these environments can potentially yield a drastic reduction in the number of examples required per task. It is also proved that if $n = O(b)$ (with $m=O(a+b/n)$) then the representation learnt will be good for learning novel tasks from the same environment, and that the number of examples required to generalise well on a novel task will be reduced to $O(a)$ (as opposed to $O(a+b)$ if no representation is used).   It is shown that gradient descent can be used to train neural network representations and experiment results are reported providing strong qualitative support for the <font color="blue">theor</font>etical results. </br></br>

<a href='http://arxiv.org/pdf/1911.09315.pdf'>1911.09315</a> &nbsp&nbsp (cs:ML, cs:AI) &nbsp&nbsp -0.6916баллов, №828</br>
<b>Rule Extraction in Unsupervised Anomaly Detection for Model\n  Explainability: Application to OneClass SVM</b></br>
Authors: , Barbado, Alberto, Corcho, &#xd3;scar</br>
  OneClass <font color="blue">SVM</font> is a popular method for unsupervised <font color="#be00be">anomal</font>y detection. As many other methods, it suffers from the \\textit{black box} problem: it is difficult to justify, in an intuitive and simple manner, why the decision frontier is identifying data points as anomalous or non anomalous. Such type of problem is being widely addressed for supervised models. However, it is still an uncharted area for unsupervised learning. In this paper, we describe a method to infer rules that justify why a point is labelled as an anomaly, so as to obtain intuitive explanations for models created using the OneClass SVM algorithm. We evaluate our proposal with different datasets, including <font color="#009600">real-world</font> data coming from industry. With this, our proposal contributes to extend Explainable AI techniques to unsupervised machine learning models. </br></br>

<a href='http://arxiv.org/pdf/1911.08350.pdf'>1911.08350</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.7025баллов, №829</br>
<b>Solar Event Tracking with Deep Regression Networks: A Proof of Concept\n  Evaluation</b></br>
Authors: , Sarker, Toqi Tahamid, Banda, Juan M.</br>
  With the advent of deep learning for computer vision tasks, the need for accurately labeled data in large volumes is vital for any application. The increasingly available large amounts of solar image data generated by the Solar Dynamic Observatory (SDO) mission make this domain particularly interesting for the development and testing of deep learning systems. The currently available labeled solar data is generated by the SDO mission\'s Feature Finding Team\'s (FFT) specialized detection modules. The major drawback of these modules is that detection and labeling is performed with a cadence of every 4 to 12 hours, depending on the module. Since SDO image data products are created every 10 seconds, there is a considerable gap between labeled observations and the continuous data stream. In order to address this shortcoming, we trained a deep <font color="#be00be">regression</font> network to track the movement of two solar phenomena: Active Region and Coronal Hole events. To the best of our knowledge, this is the first attempt of solar event <font color="#be00be">tracking</font> using a deep learning approach. Since it is impossible to fully evaluate the performance of the suggested event tracks with the original data (only partial ground truth is available), we demonstrate with several metrics the effectiveness of our approach. With the purpose of generating continuously labeled solar image data, we present this feasibility analysis showing the great promise of deep regression networks for this task. </br></br>

<a href='http://arxiv.org/pdf/1911.08780.pdf'>1911.08780</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp -0.7293баллов, №830</br>
<b>LionForests: Local Interpretation of Random Forests through Path\n  Selection</b></br>
Authors: , Mollas, Ioannis, Tsoumakas, Grigorios, Bassiliades, Nick</br>
  Towards a future where machine learning systems will integrate into every aspect of people\'s lives, researching methods to <font color="#be00be">interpret</font> such systems is necessary, instead of focusing exclusively on enhancing their performance. Enriching the trust between these systems and people will accelerate this integration process. Many <font color="blue">medic</font>al and retail banking/<font color="#be00be">financ</font>e applications use <font color="red">state-of-the-art</font> machine learning techniques to predict certain aspects of new instances. Tree ensembles, like <font color="blue">random forest</font>s, are widely acceptable solutions on these tasks, while at the same time they are avoided due to their black-box uninterpretable nature, creating an unreasonable paradox. In this paper, we provide a sequence of actions for shedding light on the predictions of the misjudged family of tree ensemble algorithms. Using classic unsupervised learning techniques and an enhanced similarity metric, to wander among transparent trees inside a forest following breadcrumbs, the interpretable essence of tree ensembles arises. An explanation provided by these systems using our approach, which we call &quot;LionForests&quot;, can be a simple, comprehensive rule. </br></br>

<a href='http://arxiv.org/pdf/1911.07819.pdf'>1911.07819</a> &nbsp&nbsp (cs:CL, cs:ML, stat:ML) &nbsp&nbsp -0.7307баллов, №831</br>
<b>Drug Repurposing for Cancer: An NLP Approach to Identify Low-Cost\n  Therapies</b></br>
Authors: , Subramanian, Shivashankar, Baldini, Ioana, Ravichandran, Sushma, Katz-Rogozhnikov, Dmitriy A., Ramamurthy, Karthikeyan Natesan, Sattigeri, Prasanna, Varshney, Kush R., Wang, Annmarie, Mangalath, Pradeep, Kleiman, Laura B.</br>
  More than 200 generic drugs approved by the U.S. Food and Drug Administration for non-<font color="#be00be">cancer</font> indications have shown promise for treating cancer. Due to their long history of safe <font color="blue">patient</font> use, low cost, and widespread availability, repurposing of generic drugs represents a major opportunity to rapidly improve outcomes for cancer patients and reduce healthcare costs worldwide. Evidence on the efficacy of non-cancer generic drugs being tested for cancer exists in scientific publications, but trying to manually identify and extract such evidence is intractable. In this paper, we introduce a system to automate this evidence extraction from PubMed abstracts. Our primary contribution is to define the natural language processing pipeline required to obtain such evidence, comprising the following modules: querying, filtering, cancer type entity extraction, therapeutic association classification, and study type classification. Using the subject matter expertise on our team, we create our own datasets for these specialized domain-specific tasks. We obtain promising performance in each of the modules by utilizing modern language modeling techniques and plan to treat them as baseline approaches for future improvement of individual components. </br></br>

<a href='http://arxiv.org/pdf/1911.08673.pdf'>1911.08673</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.7315баллов, №832</br>
<b>Global Greedy Dependency Parsing</b></br>
Authors: , Li, Zuchao, Zhao, Hai, Parnow, Kevin</br>
  Most syntactic dependency <font color="#be00be">parsing</font> models may fall into one of two categories: transition- and graph-based models. The former models enjoy high inference efficiency with linear time complexity, but they rely on the stacking or re-ranking of partially-built parse trees to build a complete parse tree and are stuck with slower training for the necessity of dynamic oracle training. The latter, graph-based models, may boast better performance but are unfortunately marred by polynomial time inference. In this paper, we propose a novel parsing order objective, resulting in a novel dependency parsing model capable of both global (in sentence scope) feature extraction as in graph models and linear time inference as in transitional models. The proposed global greedy <font color="#be00be">parser</font> only uses two arc-building actions, left and right arcs, for projective parsing. When equipped with two extra non-projective arc-building actions, the proposed parser may also smoothly support non-projective parsing. Using multiple benchmark treebanks, including the Penn Treebank (PTB), the CoNLL-X treebanks, and the Universal Dependency Treebanks, we evaluate our parser and demonstrate that the proposed novel parser achieves good performance with faster training and decoding. </br></br>

<a href='http://arxiv.org/pdf/1910.10404.pdf'>1910.10404</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.7442баллов, №833</br>
<b>INTEL-TAU: A Color Constancy Dataset</b></br>
Authors: , Laakom, Firas, Raitoharju, Jenni, Iosifidis, Alexandros, Nikkanen, Jarno, Gabbouj, Moncef</br>
  In this paper, we describe a new large dataset for illumination estimation. This dataset, called INTEL-TAU, contains 7022 images in total, which makes it the largest available high-resolution dataset for illumination estimation research. The variety of scenes captured using three different camera models, i.e., Canon 5DSR, Nikon D810, and Sony IMX135, makes the dataset appropriate for evaluating the camera and scene invariance of the different illumination estimation techniques. <font color="#be00be">Privacy</font> masking is done for sensitive information, e.g., faces. Thus, the dataset is coherent with the new General Data Protection Regulation (GDPR) regulations. Furthermore, the effect of color shading for<font color="#960096"> mobile </font>images can be evaluated with INTEL-TAU, as we provide both corrected and uncorrected versions of the raw data. We provide in this paper evaluation of several color constancy approaches </br></br>

<a href='http://arxiv.org/pdf/1911.08779.pdf'>1911.08779</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.7504баллов, №834</br>
<b>Characterizing Scalability of Sparse Matrix-Vector Multiplications on\n  Phytium FT-2000+ Many-cores</b></br>
Authors: , Chen, Donglin, Fang, Jianbin, Xu, Chuanfu, Chen, Shizhao, Wang, Zheng</br>
  Understanding the scalability of parallel programs is crucial for software optimization and hardware architecture design. As HPC hardware is moving towards many-core design, it becomes increasingly difficult for a parallel program to make effective use of all available processor cores. This makes scalability analysis increasingly important. This paper presents a quantitative study for characterizing the scalability of sparse matrix-vector multiplications (SpMV) on Phytium FT-2000+, an ARM-based many-core architecture for HPC computing. We choose to study SpMV as it is a common operation in scientific and HPC applications. Due to the newness of ARM-based many-core architectures, there is little work on understanding the SpMV scalability on such hardware design. To close the gap, we carry out a large-scale empirical evaluation involved over 1,000 representative SpMV datasets. We show that, while many computation-intensive SpMV applications contain extensive parallelism, achieving a linear speedup is non-trivial on Phytium FT-2000+. To better understand what software and hardware parameters are most important for determining the scalability of a given SpMV <font color="blue">kernel</font>, we develop a performance analytical model based on the <font color="#be00be">regression</font> tree. We show that our model is highly effective in characterizing SpMV scalability, offering useful insights to help application developers for better optimizing SpMV on an emerging HPC architecture. </br></br>

<a href='http://arxiv.org/pdf/1911.09560.pdf'>1911.09560</a> &nbsp&nbsp (cs:ML, cs:NE, stat:ML) &nbsp&nbsp -0.7531баллов, №835</br>
<b>Memory-Efficient Episodic Control Reinforcement Learning with Dynamic\n  Online k-means</b></br>
Authors: , Agostinelli, Andrea, Arulkumaran, Kai, Sarrico, Marta, Richemond, Pierre, Bharath, Anil Anthony</br>
  Recently, neuro-inspired episodic control (EC) methods have been developed to overcome the data-inefficiency of standard deep <font color="#00be00">reinforcement learning</font> approaches. Using non-/semi-parametric models to estimate the value function, they learn rapidly, retrieving cached values from similar past states. In realistic scenarios, with limited resources and noisy data, maintaining meaningful representations in memory is essential to speed up the learning and avoid catastrophic forgetting. Unfortunately, EC methods have a large space and time complexity. We investigate different solutions to these problems based on prioritising and ranking stored states, as well as online <font color="#be00be">clustering</font> techniques. We also propose a new dynamic online <font color="blue">k-mean</font>s algorithm that is both computationally-efficient and yields significantly better performance at smaller memory sizes; we validate this approach on classic reinforcement learning environments and Atari games. </br></br>

<a href='http://arxiv.org/pdf/1911.08459.pdf'>1911.08459</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -0.7593баллов, №836</br>
<b>Deep Unsupervised Clustering with Clustered Generator Model</b></br>
Authors: , Zhu, Dandan, Han, Tian, Zhou, Linqi, Yang, Xiaokang, Wu, Ying Nian</br>
  This paper addresses the problem of unsupervised <font color="#be00be">clustering</font> which remains one of the most fundamental challenges in machine learning and artificial intelligence. We propose the clustered generator model for clustering which contains both continuous and discrete latent variables. Discrete latent variables model the cluster label while the continuous ones model variations within each cluster. The learning of the model proceeds in a unified probabilistic framework and incorporates the unsupervised clustering as an inner step without the need for an extra inference model as in existing variational-based models. The latent variables learned serve as both observed data embedding or latent representation for data distribution. Our experiments show that the proposed model can achieve <font color="#960096">competitive</font> unsupervised clustering accuracy and can learn disentangled latent representations to generate realistic samples. In addition, the model can be naturally extended to per-pixel unsupervised clustering which remains largely unexplored. </br></br>

<a href='http://arxiv.org/pdf/1911.08426.pdf'>1911.08426</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.7597баллов, №837</br>
<b>A Study on various state of the art of the Art Face Recognition System\n  using Deep Learning Techniques</b></br>
Authors: , Chokkadi, Sukhada, S, Sannidhan M, B, Sudeepa K, Bhandary, Abhir</br>
  Considering the existence of very large amount of available data repositories and reach to the very advanced system of hardware, systems meant for<font color="#be00be"> facial </font>identification ave evolved enormously over the past few decades. Sketch recognition is one of the most important areas that have evolved as an integral component adopted by the agencies of law administration in current trends of forensic science. Matching of derived sketches to photo images of<font color="#be00be"> face </font>is also a difficult assignment as the considered sketches are produced upon the verbal explanation depicted by the eye witness of the crime scene and may have scarcity of sensitive elements that exist in the photograph as one can accurately depict due to the natural human error. Substantial amount of the novel research work carried out in this area up late used recognition system through traditional extraction and classification models. But very recently, few researches work focused on using deep learning techniques to take an advantage of learning models for the feature extraction and classification to rule out potential domain challenges. The first part of this review paper basically focuses on deep learning techniques used in face recognition and matching which as improved the accuracy of face recognition technique with training of huge sets of data. This paper also includes a survey on different techniques used to match composite sketches to human images which includes component-based representation approach, automatic composite sketch recognition technique etc. </br></br>

<a href='http://arxiv.org/pdf/1911.07041.pdf'>1911.07041</a> &nbsp&nbsp (cs:SD, cs:ML) &nbsp&nbsp -0.7623баллов, №838</br>
<b>Music theme recognition using CNN and self-attention</b></br>
Authors: , Sukhavasi, Manoj, Adapa, Sainath</br>
  We present an efficient architecture to detect mood/themes in<font color="#be00be"> music </font>tracks on autotagging-moodtheme subset of the MTG-Jamendo dataset. Our approach consists of two blocks, a CNN block based on MobileNetV2 architecture and a self-attention block from Transformer architecture to capture long term temporal characteristics. We show that our proposed model produces a significant improvement over the baseline model. Our model (team name: AMLAG) achieves 4th place on PR-AUC-macro Leaderboard in MediaEval 2019: <font color="#be00be">Emotion</font> and Theme Recognition in Music Using Jamendo. </br></br>

<a href='http://arxiv.org/pdf/1911.03726.pdf'>1911.03726</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.7634баллов, №839</br>
<b>Vietnamese transition-based dependency parsing with supertag features</b></br>
Authors: , Van Nguyen, Kiet, Nguyen, Ngan Luu-Thuy</br>
  In recent years, dependency <font color="#be00be">parsing</font> is a fascinating research topic and has a lot of applications in natural language processing. In this paper, we present an effective approach to improve dependency parsing by utilizing supertag features. We performed experiments with the transition-based dependency parsing approach because it can take advantage of rich features. Empirical evaluation on <font color="#be00be">Vietnamese</font> Dependency Treebank showed that, we achieved an improvement of 18.92% in labeled attachment score with gold supertags and an improvement of 3.57% with automatic supertags. </br></br>

<a href='http://arxiv.org/pdf/1911.08782.pdf'>1911.08782</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.7634баллов, №840</br>
<b>Joint Emotion Label Space Modelling for Affect Lexica</b></br>
Authors: , De Bruyne, Luna, Atanasova, Pepa, Augenstein, Isabelle</br>
  <font color="#be00be">Emotion</font> lexica are commonly used resources to combat data poverty in automatic emotion detection. However, methodological issues emerge when employing them: lexica are often not very extensive, and the way they are constructed can vary widely -- from lab conditions to crowdsourced approaches and distant supervision. Furthermore, both categorical frameworks and dimensional frameworks coexist, in which <font color="blue">theor</font>ists provide many different sets of categorical labels or dimensional axes. The heterogenous nature of the resulting emotion detection resources results in a need for a unified approach to utilising them. This paper contributes to the field of emotion analysis in NLP by a) presenting the first study to unify existing emotion detection resources automatically and thus learn more about the relationships between them; b) exploring the use of existing lexica for the above-mentioned task; c) presenting an approach to automatically combining emotion lexica, namely by a multi-view variational auto-encoder (VAE), which facilitates the mapping of datasets into a joint emotion label space. We test the utility of joint emotion lexica by using them as additional features in state-of-the art emotion detection models. Our overall findings are that emotion lexica can offer complementary information to even extremely large pre-trained models such as BERT. The performance of our models is comparable to state-of-the art models that are specifically engineered for certain datasets, and even <font color="#00be00">outperform</font> the state-of-the art on four datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.07693.pdf'>1911.07693</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.7675баллов, №841</br>
<b>A Multi-Task Gradient Descent Method for Multi-Label Learning</b></br>
Authors: , Bai, Lu, Ong, Yew-Soon, He, Tiantian, Gupta, Abhishek</br>
  Multi-label learning studies the problem where an instance is associated with a set of labels. By treating single-label learning problem as one task, the multi-label learning problem can be casted as solving multiple related tasks simultaneously. In this paper, we propose a novel Multi-task Gradient Descent (MGD) algorithm to solve a group of related tasks simultaneously. In the proposed algorithm, each task minimizes its individual cost function using reformative gradient descent, where the relations among the tasks are facilitated through effectively transferring model parameter values across multiple tasks. <font color="blue">Theor</font>etical analysis shows that the proposed algorithm is convergent with a proper transfer mechanism. Compared with the existing approaches, MGD is easy to implement, has less requirement on the training model, can achieve seamless asymmetric transformation such that negative transfer is mitigated, and can benefit from parallel computing when the number of tasks is large. The <font color="#960096">competitive</font> experimental results on multi-label learning datasets validate the effectiveness of the proposed algorithm. </br></br>

<a href='http://arxiv.org/pdf/1911.07391.pdf'>1911.07391</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.7707баллов, №842</br>
<b>Justification-Based Reliability in Machine Learning</b></br>
Authors: , Virani, Nurali, Iyer, Naresh, Yang, Zhaoyuan</br>
  With the advent of Deep Learning, the field of machine learning (ML) has surpassed <font color="#00be00">human-level</font> performance on diverse classification tasks. At the same time, there is a stark need to characterize and quantify reliability of a model\'s prediction on individual samples. This is especially true in application of such models in safety-critical domains of industrial control and healthcare. To address this need, we link the question of reliability of a model\'s individual prediction to the epistemic uncertainty of the model\'s prediction. More specifically, we extend the <font color="blue">theor</font>y of Justified True Belief (JTB) in epistemology, created to study the validity and limits of human-acquired knowledge, towards characterizing the validity and limits of knowledge in supervised classifiers. We present an analysis of neural network classifiers linking the reliability of its prediction on an input to characteristics of the support gathered from the input and latent spaces of the network. We hypothesize that the JTB analysis exposes the epistemic uncertainty (or ignorance) of a model with respect to its inference, thereby allowing for the inference to be only as strong as the justification permits. We explore various forms of support (for e.g., k-<font color="#be00be">nearest neighbo</font>rs (k-NN) and l_p-norm based) generated for an input, using the training data to construct a justification for the prediction with that input. Through experiments conducted on simulated and real datasets, we demonstrate that our approach can provide reliability for individual predictions and characterize regions where such reliability cannot be ascertained. </br></br>

<a href='http://arxiv.org/pdf/1911.09053.pdf'>1911.09053</a> &nbsp&nbsp (cs:CV, cs:ML, stat:ML) &nbsp&nbsp -0.7811баллов, №843</br>
<b>Utility Analysis of Network Architectures for 3D Point Cloud Processing</b></br>
Authors: , Huang, Shikun, Zhang, Binbin, Shen, Wen, Wei, Zhihua, Zhang, Quanshi</br>
  In this paper, we <font color="blue">diagnos</font>e deep neural networks for 3D <font color="#be00be">point cloud</font> processing to explore utilities of different network architectures. We propose a number of hypotheses on the effects of specific network architectures on the representation capacity of DNNs. In order to prove the hypotheses, we design five metrics to diagnose various types of DNNs from the following perspectives, information discarding, information concentration, rotation robustness, adversarial robustness, and neighborhood inconsistency. We conduct comparative studies based on such metrics to verify the hypotheses. We further use the verified hypotheses to revise architectures of existing DNNs to improve their utilities. Experiments demonstrate the effectiveness of our method. </br></br>

<a href='http://arxiv.org/pdf/1911.06716.pdf'>1911.06716</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.7815баллов, №844</br>
<b>A Generalized Markov Chain Model to Capture Dynamic Preferences and\n  Choice Overload</b></br>
Authors: , Goutam, Kumar, Goyal, Vineet, Soret, Agathe</br>
  Assortment optimization is an important problem that arises in many practical applications such as retailing and online advertising where the goal is to find a subset of products from a universe of substitutable products that maximize a seller\'s expected revenue. The demand and the revenue depend on the substitution behavior of the <font color="#be00be">customer</font>s that is captured by a choice model. One of the key challenges is to find the right model for the customer substitution behavior. Many parametric random utility based models have been considered in the literature to capture substitution. However, in all these models, the probability of purchase increases as we add more options to the assortment. This is not true in general and in many settings, the probability of purchase may decrease if we add more products to the assortment, referred to as the choice overload. In this paper we attempt to address these serious limitations and propose a generalization of the Markov chain based choice model considered in Blanchet et al. In particular, we handle dynamic preferences and the choice overload phenomenon using a Markovian comparison model that is a generalization of the Markovian substitution framework of Blanchet et al. The Markovian comparison framework allows us to implicitly model the search cost in the choice process and thereby, modeling both dynamic preferences as well as the choice overload phenomenon. We consider the assortment optimization problem for the special case of our generalized Markov chain model where the underlying Markov chain is rank-1 (this is a generalization of the Multinomial Logit model). We show that the assortment optimization problem under this model is NP-hard and present a fully polynomial-time approximation scheme (FPTAS) for this problem. </br></br>

<a href='http://arxiv.org/pdf/1911.06876.pdf'>1911.06876</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp -0.7817баллов, №845</br>
<b>Explanatory Masks for Neural Network Interpretability</b></br>
Authors: , Phillips, Lawrence, Goh, Garrett, Hodas, Nathan</br>
  Neural network <font color="#be00be">interpret</font>ability is a vital component for applications across a wide variety of domains. In such cases it is often useful to analyze a network which has already been trained for its specific purpose. In this work, we develop a method to produce explanation masks for pre-trained networks. The mask localizes the most important aspects of each input for prediction of the original network. Masks are created by a secondary network whose goal is to create as small an explanation as possible while still preserving the predictive accuracy of the original network. We demonstrate the applicability of our method for image classification with CNNs, <font color="#be00be">sentiment</font> analysis with RNNs, and chemical property prediction with mixed CNN/RNN architectures. </br></br>

<a href='http://arxiv.org/pdf/1911.09411.pdf'>1911.09411</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -0.7850баллов, №846</br>
<b>Random Machines: A bagged-weighted support vector model with free kernel\n  choice</b></br>
Authors: , Ara, Anderson, Maia, Mateus, Mac&#xea;do, Samuel, Louzada, Francisco</br>
  Improvement of statistical learning models in order to increase efficiency in solving classification or <font color="#be00be">regression</font> problems is still a goal pursued by the scientific community. In this way, the support vector machine model is one of the most successful and powerful algorithms for those tasks. However, its performance depends directly from the choice of the <font color="blue">kernel</font> function and their hyperparameters. The traditional choice of them, actually, can be computationally expensive to do the kernel choice and the tuning processes. In this article, it is proposed a novel framework to deal with the kernel function selection called Random Machines. The results improved accuracy and reduced computational time. The data study was performed in simulated data and over 27 real benchmarking datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.07846.pdf'>1911.07846</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.7891баллов, №847</br>
<b>Multiple Face Analyses through Adversarial Learning</b></br>
Authors: , Wang, Shangfei, Yin, Shi, Hao, Longfei, Liang, Guang</br>
  This inherent relations among multiple<font color="#be00be"> face </font>analysis tasks, such as landmark detection, head pose estimation, gender recognition and face attribute estimation are crucial to boost the performance of each task, but have not been thoroughly explored since typically these multiple face analysis tasks are handled as separate tasks. In this paper, we propose a novel deep multi-task adversarial learning method to localize<font color="#be00be"> facial </font>landmark, estimate head pose and recognize gender jointly or estimate multiple face attributes simultaneously through exploring their dependencies from both image representation-level and label-level. Specifically, the proposed method consists of a deep recognition network R and a discriminator D. The deep recognition network is used to learn the shared middle-level image representation and conducts multiple face analysis tasks simultaneously. Through multi-task learning mechanism, the recognition network explores the dependencies among multiple face analysis tasks, such as facial landmark localization, head pose estimation, gender recognition and face attribute estimation from image representation-level. The discriminator is introduced to enforce the distribution of the multiple face analysis tasks to converge to that inherent in the ground-truth labels. During training, the recognizer tries to confuse the discriminator, while the discriminator competes with the recognizer through distinguishing the predicted label combination from the ground-truth one. Though adversarial learning, we explore the dependencies among multiple face analysis tasks from label-level. Experimental results on four benchmark databases, i.e., the AFLW database, the Multi-PIE database, the CelebA database and the LFWA database, demonstrate the effectiveness of the proposed method for multiple face analyses. </br></br>

<a href='http://arxiv.org/pdf/1911.05811.pdf'>1911.05811</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.7924баллов, №848</br>
<b>Triply Robust Off-Policy Evaluation</b></br>
Authors: , Liu, Anqi, Liu, Hao, Anandkumar, Anima, Yue, Yisong</br>
  We propose a robust <font color="#be00be">regression</font> approach to off-policy evaluation (OPE) for contextual <font color="blue">bandit</font>s. We frame OPE as a covariate-shift problem and leverage modern robust regression tools. Ours is a general approach that can be used to augment any existing OPE method that utilizes the direct method. When augmenting doubly robust methods, we call the resulting method Triply Robust. We prove upper bounds on the resulting bias and variance, as well as derive novel minimax bounds based on robust minimax analysis for covariate shift. Our robust regression method is compatible with deep learning, and is thus applicable to complex OPE settings that require powerful function approximators. Finally, we demonstrate superior empirical performance across the standard OPE benchmarks, especially in the case where the logging policy is unknown and must be estimated from data. </br></br>

<a href='http://arxiv.org/pdf/1911.08149.pdf'>1911.08149</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.7956баллов, №849</br>
<b>Differentiating Features for Scene Segmentation Based on Dedicated\n  Attention Mechanisms</b></br>
Authors: , Xiong, Zhiqiang, Wang, Zhicheng, Yu, Zhaohui, Gu, Xi</br>
  Semantic <font color="#be00be">segmentation</font> is a challenge in scene <font color="#be00be">parsing</font>. It requires both context information and rich spatial information. In this paper, we differentiate features for scene segmentation based on dedicated attention mechanisms (DF-DAM), and two attention modules are proposed to optimize the high-level and low-level features in the encoder, respectively. Specifically, we use the high-level and low-level features of ResNet as the source of context information and spatial information, respectively, and optimize them with attention fusion module and 2D position attention module, respectively. For attention fusion module, we adopt dual channel weight to selectively adjust the channel map for the highest two stage features of ResNet, and fuse them to get context information. For 2D position attention module, we use the context information obtained by attention fusion module to assist the selection of the lowest-stage features of ResNet as supplementary spatial information. Finally, the two sets of information obtained by the two modules are simply fused to obtain the prediction. We evaluate our approach on Cityscapes and PASCAL VOC 2012 datasets. In particular, there aren\'t complicated and redundant processing modules in our architecture, which greatly reduces the complexity, and we achieving 82.3% Mean IoU on PASCAL VOC 2012 test dataset without pre-training on MS-COCO dataset. </br></br>

<a href='http://arxiv.org/pdf/1911.07929.pdf'>1911.07929</a> &nbsp&nbsp (cs:CV, cs:ML, stat:ML) &nbsp&nbsp -0.7963баллов, №850</br>
<b>A Smartphone-Based Skin Disease Classification Using MobileNet CNN</b></br>
Authors: , Velasco, Jessica, Pascion, Cherry, Alberio, Jean Wilmar, Apuang, Jonathan, Cruz, John Stephen, Gomez, Mark Angelo, Molina, Benjamin Jr., Tuala, Lyndon, Thio-ac, August, Jorda, Romeo Jr.</br>
  The MobileNet model was used by applying transfer learning on the 7 skin <font color="blue">diseas</font>es to create a skin disease classification system on <font color="#be00be">Android</font> application. The proponents gathered a total of 3,406 images and it is considered as imbalanced dataset because of the unequal number of images on its classes. Using different sampling method and preprocessing of input data was explored to further improved the accuracy of the MobileNet. Using under-sampling method and the default preprocessing of input data achieved an 84.28% accuracy. While, using imbalanced dataset and default preprocessing of input data achieved a 93.6% accuracy. Then, researchers explored oversampling the dataset and the model attained a 91.8% accuracy. Lastly, by using oversampling technique and data augmentation on preprocessing the input data provide a 94.4% accuracy and this model was deployed on the developed Android application. </br></br>

<a href='http://arxiv.org/pdf/1910.13488.pdf'>1910.13488</a> &nbsp&nbsp (cs:CL, cs:SD) &nbsp&nbsp -0.8005баллов, №851</br>
<b>Does Speech enhancement of publicly available data help build robust\n  Speech Recognition Systems?</b></br>
Authors: , Ghai, Bhavya, Ramanan, Buvana, Mueller, Klaus</br>
  Automatic <font color="#be00be">speech recognition</font> (ASR) systems play a key role in many commercial products including voice assistants. Typically, they require large amounts of clean speech data for training which gives an undue advantage to large organizations which have tons of <font color="#be00be">private</font> data. In this paper, we have first curated a fairly big dataset using <font color="#00be00">publicly available</font> data sources. Thereafter, we tried to investigate if we can use publicly available noisy data to train robust ASR systems. We have used <font color="#be00be">speech enhancement</font> to clean the noisy data first and then used it together with its cleaned version to train ASR systems. We have found that using speech enhancement gives 9.5\\% better word error rate than training on just noisy data and 9\\% better than training on just clean data. It\'s performance is also comparable to the ideal case scenario when trained on noisy and its clean version. </br></br>

<a href='http://arxiv.org/pdf/1910.01735.pdf'>1910.01735</a> &nbsp&nbsp (cs:CV, cs:ML, stat:ML) &nbsp&nbsp -0.8067баллов, №852</br>
<b>GmCN: Graph Mask Convolutional Network</b></br>
Authors: , Jiang, Bo, Wang, Beibei, Tang, Jin, Luo, Bin</br>
  Graph Convolutional Networks (GCNs) have shown very powerful for graph data representation and learning tasks. Existing GCNs usually conduct feature aggregation on a fixed neighborhood graph in which each node computes its representation by aggregating the feature representations of all its neighbors which is biased by its own representation. However, this fixed aggregation strategy is not guaranteed to be optimal for GCN based graph learning and also can be affected by some graph structure noises, such as incorrect or undesired edge connections. To address these issues, we propose a novel Graph mask Convolutional Network (GmCN) in which nodes can adaptively select the optimal neighbors in their feature aggregation to better serve GCN learning. GmCN can be <font color="blue">theor</font>etically <font color="#be00be">interpret</font>ed by a regularization framework, based on which we derive a simple update algorithm to determine the optimal mask adaptively in GmCN training process. Experiments on several datasets validate the effectiveness of GmCN. </br></br>

<a href='http://arxiv.org/pdf/1911.07335.pdf'>1911.07335</a> &nbsp&nbsp (cs:CL, cs:ML, stat:ML) &nbsp&nbsp -0.8069баллов, №853</br>
<b>Overcoming Practical Issues of Deep Active Learning and its Applications\n  on Named Entity Recognition</b></br>
Authors: , Chang, Haw-Shiuan, Vembu, Shankar, Mohan, Sunil, Uppaal, Rheeya, McCallum, Andrew</br>
  Existing deep active learning algorithms achieve impressive sampling efficiency on natural language processing tasks. However, they exhibit several weaknesses in practice, including (a) inability to use uncertainty sampling with black-box models, (b) lack of robustness to noise in labeling, (c) lack of transparency. In response, we propose a transparent batch active sampling framework by estimating the error decay curves of multiple feature-defined subsets of the data. Experiments on four <font color="blue">named entity</font> recognition (NER) tasks demonstrate that the proposed methods significantly <font color="#00be00">outperform</font> diversification-based methods for black-box<font color="#be00be"> NER </font>taggers and can make the sampling process more robust to labeling noise when combined with uncertainty-based methods. Furthermore, the analysis of experimental results sheds light on the weaknesses of different active sampling strategies, and when traditional uncertainty-based or diversification-based methods can be expected to work well. </br></br>

<a href='http://arxiv.org/pdf/1911.08820.pdf'>1911.08820</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.8100баллов, №854</br>
<b>A Fast Sampling Gradient Tree Boosting Framework</b></br>
Authors: , Zhou, Daniel Chao, Jin, Zhongming, Zhang, Tong</br>
  As an adaptive, <font color="#be00be">interpret</font>able, robust, and accurate meta-algorithm for arbitrary differentiable loss functions, gradient tree boosting is one of the most popular machine learning techniques, though the computational expensiveness severely limits its usage. Stochastic gradient boosting could be adopted to accelerates gradient boosting by uniformly sampling training instances, but its estimator could introduce a high variance. This situation arises motivation for us to optimize gradient tree boosting. We combine gradient tree boosting with importance sampling, which achieves better performance by reducing the stochastic variance. Furthermore, we use a regularizer to improve the diagonal approximation in the Newton step of gradient boosting. The <font color="blue">theor</font>etical analysis supports that our strategies achieve a linear convergence rate on logistic loss. Empirical results show that our algorithm achieves a 2.5x--18x acceleration on two different gradient boosting algorithms (LogitBoost and LambdaMART) without appreciable performance loss. </br></br>

<a href='http://arxiv.org/pdf/1910.05395.pdf'>1910.05395</a> &nbsp&nbsp (cs:CV, cs:RO) &nbsp&nbsp -0.8223баллов, №855</br>
<b>FuseMODNet: Real-Time Camera and LiDAR based Moving Object Detection for\n  robust low-light Autonomous Driving</b></br>
Authors: , Rashed, Hazem, Ramzy, Mohamed, Vaquero, Victor, Sallab, Ahmad El, Sistu, Ganesh, Yogamani, Senthil</br>
  Moving <font color="#be00be">object detection</font> is a critical task for autonomous vehicles. As dynamic objects represent higher collision risk than static ones, our own ego-trajectories have to be planned attending to the future states of the moving elements of the scene. Motion can be perceived using temporal information such as optical flow. Conventional optical flow computation is based on camera sensors only, which makes it prone to failure in conditions with low illumination. On the other hand, <font color="blue">LiDAR</font> sensors are independent of illumination, as they measure the time-of-flight of their own emitted lasers. In this work, we propose a robust and real-time CNN architecture for Moving Object Detection (MOD) under low-light conditions by capturing motion information from both camera and LiDAR sensors. We demonstrate the impact of our algorithm on KITTI dataset where we simulate a low-light environment creating a novel dataset &quot;Dark KITTI&quot;. We obtain a 10.1% relative improvement on Dark-KITTI, and a 4.25% improvement on standard KITTI relative to our baselines. The proposed algorithm runs at 18 fps on a standard desktop GPU using $256\\times1224$ resolution images. </br></br>

<a href='http://arxiv.org/pdf/1911.09476.pdf'>1911.09476</a> &nbsp&nbsp (cs:RO, cs:AI, cs:ML) &nbsp&nbsp -0.8310баллов, №856</br>
<b>Incremental Learning of Motion Primitives for Pedestrian Trajectory\n  Prediction at Intersections</b></br>
Authors: , Habibi, Golnaz, Japuria, Nikita, How, Jonathan P.</br>
  This paper presents a novel incremental learning algorithm for <font color="#be00be">pedestrian</font> motion prediction, with the ability to improve the learned model over time when data is incrementally available. In this setup, trajectories are modeled as simple segments called motion primitives. Transitions between motion primitives are modeled as <font color="blue">Gaussi</font>an Processes. When new data is available, the motion primitives learned from the new data are compared with the previous ones by measuring the inner product of the motion primitive vectors. Similar motion primitives and transitions are fused and novel motion primitives are added to capture newly observed behaviors. The proposed approach is tested and compared with other baselines in intersection scenarios where the data is incrementally available either from a single intersection or from multiple intersections with different geometries. In both cases, our method incrementally learns motion patterns and <font color="#00be00">outperform</font>s the offline learning approach in terms of prediction errors. The results also show that the model size in our algorithm grows at a much lower rate than standard incremental learning, where newly learned motion primitives and transitions are simply accumulated over time. </br></br>

<a href='http://arxiv.org/pdf/1911.08715.pdf'>1911.08715</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.8334баллов, №857</br>
<b>An Inception Inspired Deep Network to Analyse Fundus Images</b></br>
Authors: , Uslu, Fatmatulzehra</br>
  A fundus image usually contains the optic disc, <font color="blue">patholog</font>ies and other structures in addition to vessels to be segmented. This study proposes a deep network for vessel <font color="#be00be">segmentation</font>, whose architecture is inspired by inception modules. The network contains three sub-networks, each with a different filter size, which are connected in the last layer of the proposed network. According to experiments conducted in the DRIVE and IOSTAR, the performance of our network is found to be better than or comparable to that of the previous methods. We also observe that the sub-networks pay attention to different parts of an input image when producing an output map in the last layer of the proposed network; though, training of the proposed network is not constrained for this purpose. </br></br>

<a href='http://arxiv.org/pdf/1910.14315.pdf'>1910.14315</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.8447баллов, №858</br>
<b>BottleNet++: An End-to-End Approach for Feature Compression in\n  Device-Edge Co-Inference Systems</b></br>
Authors: , Shao, Jiawei, Zhang, Jun</br>
  The emergence of various intelligent<font color="#960096"> mobile </font>applications demands the deployment of powerful deep learning models at resource-constrained mobile devices. The device-edge co-inference framework provides a promising solution by splitting a neural network at a mobile device and an edge computing server. In order to balance the on-device computation and the communication overhead, the splitting point needs to be carefully picked, while the intermediate feature needs to be compressed before transmission. Existing studies decoupled the design of model splitting, feature compression, and communication, which may lead to excessive resource consumption of the mobile device. In this paper, we introduce an end-to-end architecture, named BottleNet++, that consists of an encoder, a non-trainable channel layer, and a decoder for more efficient feature compression and transmission. The encoder and decoder essentially implement joint source-channel coding via convolutional neural networks (CNNs), while explicitly considering the effect of channel noise. By exploiting the strong sparsity and the fault-tolerant property of the intermediate feature in a deep neural network (DNN), BottleNet++ achieves a much higher compression ratio than existing methods. Furthermore, by providing the channel condition to the encoder as an input, our method enjoys a strong generalization ability in different channel conditions. Compared with merely transmitting intermediate data without feature compression, BottleNet++ achieves up to 64x bandwidth reduction over the additive white <font color="blue">Gaussi</font>an noise channel and up to 256x bit compression ratio in the binary erasure channel, with less than 2% reduction in accuracy. With a higher compression ratio, BottleNet++ enables splitting a DNN at earlier layers, which leads to up to 3x reduction in on-device computation compared with other compression methods. </br></br>

<a href='http://arxiv.org/pdf/1911.08744.pdf'>1911.08744</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.8457баллов, №859</br>
<b>Log Message Anomaly Detection and Classification Using Auto-B/LSTM and\n  Auto-GRU</b></br>
Authors: , Farzad, Amir, Gulliver, T. Aaron</br>
  Log messages are now widely used in software systems. They are important for classification as millions of logs are generated each day. Most logs are unstructured which makes classification a challenge. In this paper, Deep Learning (DL) methods called Auto-LSTM, Auto-BLSTM and Auto-GRU are developed for <font color="#be00be">anomal</font>y detection and log classification. These models are used to convert unstructured log data to trained features which is suitable for classification algorithms. They are evaluated using four data sets, namely BGL, Openstack, Thunderbird and IMDB. The first three are popular log data sets while the fourth is a movie review data set which is used for <font color="#be00be">sentiment</font> classification and is used here to show that the models can be generalized to other text classification tasks. The results obtained show that Auto-LSTM, Auto-BLSTM and Auto-GRU perform better than other well-known algorithms. </br></br>

<a href='http://arxiv.org/pdf/1911.09086.pdf'>1911.09086</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.8477баллов, №860</br>
<b>Shapelets for earthquake detection</b></br>
Authors: , Arul, Monica, Kareem, Ahsan</br>
  This paper introduces EQShapelets (EarthQuake Shapelets) a time-series shape-based approach embedded in machine learning to autonomously detect earthquakes. It promises to overcome the challenges in the field of seismology related to automated detection and cataloging of earthquakes. EQShapelets are amplitude and phase-independent, i.e., their detection sensitivity is irrespective of the magnitude of the earthquake and the time of occurrence. They are also robust to noise and other spurious signals. The detection capability of EQShapelets is tested on one week of continuous seismic data provided by the Northern California Seismic Network (NCSN) obtained from a station in central California near the Calaveras Fault. EQShapelets combined with a <font color="blue">Random Forest</font> classifier, detected all of the cataloged earthquakes and 281 uncataloged events with lower false detection rate thus offering a better performance than autocorrelation and FAST algorithms. The primary advantage of EQShapelets over competing methods is the <font color="#be00be">interpret</font>ability and insight it offers. Shape-based approaches are intuitive, visually meaningful and offers immediate insight into the problem domain that goes beyond their use in accurate detection. EQShapelets, if implemented at a large scale, can significantly reduce catalog completeness magnitudes and can serve as an effective tool for near real-time earthquake monitoring and cataloging. </br></br>

<a href='http://arxiv.org/pdf/1911.07084.pdf'>1911.07084</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp -0.8517баллов, №861</br>
<b>Missingness as Stability: Understanding the Structure of Missingness in\n  Longitudinal EHR data and its Impact on Reinforcement Learning in Healthcare</b></br>
Authors: , Fleming, Scott L., Jeyapragasan, Kuhan, Duan, Tony, Ding, Daisy, Gombar, Saurabh, Shah, Nigam, Brunskill, Emma</br>
  There is an emerging trend in the <font color="#00be00">reinforcement learning</font> for healthcare literature. In order to prepare longitudinal, irregularly sampled, <font color="blue">clinic</font>al datasets for reinforcement learning algorithms, many researchers will resample the time series data to short, regular intervals and use last-observation-carried-forward (LOCF) imputation to fill in these gaps. Typically, they will not maintain any explicit information about which values were imputed. In this work, we (1) call attention to this practice and discuss its potential implications; (2) propose an alternative representation of the <font color="blue">patient</font> state that addresses some of these issues; and (3) demonstrate in a novel but representative clinical dataset that our alternative representation yields consistently better results for achieving optimal control, as measured by off-policy policy evaluation, compared to representations that do not incorporate missingness information. </br></br>

<a href='http://arxiv.org/pdf/1911.07115.pdf'>1911.07115</a> &nbsp&nbsp (cs:ML, cs:NE) &nbsp&nbsp -0.8517баллов, №862</br>
<b>General Regression Neural Networks, Radial Basis Function Neural\n  Networks, Support Vector Machines, and Feedforward Neural Networks</b></br>
Authors: , Jenkins, Alison, Gupta, Vinika, Lenoir, Mary</br>
  The aim of this project is to develop a code to discover the optimal sigma value that maximum the F1 score and the optimal sigma value that maximizes the accuracy and to find out if they are the same. Four algorithms which can be used to solve this problem are: <font color="#be00be">Genetic</font> <font color="#be00be">Regression</font> Neural Networks (GRNNs), Radial Based Function (RBF) Neural Networks (RBFNNs), Support Vector Machines (<font color="blue">SVM</font>s) and Feedforward Neural Network (FFNNs). </br></br>

<a href='http://arxiv.org/pdf/1911.07652.pdf'>1911.07652</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.8519баллов, №863</br>
<b>Information-Theoretic Perspective of Federated Learning</b></br>
Authors: , Adilova, Linara, Rosenzweig, Julia, Kamp, Michael</br>
  An approach to distributed machine learning is to train models on local datasets and aggregate these models into a single, stronger model. A popular instance of this form of parallelization is <font color="#be00be">federated</font> learning, where the nodes periodically send their local models to a coordinator that aggregates them and redistributes the aggregation back to continue training with it. The most frequently used form of aggregation is averaging the model parameters, e.g., the weights of a neural network. However, due to the non-convexity of the loss surface of neural networks, averaging can lead to detrimental effects and it remains an open question under which conditions averaging is beneficial. In this paper, we study this problem from the perspective of information <font color="blue">theor</font>y: We measure the mutual information between representation and inputs as well as representation and labels in local models and compare it to the respective information contained in the representation of the averaged model. Our empirical results confirm previous observations about the practical usefulness of averaging for neural networks, even if local dataset distributions vary strongly. Furthermore, we obtain more insights about the impact of the aggregation frequency on the information flow and thus on the success of distributed learning. These insights will be helpful both in improving the current synchronization process and in further understanding the effects of model aggregation. </br></br>

<a href='http://arxiv.org/pdf/1911.08854.pdf'>1911.08854</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.8521баллов, №864</br>
<b>The dynamics of the stomatognathic system from 4D multimodal data</b></br>
Authors: , Tomaka, Agnieszka A., Luchowski, Leszek, Pojda, Dariusz, Tarnawski, Micha&#x142;, Domino, Krzysztof</br>
  The purpose of this chapter is to discuss methods of acquisition, visualization and analysis of the dynamics of a complex bio<font color="blue">medic</font>al system, illustrated by the human stomatognathic system. The stomatognathic system consists of the teeth and the skull bones with the maxilla and the mandible. Its dynamics can be described by the change of mutual position of the lower/mandibular part versus the upper/maxillary one due to the physiological motion of opening, chewing and swallowing. In order to analyse the dynamics of the stomatognathic system its morphology and motion has to be digitized, which is done using static and dynamic multimodal imagery like CBCT and 3D scans data and temporal measurements of motion. The integration of multimodal data incorporates different direct and indirect methods of registration - aligning of all the data in the same coordinate system. The integrated sets of data form 4D multimodal data which can be further visualized, modeled, and subjected to multivariate time series analysis. Example results are shown. Although there is no direct method of imaging the TMJ motion, the integration of multimodal data forms an adequate tool. As medical imaging becomes ever more diverse and ever more accessible, organizing the imagery and measurements into unified, comprehensive records can deliver to the doctor the most information in the most accessible form, creating a new quality in data simulation, analysis and <font color="#be00be">interpret</font>ation. </br></br>

<a href='http://arxiv.org/pdf/1911.07069.pdf'>1911.07069</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.8561баллов, №865</br>
<b>Kvasir-SEG: A Segmented Polyp Dataset</b></br>
Authors: , Jha, Debesh, Smedsrud, Pia H., Riegler, Michael A., Halvorsen, P&#xe5;l, de Lange, Thomas, Johansen, Dag, Johansen, H&#xe5;vard D.</br>
  Pixel-wise image <font color="#be00be">segmentation</font> is a highly demanding task in <font color="blue">medic</font>al-image analysis. In practice, it is difficult to find annotated medical images with corresponding segmentation masks. In this paper, we present Kvasir-SEG: an open-access dataset of gastrointestinal polyp images and corresponding segmentation masks, manually annotated by a medical doctor and then verified by an experienced gastroenterologist. Moreover, we also generated the bounding boxes of the polyp regions with the help of segmentation masks. We demonstrate the use of our dataset with a traditional segmentation approach and a modern deep-learning based Convolutional Neural Network (CNN) approach. The dataset will be of value for researchers to reproduce results and compare methods. By adding segmentation masks to the Kvasir dataset, which only provide frame-wise annotations, we enable multimedia and computer vision researchers to contribute in the field of polyp segmentation and automatic analysis of colonoscopy images. </br></br>

<a href='http://arxiv.org/pdf/1911.07381.pdf'>1911.07381</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.8599баллов, №866</br>
<b>Learning Similarity Attention</b></br>
Authors: , Zheng, Meng, Karanam, Srikrishna, Chen, Terrence, Radke, Richard J., Wu, Ziyan</br>
  We consider the problem of learning similarity functions. While there has been substantial progress in learning suitable distance metrics, these techniques in general lack decision reasoning, i.e., explaining why the input set of images is similar or dissimilar. In this work, we solve this key problem by proposing the first method to generate generic visual similarity explanations with gradient-based attention. We demonstrate that our technique is agnostic to the specific similarity model type, e.g., we show applicability to Siamese, triplet, and quadruplet models. Furthermore, we make our proposed similarity attention a principled part of the learning process, resulting in a new paradigm for learning similarity functions. We demonstrate that our learning mechanism results in more generalizable, as well as explainable, similarity models. Finally, we demonstrate the generality of our framework by means of experiments on a variety of tasks, including image retrieval, person <font color="blue">re-identification</font>, and low-shot semantic <font color="#be00be">segmentation</font>. </br></br>

<a href='http://arxiv.org/pdf/1911.06475.pdf'>1911.06475</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.8611баллов, №867</br>
<b>Interpreting chest X-rays via CNNs that exploit disease dependencies and\n  uncertainty labels</b></br>
Authors: , Pham, Hieu H., Le, Tung T., Tran, Dat Q., Ngo, Dat T., Nguyen, Ha Q.</br>
  Chest radiography is one of the most common types of <font color="blue">diagnos</font>tic radiology exams, which is critical for screening and diagnosis of many different thoracic <font color="blue">diseas</font>es. Specialized algorithms have been developed to detect several specific <font color="blue">patholog</font>ies such as lung nodule or lung <font color="#be00be">cancer</font>. However, accurately detecting the presence of multiple diseases from chest X-rays (CXRs) is still a challenging task. This paper presents a supervised multi-label classification framework based on deep convolutional neural networks (CNNs) for predicting the risk of 14 common thoracic diseases. We tackle this problem by training <font color="red">state-of-the-art</font> CNNs that exploit dependencies among abnormality labels. We also propose to use the label smoothing technique for a better handling of uncertain samples, which occupy a significant portion of almost every CXR dataset. Our model is trained on over 200,000 CXRs of the recently released CheXpert dataset and achieves a mean area under the curve (AUC) of 0.940 in predicting 5 selected pathologies from the validation set. This is the highest AUC score yet reported to date. The proposed method is also evaluated on the independent test set of the CheXpert competition, which is composed of 500 CXR studies annotated by a panel of 5 experienced radiologists. The performance is on average better than 2.6 out of 3 other individual radiologists with a mean AUC of 0.930, which ranks first on the CheXpert leaderboard at the time of writing this paper. </br></br>

<a href='http://arxiv.org/pdf/1911.09564.pdf'>1911.09564</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.8812баллов, №868</br>
<b>Parameter-Free Locally Differentially Private Stochastic Subgradient\n  Descent</b></br>
Authors: , Jun, Kwang-Sung, Orabona, Francesco</br>
  We consider the problem of minimizing a convex risk with stochastic subgradients guaranteeing $\\epsilon$-locally differentially <font color="#be00be">private</font> ($\\epsilon$-LDP). While it has been shown that stochastic optimization is possible with $\\epsilon$-LDP via the standard SGD (Song et al., 2013), its convergence rate largely depends on the learning rate, which must be tuned via repeated runs. Further, tuning is detrimental to <font color="#be00be">privacy</font> loss since it significantly increases the number of gradient requests. In this work, we propose BANCO (Betting Algorithm for Noisy COins), the first $\\epsilon$-LDP SGD algorithm that essentially matches the convergence rate of the tuned SGD without any learning rate parameter, reducing privacy loss and saving privacy budget. </br></br>

<a href='http://arxiv.org/pdf/1911.09319.pdf'>1911.09319</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -0.8829баллов, №869</br>
<b>An Empirical Study of Sections in Classifying Disease Outbreak Reports</b></br>
Authors: , Doan, Son, Conway, Mike, Collier, Nigel</br>
  Identifying articles that relate to infectious <font color="blue">diseas</font>es is a necessary step for any automatic bio-<font color="#be00be">surveillance</font> system that monitors news articles from the Internet. Unlike scientific articles which are available in a strongly structured form, news articles are usually loosely structured. In this chapter, we investigate the importance of each section and the effect of section weighting on performance of text classification. The experimental results show that (1) classification models using the headline and leading sentence achieve a high performance in terms of F-score compared to other parts of the article; (2) all section with bag-of-word representation (full text) achieves the highest recall; and (3) section weighting information can help to improve accuracy. </br></br>

<a href='http://arxiv.org/pdf/1911.07453.pdf'>1911.07453</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.8835баллов, №870</br>
<b>Vulnerability Analysis for Data Driven Pricing Schemes</b></br>
Authors: , Cui, Jingshi, Wang, Haoxiang, Wu, Chenye, Yu, Yang</br>
  Data analytics and machine learning techniques are being rapidly adopted into the power system, including power system control as well as electricity <font color="#be00be">market</font> design. In this paper, from an adversarial machine learning point of view, we examine the vulnerability of data-driven electricity market design. More precisely, we follow the idea that consumer\'s load profile should uniquely determine its electricity rate, which yields a <font color="#be00be">clustering</font> oriented pricing scheme. We first identify the strategic behaviors of malicious users by defining a notion of disguising. Based on this notion, we characterize the sensitivity zones to evaluate the percentage of malicious users in each cluster. Based on a thorough cost benefit analysis, we conclude with the vulnerability analysis. </br></br>

<a href='http://arxiv.org/pdf/1911.07676.pdf'>1911.07676</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -0.8920баллов, №871</br>
<b>Learning with Good Feature Representations in Bandits and in RL with a\n  Generative Model</b></br>
Authors: , Lattimore, Tor, Szepesvari, Csaba</br>
  The construction in the recent paper by Du et al. [2019] implies that searching for a near-optimal action in a <font color="blue">bandit</font> sometimes requires examining essentially all the actions, even if the learner is given linear features in $\\mathbb R^d$ that approximate the rewards with a small uniform error. In this note we use the Kiefer-Wolfowitz <font color="blue">theor</font>em to show that by checking only a few actions, a learner can always find an action which is suboptimal with an error of at most $O(\\varepsilon \\sqrt{d})$ where $\\varepsilon$ is the approximation error of the features. Thus, features are useful when the approximation error is small relative to the dimensionality of the features. The idea is applied to stochastic bandits and <font color="#00be00">reinforcement learning</font> with a generative model where the learner has access to $d$-dimensional linear features that approximate the action-value functions for all policies to an accuracy of $\\varepsilon$. For bandits we prove a bound on the regret of order $\\sqrt{dn \\log(k)} + \\varepsilon n \\sqrt{d} \\log(n)$ with $k$ the number of actions and $n$ the horizon. For RL we show that approximate policy iteration can learn a policy that is optimal up to an additive error of order $\\varepsilon \\sqrt{d} / (1 - \\gamma)^2$ and using about $d / (\\varepsilon^2(1-\\gamma)^4)$ samples from the generative model. </br></br>

<a href='http://arxiv.org/pdf/1911.07934.pdf'>1911.07934</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.8930баллов, №872</br>
<b>Training Set Affect on Super Resolution for Automated Target Recognition</b></br>
Authors: , Ciolino, Matthew, Noever, David, Kalin, Josh</br>
  Do we need to have more expensive remote sensing satellites when we could use single image <font color="#be00be">super-resolution</font> (SISR) to get the spatial resolution that we want? By using a Super Resolution Generative Adversarial Network, (SRGAN) we can get higher resolution images. Previous work by Shermeyer et al. [1] have used SISR as a preprocessing step describe an increase in mAP of 10-36 % in <font color="#be00be">object detection</font> for native 30cm to 15cm satellite imagery. This suggests a possible improvement to automated target recognition in image classification and object detection. The SRGAN takes a low-resolution image and maps it to a high-resolution image creating the super resolution product. We train 5 SRGANs on different land-use classes (e.g. agriculture, cities) and find the qualitative and quantitative differences in SISR, binary classification, and object detection performance. </br></br>

<a href='http://arxiv.org/pdf/1911.09586.pdf'>1911.09586</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.8976баллов, №873</br>
<b>The Performance of Machine and Deep Learning Classifiers in Detecting\n  Zero-Day Vulnerabilities</b></br>
Authors: , Abri, Faranak, Siami-Namini, Sima, Khanghah, Mahdi Adl, Soltani, Fahimeh Mirza, Namin, Akbar Siami</br>
  The detection of zero-day attacks and vulnerabilities is a challenging problem. It is of utmost importance for network administrators to identify them with high accuracy. The higher the accuracy is, the more robust the defense mechanism will be. In an ideal scenario (i.e., 100% accuracy) the system can detect zero-day <font color="#be00be">malware</font> without being concerned about mistakenly tagging benign files as malware or enabling disruptive malicious code running as none-malicious ones. This paper investigates different machine learning algorithms to find out how well they can detect zero-day malware. Through the examination of 34 machine/deep learning classifiers, we found that the <font color="blue">random forest</font> classifier offered the best accuracy. The paper poses several research questions regarding the performance of machine and deep learning algorithms when detecting zero-day malware with zero rates for false positive and false negative. </br></br>

<a href='http://arxiv.org/pdf/1911.09052.pdf'>1911.09052</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.8986баллов, №874</br>
<b>Collaborative Machine Learning Markets with Data-Replication-Robust\n  Payments</b></br>
Authors: , Ohrimenko, Olga, Tople, Shruti, Tschiatschek, Sebastian</br>
  We study the problem of collaborative machine learning <font color="#be00be">market</font>s where multiple parties can achieve improved performance on their machine learning tasks by combining their training data. We discuss desired properties for these machine learning markets in terms of fair revenue distribution and potential threats, including data replication. We then instantiate a collaborative market for cases where parties share a common machine learning task and where parties\' tasks are different. Our marketplace incentivizes parties to submit high quality training and true validation data. To this end, we introduce a novel payment division function that is robust-to-replication and customized output models that perform well only on requested machine learning tasks. In experiments, we validate the assumptions underlying our <font color="blue">theor</font>etical analysis and show that these are approximately satisfied for commonly used machine learning models. </br></br>

<a href='http://arxiv.org/pdf/1911.07940.pdf'>1911.07940</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.9004баллов, №875</br>
<b>Neighborhood Watch: Representation Learning with Local-Margin Triplet\n  Loss and Sampling Strategy for K-Nearest-Neighbor Image Classification</b></br>
Authors: , Thammasorn, Phawis, Hippe, Daniel, Chaovalitwongse, Wanpracha, Spraker, Matthew, Wootton, Landon, Nyflot, Matthew, Combs, Stephanie, Peeken, Jan, Ford, Eric</br>
  Deep representation learning using triplet network for classification suffers from a lack of <font color="blue">theor</font>etical foundation and difficulty in tuning both the network and classifiers for performance. To address the problem, local-margin triplet loss along with local positive and negative mining strategy is proposed with theory on how the strategy integrate nearest-neighbor hyper-parameter with triplet learning to increase subsequent classification performance. Results in experiments with 2 public datasets, MNIST and Cifar-10, and 2 small <font color="blue">medic</font>al image datasets demonstrate that proposed strategy <font color="#00be00">outperform</font>s end-to-end softmax and typical triplet loss in settings without data augmentation while maintaining utility of transferable feature for related tasks. The method serves as a good performance baseline where end-to-end methods encounter difficulties such as small sample data with limited allowable data augmentation. </br></br>

<a href='http://arxiv.org/pdf/1911.08483.pdf'>1911.08483</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.9116баллов, №876</br>
<b>Automatic Brain Tumour Segmentation and Biophysics-Guided Survival\n  Prediction</b></br>
Authors: , Wang, Shuo, Dai, Chengliang, Mo, Yuanhan, Angelini, Elsa, Guo, Yike, Bai, Wenjia</br>
  Gliomas are the most common malignant <font color="#00be00">brain</font> tumourswith intrinsic heterogeneity. Accurate <font color="#be00be">segmentation</font> of gliomas and theirsub-regions on multi-parametric <font color="blue">magnetic resonance</font> images (mpMRI)is of great <font color="blue">clinic</font>al importance, which defines tumour size, shape andappearance and provides abundant information for preoperative diag-nosis, treatment planning and survival prediction. Recent developmentson deep learning have significantly improved the performance of auto-mated <font color="blue">medic</font>al image segmentation. In this paper, we compare several<font color="red">state-of-the-art</font> convolutional neural network models for brain tumourimage segmentation. Based on the ensembled segmentation, we presenta biophysics-guided prognostic model for <font color="blue">patient</font> overall survival predic-tion which <font color="#00be00">outperform</font>s a data-driven radiomics approach. Our methodwon the second place of the MICCAI 2019 BraTS Challenge for theoverall survival prediction. </br></br>

<a href='http://arxiv.org/pdf/1911.07679.pdf'>1911.07679</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.9153баллов, №877</br>
<b>Towards Quantification of Bias in Machine Learning for Healthcare: A\n  Case Study of Renal Failure Prediction</b></br>
Authors: , Williams, Josie, Razavian, Narges</br>
  As machine learning (ML) models, trained on <font color="#009600">real-world</font> datasets, become common practice, it is critical to measure and quantify their potential biases. In this paper, we focus on renal failure and compare a commonly used traditional risk score, Tangri, with a more powerful machine learning model, which has access to a larger variable set and trained on 1.6 million <font color="blue">patient</font>s\' EHR data. We will compare and discuss the generalization and applicability of these two models, in an attempt to quantify biases of status quo <font color="blue">clinic</font>al practice, compared to ML-driven models. </br></br>

<a href='http://arxiv.org/pdf/1911.09249.pdf'>1911.09249</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.9162баллов, №878</br>
<b>Semantic Segmentation of Thigh Muscle using 2.5D Deep Learning Network\n  Trained with Limited Datasets</b></br>
Authors: , Haque, Hasnine, Hashimoto, Masahiro, Uetake, Nozomu, Jinzaki, Masahiro</br>
  Purpose: We propose a 2.5D deep learning neural network (DLNN) to automatically classify thigh muscle into 11 classes and evaluate its classification accuracy over 2D and 3D DLNN when trained with limited datasets. Enables operator invariant quantitative assessment of the thigh muscle volume change with respect to the <font color="blue">diseas</font>e progression. Materials and methods: Retrospective datasets consist of 48 thigh volume (TV) cropped from CT DICOM images. Cropped volumes were aligned with femur axis and resample in 2 mm voxel-spacing. Proposed 2.5D DLNN consists of three 2D U-Net trained with axial, coronal and sagittal muscle slices respectively. A voting algorithm was used to combine the output of U-Nets to create final <font color="#be00be">segmentation</font>. 2.5D U-Net was trained on PC with 38 TV and the remaining 10 TV were used to evaluate segmentation accuracy of 10 classes within Thigh. The result segmentation of both left and right thigh were de-cropped to original CT volume space. Finally, segmentation accuracies were compared between proposed DLNN and 2D/3D U-Net. Results: Average segmentation DSC score accuracy of all classes with 2.5D U-Net as 91.18% and Average Surface distance (ASD) accuracy as 0.84 mm. We found, mean DSC score for 2D U-Net was 3.3% lower than the that of 2.5D U-Net and mean DSC score of 3D U-Net was 5.7% lower than that of 2.5D U-Net when trained with same datasets. Conclusion: We achieved a faster computationally efficient and automatic segmentation of thigh muscle into 11 classes with reasonable accuracy. Enables quantitative evaluation of muscle atrophy with disease progression. </br></br>

<a href='http://arxiv.org/pdf/1911.07608.pdf'>1911.07608</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.9386баллов, №879</br>
<b>Application of Reinforcement Learning for 5G Scheduling Parameter\n  Optimization</b></br>
Authors: , Habiby, Ali Asgher Mansoor, Thoppu, Ahamed</br>
  RF Network parametric optimization requires a wealth of experience and knowledge to achieve the optimal balance between coverage, capacity, system efficiency and <font color="#be00be">customer</font> experience from the telecom sites serving the users. With 5G, the complications of Air interface scheduling have increased due to the usage of massive MIMO, beamforming and introduction of higher modulation schemes with varying numerologies. In this work, we tune a machine learning model to &quot;learn&quot; the best combination of parameters for a given traffic profile using Cross Entropy Method <font color="#00be00">Reinforcement Learning</font> and compare these with RF Subject Matter Expert &quot;SME&quot; <font color="blue">recommendat</font>ions. This work is aimed towards automatic parameter tuning and feature optimization by acting as a Self Organizing Network module </br></br>

<a href='http://arxiv.org/pdf/1911.08588.pdf'>1911.08588</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -0.9482баллов, №880</br>
<b>Mini Lesions Detection on Diabetic Retinopathy Images via Large Scale\n  CNN Features</b></br>
Authors: , Chen, Qilei, Sun, Xinzi, Zhang, Ning, Cao, Yu, Liu, Benyuan</br>
  Diabetic retinopathy (DR) is a diabetes complication that affects eyes. DR is a primary cause of blindness in working-age people and it is estimated that 3 to 4 million people with diabetes are blinded by DR every year worldwide. Early <font color="blue">diagnos</font>is have been considered an effective way to mitigate such problem. The ultimate goal of our research is to develop novel machine learning techniques to analyze the DR images generated by the fundus camera for automatically DR diagnosis. In this paper, we focus on identifying small lesions on DR fundus images. The results from our analysis, which include the lesion category and their exact locations in the image, can be used to facilitate the determination of DR severity (indicated by DR stages). Different from traditional <font color="#be00be">object detection</font> for natural images, lesion detection for fundus images have unique challenges. Specifically, the size of a lesion instance is usually very small, compared with the original resolution of the fundus images, making them diffcult to be detected. We analyze the lesion-vs-image scale carefully and propose a large-size feature pyramid network (LFPN) to preserve more image details for mini lesion instance detection. Our method includes an effective region proposal strategy to increase the sensitivity. The experimental results show that our proposed method is superior to the original feature pyramid network (FPN) method and Faster RCNN. </br></br>

<a href='http://arxiv.org/pdf/1911.07994.pdf'>1911.07994</a> &nbsp&nbsp (cs:SD) &nbsp&nbsp -0.9586баллов, №881</br>
<b>Language Aided Speaker Diarization Using Speaker Role Information</b></br>
Authors: , Flemotomos, Nikolaos, Georgiou, Panayiotis, Narayanan, Shrikanth</br>
  Speaker diarization relies on the assumption that acoustic embeddings from speech segments corresponding to a particular speaker share common characteristics. Thus, they are concentrated in a specific region of the speaker space; a region which represents that speaker\'s identity. Those identities however are not known a priori, so a <font color="#be00be">clustering</font> algorithm is employed, which is typically based solely on audio. In this work we explore conversational scenarios where the speakers play distinct roles and are expected to follow different linguistic patterns. We aim to exploit this distinct linguistic variability and build a language-based segmenter and a role recognizer which can be used to construct the speaker identities. That way, we are able to boost the diarization performance by converting the clustering task to a classification one. The proposed method is applied in <font color="#009600">real-world</font> dyadic psychotherapy interactions between a provider and a <font color="blue">patient</font> and demonstrated to show improved results. </br></br>

<a href='http://arxiv.org/pdf/1911.09045.pdf'>1911.09045</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.9677баллов, №882</br>
<b>A CNN-RNN Framework for Crop Yield Prediction</b></br>
Authors: , Khaki, Saeed, Wang, Lizhi, Archontoulis, Sotirios V.</br>
  Crop yield prediction is extremely challenging due to its dependence on multiple factors such as crop genotype, environmental factors, management practices, and their interactions. This paper presents a deep learning framework using convolutional neural networks (CNN) and recurrent neural networks (RNN) for crop yield prediction based on environmental data and management practices. The proposed CNN-RNN model, along with other popular methods such as <font color="blue">random forest</font> (RF), deep fully-connected neural networks (DFNN), and LASSO, was used to forecast corn and soybean yield across the entire Corn Belt (including 13 states) in the United States for years 2016, 2017, and 2018 using historical data. The new model achieved a root-mean-square-error (RMSE) 9% and 8% of their respective average yields, substantially <font color="#00be00">outperform</font>ing all other methods that were tested. The CNN-RNN have three salient features that make it a potentially useful method for other crop yield prediction studies. (1) The CNN-RNN model was designed to capture the time dependencies of environmental factors and the <font color="#be00be">genetic</font> improvement of seeds over time without having their genotype information. (2) The model demonstrated the capability to generalize the yield prediction to untested environments without significant drop in the prediction accuracy. (3) Coupled with the backpropagation method, the model could reveal the extent to which <font color="#be00be">weather</font> conditions, accuracy of weather predictions, soil conditions, and management practices were able to explain the variation in the crop yields. </br></br>

<a href='http://arxiv.org/pdf/1911.08508.pdf'>1911.08508</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -0.9700баллов, №883</br>
<b>Parameters Estimation for the Cosmic Microwave Background with Bayesian\n  Neural Networks</b></br>
Authors: , Hortua, Hector J., Volpi, Riccardo, Marinelli, Dimitri, Malag&#xf2;, Luigi</br>
  In this paper, we present the first study that compares different models of <font color="blue">Bayes</font>ian Neural Networks (BNNs) to predict the posterior distribution of the cosmological parameters directly from the Cosmic Microwave Background (CMB) map. We focus our analysis on four different methods to sample the weights of the network during training: Dropout, DropConnect, Reparameterization Trick (RT), and Flipout. We find that Flipout <font color="#00be00">outperform</font>s all other methods regardless of the architecture used, and provides tighter constraints for the cosmological parameters. Additionally, we describe existing strategies for calibrating the networks and propose new ones. We show how tuning the regularization parameter for the scale of the approximate posterior on the weights in Flipout and RT we can produce unbiased and reliable uncertainty estimates, i.e., the regularizer acts as a hyper parameter analogous to the dropout rate in Dropout. The best performances are nevertheless achieved with a more convenient method, in which the network is let free during training to achieve the best uncalibrated performances, and the confidence intervals are then calibrated in a subsequent phase. Furthermore, we claim that the correct calibration of these networks does not change the behavior for the epistemic and aleatoric uncertainties provided for BNNs when the training dataset size changes. The results reported in the paper can be extended to other cosmological datasets in order to estimate confidence regions for features that can be extracted directly from the raw data, such as non-<font color="blue">Gaussi</font>anity signals or foreground emissions. </br></br>

<a href='http://arxiv.org/pdf/1911.09061.pdf'>1911.09061</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.9701баллов, №884</br>
<b>Challenges with Extreme Class-Imbalance and Temporal Coherence: A Study\n  on Solar Flare Data</b></br>
Authors: , Ahmadzadeh, Azim, Hostetter, Maxwell, Aydin, Berkay, Georgoulis, Manolis K., Kempton, Dustin J., Mahajan, Sushant S., Angryk, Rafal A.</br>
  In analyses of rare-events, regardless of the domain of application, class-imbalance issue is intrinsic. Although the challenges are known to data experts, their explicit impact on the analytic and the decisions made based on the findings are often overlooked. This is in particular prevalent in interdisciplinary research where the <font color="blue">theor</font>etical aspects are sometimes overshadowed by the challenges of the application. To show-case these undesirable impacts, we conduct a series of experiments on a recently created benchmark data, named Space <font color="#be00be">Weather</font> ANalytics for Solar Flares (SWAN-SF). This is a multivariate time series dataset of magnetic parameters of active regions. As a remedy for the imbalance issue, we study the impact of data manipulation (undersampling and oversampling) and model manipulation (using class weights). Furthermore, we bring to focus the auto-correlation of time series that is inherited from the use of sliding window for monitoring flares\' history. Temporal coherence, as we call this phenomenon, invalidates the randomness assumption, thus impacting all sampling practices including different cross-validation techniques. We illustrate how failing to notice this concept could give an artificial boost in the forecast performance and result in misleading findings. Throughout this study we utilized Support Vector Machine as a classifier, and True Skill Statistics as a verification metric for comparison of experiments. We conclude our work by specifying the correct practice in each case, and we hope that this study could benefit researchers in other domains where time series of rare events are of interest. </br></br>

<a href='http://arxiv.org/pdf/1911.07927.pdf'>1911.07927</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -0.9773баллов, №885</br>
<b>Deep Learning Captures More Accurate Diffusion Fiber Orientations\n  Distributions than Constrained Spherical Deconvolution</b></br>
Authors: , Nath, Vishwesh, Schilling, Kurt G., Hansen, Colin B., Parvathaneni, Prasanna, Hainline, Allison E., Bermudez, Camilo, Plassard, Andrew J., Janve, Vaibhav, Gao, Yurui, Blaber, Justin A., St&#x119;pniewska, Iwona, Anderson, Adam W., Landman, Bennett A.</br>
  Confocal <font color="#be00be">histolog</font>y provides an opportunity to establish intra-voxel fiber orientation distributions that can be used to quantitatively assess the biological relevance of diffusion weighted<font color="#be00be"> MRI </font>models, e.g., constrained spherical deconvolution (CSD). Here, we apply deep learning to investigate the potential of single shell diffusion weighted MRI to explain histologically observed fiber orientation distributions (FOD) and compare the derived deep learning model with a leading CSD approach. This study (1) demonstrates that there exists additional information in the diffusion signal that is not currently exploited by CSD, and (2) provides an illustrative data-driven model that makes use of this information. </br></br>

<a href='http://arxiv.org/pdf/1911.04474.pdf'>1911.04474</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp -0.9799баллов, №886</br>
<b>TENER: Adapting Transformer Encoder for Named Entity Recognition</b></br>
Authors: , Yan, Hang, Deng, Bocao, Li, Xiaonan, Qiu, Xipeng</br>
  The Bidirectional long short-term memory networks (BiLSTM) have been widely used as an encoder in models solving the <font color="blue">named entity</font> recognition (NER) task. Recently, the Transformer is broadly adopted in various Natural Language Processing (NLP) tasks owing to its parallelism and advantageous performance. Nevertheless, the performance of the Transformer in<font color="#be00be"> NER </font>is not as good as it is in other NLP tasks. In this paper, we propose TENER, a NER architecture adopting adapted Transformer Encoder to model the character-level features and word-level features. By incorporating the direction and relative distance aware attention and the un-scaled attention, we prove the Transformer-like encoder is just as effective for NER as other NLP tasks. </br></br>

<a href='http://arxiv.org/pdf/1911.07135.pdf'>1911.07135</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -0.9810баллов, №887</br>
<b>The Secret Revealer: Generative Model-Inversion Attacks Against Deep\n  Neural Networks</b></br>
Authors: , Zhang, Yuheng, Jia, Ruoxi, Pei, Hengzhi, Wang, Wenxiao, Li, Bo, Song, Dawn</br>
  This paper studies model-inversion attacks, in which the access to a model is abused to infer information about the training data. Since its first introduction by~\\citet{fredrikson2014<font color="#be00be">privacy</font>}, such attacks have raised serious concerns given that training data usually contain privacy sensitive information. Thus far, successful model-inversion attacks have only been demonstrated on simple models, such as linear <font color="#be00be">regression</font> and logistic regression. Previous attempts to invert neural networks, even the ones with simple architectures, have failed to produce convincing results. Here we present a novel attack method, termed the \\emph{generative model-inversion attack}, which can invert deep neural networks with high success rates. Rather than reconstructing <font color="#be00be">private</font> training data from scratch, we leverage partial public information, which can be very generic, to learn a distributional prior via generative adversarial networks (GANs) and use it to guide the inversion process. Moreover, we <font color="blue">theor</font>etically prove that a model\'s predictive power and its vulnerability to inversion attacks are indeed two sides of the same coin---highly predictive models are able to establish a strong correlation between features and labels, which coincides exactly with what an adversary exploits to mount the attacks. Our extensive experiments demonstrate that the proposed attack improves identification accuracy over the existing work by about $75\\%$ for reconstructing<font color="#be00be"> face </font>images from a <font color="red">state-of-the-art</font> face recognition classifier. We also show that differential privacy, in its canonical form, is of little avail to defend against our attacks. </br></br>

<a href='http://arxiv.org/pdf/1911.08090.pdf'>1911.08090</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp -0.9819баллов, №888</br>
<b>Deep Detector Health Management under Adversarial Campaigns</b></br>
Authors: , Echauz, Javier, Kenemer, Keith, Hussein, Sarfaraz, Dhaliwal, Jay, Shintre, Saurabh, Grzonkowski, Slawomir, Gardner, Andrew</br>
  Machine learning models are vulnerable to adversarial inputs that induce seemingly unjustifiable errors. As automated classifiers are increasingly used in industrial control systems and machinery, these adversarial errors could grow to be a serious problem. Despite numerous studies over the past few years, the field of adversarial ML is still considered alchemy, with no practical unbroken defenses demonstrated to date, leaving PHM practitioners with few meaningful ways of addressing the problem. We introduce turbidity detection as a practical superset of the adversarial input detection problem, coping with adversarial campaigns rather than statistically invisible one-offs. This perspective is coupled with ROC-<font color="blue">theor</font>etic design guidance that prescribes an inexpensive domain adaptation layer at the output of a deep learning model during an attack campaign. The result aims to approximate the <font color="blue">Bayes</font> optimal mitigation that ameliorates the detection model\'s degraded health. A proactively reactive type of prognostics is achieved via Monte Carlo simulation of various adversarial campaign scenarios, by sampling from the model\'s own turbidity distribution to quickly deploy the correct mitigation during a <font color="#009600">real-world</font> campaign. </br></br>

<a href='http://arxiv.org/pdf/1911.08125.pdf'>1911.08125</a> &nbsp&nbsp (cs:CL, cs:AI) &nbsp&nbsp -0.9911баллов, №889</br>
<b>In Search of Credible News</b></br>
Authors: , Hardalov, Momchil, Koychev, Ivan, Nakov, Preslav</br>
  We study the problem of finding fake online news. This is an important problem as news of questionable credibility have recently been proliferating in social media at an alarming scale. As this is an understudied problem, especially for languages other than English, we first collect and release to the research community three new balanced credible vs. <font color="#be00be">fake news</font> datasets derived from four online sources. We then propose a language-independent approach for automatically distinguishing credible from fake news, based on a rich feature set. In particular, we use linguistic (n-gram), credibility-related (capitalization, punctuation, pronoun use, <font color="#be00be">sentiment</font> polarity), and semantic (embeddings and DBPedia data) features. Our experiments on three different testsets show that our model can distinguish credible from fake news with very high accuracy. </br></br>

<a href='http://arxiv.org/pdf/1911.09005.pdf'>1911.09005</a> &nbsp&nbsp (cs:AI) &nbsp&nbsp -1.0183баллов, №890</br>
<b>Hard Choices in Artificial Intelligence: Addressing Normative\n  Uncertainty through Sociotechnical Commitments</b></br>
Authors: , Dobbe, Roel, Gilbert, Thomas Krendl, Mintz, Yonatan</br>
  As AI systems become prevalent in high stakes domains such as <font color="#be00be">surveillance</font> and healthcare, researchers now examine how to design and implement them in a safe manner. However, the potential harms caused by systems to stakeholders in complex social contexts and how to address these remains unclear. In this paper, we explain the inherent normative uncertainty in debates about the safety of AI systems. We then address this as a problem of vagueness by examining its place in the design, training, and deployment stages of AI system development. We adopt Ruth Chang\'s <font color="blue">theor</font>y of intuitive comparability to illustrate the dilemmas that manifest at each stage. We then discuss how stakeholders can navigate these dilemmas by incorporating distinct forms of dissent into the development pipeline, drawing on Elizabeth Anderson\'s work on the epistemic powers of democratic institutions. We outline a framework of sociotechnical commitments to formal, substantive and discursive challenges that address normative uncertainty across stakeholders, and propose the cultivation of related virtues by those responsible for development. </br></br>

<a href='http://arxiv.org/pdf/1911.07662.pdf'>1911.07662</a> &nbsp&nbsp (stat:ML, cs:ML, cs:NE) &nbsp&nbsp -1.0329баллов, №891</br>
<b>How data, synapses and neurons interact with each other: a variational\n  principle marrying gradient ascent and message passing</b></br>
Authors: , Huang, Haiping</br>
  Unsupervised learning requiring only raw data is not only a fundamental function of the cerebral cortex, but also a foundation for a next generation of artificial neural networks. However, a unified <font color="blue">theor</font>etical framework to treat sensory inputs, synapses and neural activity together is still lacking. The computational obstacle originates from the discrete nature of synapses, and complex interactions among these three essential elements of learning. Here, we propose a variational mean-field theory in which only the distribution of synaptic weight is considered. The unsupervised learning can then be decomposed into two interwoven steps: a maximization step is carried out as a gradient ascent of the lower-bound on the data log-likelihood, and an expectation step is carried out as a message passing procedure on an equivalent or dual neural network whose parameter is specified by the variational parameter of the weight distribution. Therefore, our framework explains how data (or sensory inputs), synapses and neural activities interact with each other to achieve the goal of extracting statistical regularities in sensory inputs. This variational framework is verified in restricted <font color="blue">Boltzmann</font> machines with planted synaptic weights and learning handwritten digits. </br></br>

<a href='http://arxiv.org/pdf/1911.07755.pdf'>1911.07755</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -1.0429баллов, №892</br>
<b>Learning Probably Approximately Correct Maximin Strategies in\n  Simulation-Based Games with Infinite Strategy Spaces</b></br>
Authors: , Marchesi, Alberto, Trov&#xf2;, Francesco, Gatti, Nicola</br>
  We tackle the problem of learning equilibria in simulation-based games. In such games, the players\' utility functions cannot be described analytically, as they are given through a black-box simulator that can be queried to obtain noisy estimates of the utilities. This is the case in many <font color="#009600">real-world</font> games in which a complete description of the elements involved is not available upfront, such as complex military settings and online auctions. In these situations, one usually needs to run costly simulation processes to get an accurate estimate of the game outcome. As a result, solving these games begets the challenge of designing learning algorithms that can find (approximate) equilibria with high confidence, using as few simulator queries as possible. Moreover, since running the simulator during the game is unfeasible, the algorithms must first perform a pure exploration learning phase and, then, use the (approximate) equilibrium learned this way to play the game. In this work, we focus on two-player zero-sum games with infinite strategy spaces. Drawing from the best arm identification literature, we design two algorithms with <font color="blue">theor</font>etical guarantees to learn maximin strategies in these games. The first one works in the fixed-confidence setting, guaranteeing the desired confidence level while minimizing the number of queries. Instead, the second algorithm fits the fixed-budget setting, maximizing the confidence without exceeding the given maximum number of queries. First, we formally prove {\\delta}-PAC theoretical guarantees for our algorithms under some regularity assumptions, which are encoded by letting the utility functions be drawn from a <font color="blue">Gaussi</font>an process. Then, we experimentally evaluate our techniques on a testbed made of randomly generated games and instances representing simple real-world security settings. </br></br>

<a href='http://arxiv.org/pdf/1911.07731.pdf'>1911.07731</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -1.0469баллов, №893</br>
<b>Multi-modal Deep Guided Filtering for Comprehensible Medical Image\n  Processing</b></br>
Authors: , Stimpel, Bernhard, Syben, Christopher, Schirrmacher, Franziska, Hoelter, Philipp, D&#xf6;rfler, Arnd, Maier, Andreas</br>
  Deep learning-based image processing is capable of creating highly appealing results. However, it is still widely considered as a &quot;blackbox&quot; transformation. In <font color="blue">medic</font>al imaging, this lack of comprehensibility of the results is a sensitive issue. The integration of known operators into the deep learning environment has proven to be advantageous for the comprehensibility and reliability of the computations. Consequently, we propose the use of the locally linear guided filter in combination with a learned guidance map for general purpose medical image processing. The output images are only processed by the guided filter while the guidance map can be trained to be task-optimal in an end-to-end fashion. We investigate the performance based on two popular tasks: image super resolution and denoising. The evaluation is conducted based on pairs of multi-modal <font color="blue">magnetic resonance</font> imaging and cross-modal computed <font color="#be00be">tomography</font> and magnetic resonance imaging datasets. For both tasks, the proposed approach is on par with <font color="red">state-of-the-art</font> approaches. Additionally, we can show that the input image\'s content is almost unchanged after the processing which is not the case for conventional deep learning approaches. On top, the proposed pipeline offers increased robustness against degraded input as well as <font color="blue">adversarial att</font>acks. </br></br>

<a href='http://arxiv.org/pdf/1911.08004.pdf'>1911.08004</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.0512баллов, №894</br>
<b>Consistent recovery threshold of hidden nearest neighbor graphs</b></br>
Authors: , Ding, Jian, Wu, Yihong, Xu, Jiaming, Yang, Dana</br>
  Motivated by applications such as discovering strong ties in social networks and assembling genome subsequences in biology, we study the problem of recovering a hidden $2k$-<font color="#be00be">nearest neighbo</font>r (NN) graph in an $n$-vertex complete graph, whose edge weights are independent and distributed according to $P_n$ for edges in the hidden $2k$-NN graph and $Q_n$ otherwise. The special case of Bernoulli distributions corresponds to a variant of the Watts-Strogatz small-world graph. We focus on two types of asymptotic recovery guarantees as $n\\to \\infty$: (1) exact recovery: all edges are classified correctly with probability tending to one; (2) almost exact recovery: the expected number of misclassified edges is $o(nk)$. We show that the maximum likelihood estimator achieves (1) exact recovery for $2 \\le k \\le n^{o(1)}$ if $ \\liminf \\frac{2\\alpha_n}{\\log n}&gt;1$; (2) almost exact recovery for $ 1 \\le k \\le o\\left( \\frac{\\log n}{\\log \\log n} \\right)$ if $\\liminf \\frac{kD(P_n||Q_n)}{\\log n}&gt;1$, where $\\alpha_n \\triangleq -2 \\log \\int \\sqrt{d P_n d Q_n}$ is the R\\\'enyi divergence of order $\\frac{1}{2}$ and $D(P_n||Q_n)$ is the Kullback-Leibler divergence. Under mild distributional assumptions, these conditions are shown to be information-<font color="blue">theor</font>etically necessary for any algorithm to succeed. A key challenge in the analysis is the enumeration of $2k$-NN graphs that differ from the hidden one by a given number of edges. </br></br>

<a href='http://arxiv.org/pdf/1911.09156.pdf'>1911.09156</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -1.0585баллов, №895</br>
<b>The politics of deceptive borders: \'biomarkers of deceit\' and the case\n  of iBorderCtrl</b></br>
Authors: , S&#xe1;nchez-Monedero, Javier, Dencik, Lina</br>
  This paper critically examines a recently developed proposal for a border control system called iBorderCtrl, designed to detect deception based on<font color="#be00be"> facial </font>recognition technology and the measurement of micro-expressions, termed \'biomarkers of deceit\'. Funded under the European Commission\'s Horizon 2020 programme, we situate our analysis in the wider political economy of \'<font color="#be00be">emotion</font>al AI\' and the history of deception detection technologies. We then move on to interrogate the design of iBorderCtrl using <font color="#00be00">publicly available</font> documents and assess the assumptions and scientific validation underpinning the project design. Finally, drawing on a <font color="blue">Bayes</font>ian analysis we outline statistical fallacies in the foundational premise of massive screening and argue that it is very unlikely that the model that iBorderCtrl provides for deception detection would work in practice. By interrogating actual systems in this way, we argue that we can begin to question the very premise of the development of data-driven systems, and emotional AI and deception detection in particular, pushing back on the assumption that these systems are fulfilling the tasks they claim to be attending to and instead as what function such projects carry out in the creating of subjects and management of populations. This function is not merely technical but, rather, we argue, distinctly political and forms part of a mode of governance increasingly shaping life opportunities and fundamental rights. </br></br>

<a href='http://arxiv.org/pdf/1911.08303.pdf'>1911.08303</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -1.0627баллов, №896</br>
<b>Lightweight Residual Network for The Classification of Thyroid Nodules</b></br>
Authors: , Nikhila, Ponugoti, Nathan, Sabari, Ataide, Elmer Jeto Gomes, Illanes, Alfredo, Friebe, Dr. Michael, Abbineni, Srichandana</br>
  Ultrasound is a useful technique for <font color="blue">diagnos</font>ing thyroid nodules. Benign and malignant nodules that automatically discriminate in the ultrasound pictures can provide diagnostic <font color="blue">recommendat</font>ions or, improve diagnostic accuracy in the absence of specialists. The main issue here is how to collect suitable features for this particular task. We suggest here a technique for extracting features from ultrasound pictures based on the Residual U-net. We attempt to introduce significant semantic characteristics to the classification. Our model gained 95% classification accuracy. </br></br>

<a href='http://arxiv.org/pdf/1911.08370.pdf'>1911.08370</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -1.0642баллов, №897</br>
<b>Event detection in Colombian security Twitter news using fine-grained\n  latent topic analysis</b></br>
Authors: , Vargas-Calder&#xf3;n, Vladimir, Parra-A., Nicol&#xe1;s, Camargo, Jorge E., Vinck-Posada, Herbert</br>
  Cultural and social dynamics are important concepts that must be understood in order to grasp what a community cares about. To that end, an excellent source of information on what occurs in a community is the news, especially in recent years, when mass media giants use social networks to communicate and interact with their audience. In this work, we use a method to discover latent topics in tweets from Colombian Twitter news accounts in order to identify the most prominent events in the country. We pay particular attention to security, violence and crime-related tweets because of the violent environment that surrounds Colombian society. The latent topic discovery method that we use builds vector representations of the tweets by using FastText and finds clusters of tweets through the <font color="blue">K-mean</font>s <font color="#be00be">clustering</font> algorithm. The number of clusters is found by measuring the $C_V$ coherence for a range of number of topics of the Latent Dirichlet Allocation (LDA) model. We finally use Uniform Manifold Approximation and Projection (UMAP) for dimensionality reduction to visualise the tweets vectors. Once the clusters related to security, violence and crime are identified, we proceed to apply the same method within each cluster to perform a fine-grained analysis in which specific events mentioned in the news are grouped together. Our method is able to discover event-specific sets of news, which is the baseline to perform an extensive analysis of how people engage in Twitter threads on the different types of news, with an emphasis on security, violence and crime-related tweets. </br></br>

<a href='http://arxiv.org/pdf/1911.08703.pdf'>1911.08703</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -1.0757баллов, №898</br>
<b>Bayesian sparse convex clustering via global-local shrinkage priors</b></br>
Authors: , Shimamura, Kaito, Kawano, Shuichi</br>
  Sparse convex <font color="#be00be">clustering</font> is to cluster observations and conduct variable selection simultaneously in the framework of convex clustering. Although the weighted $L_1$ norm as the regularization term is usually employed in the sparse convex clustering, this increases the dependence on the data and reduces the estimation accuracy if the sample size is not sufficient. To tackle these problems, this paper proposes a <font color="blue">Bayes</font>ian sparse convex clustering via the idea of Bayesian lasso and global-local shrinkage priors. We introduce Gibbs sampling algorithms for our method using scale mixtures of normals. The effectiveness of the proposed methods is shown in simulation studies and a real data analysis. </br></br>

<a href='http://arxiv.org/pdf/1911.07916.pdf'>1911.07916</a> &nbsp&nbsp (cs:CV, cs:ML, stat:ML) &nbsp&nbsp -1.0862баллов, №899</br>
<b>Face shape classification using Inception v3</b></br>
Authors: , Tio, Adonis Emmanuel</br>
  In this paper, we present experimental results obtained from retraining the last layer of the Inception v3 model in classifying images of human faces into one of five basic<font color="#be00be"> face </font>shapes. The accuracy of the retrained Inception v3 model was compared with that of the following classification methods that uses<font color="#be00be"> facial </font>landmark distance ratios and angles as features: linear discriminant analysis (LDA), support vector machines with linear <font color="blue">kernel</font> (<font color="blue">SVM</font>-LIN), support vector machines with radial basis function kernel (SVM-RBF), artificial neural networks or multilayer perceptron (MLP), and k-<font color="#be00be">nearest neighbo</font>rs (<font color="#be00be">KNN</font>). All classifiers were trained and tested using a total of 500 images of female celebrities with known face shapes collected from the Internet. Results show that training accuracy and overall accuracy ranges from 98.0% to 100% and from 84.4% to 84.8% for Inception v3 and from 50.6% to 73.0% and from 36.4% to 64.6% for the other classifiers depending on the training set size used. This result shows that the retrained Inception v3 model was able to fit the training data well and <font color="#00be00">outperform</font> the other classifiers without the need to handpick specific features to include in model training. Future work should consider expanding the labeled dataset, preferably one that can also be freely distributed to the research community, so that proper model cross-validation can be performed. As far as we know, this is the first in the literature to use convolutional neural networks in face-shape classification. The scripts are available at <font color="#006400">http</font>s://<font color="red">github</font>.com/adonistio/inception-face-shape-classifier. </br></br>

<a href='http://arxiv.org/pdf/1911.07418.pdf'>1911.07418</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.0868баллов, №900</br>
<b>Grassmannian Packings in Neural Networks: Learning with Maximal Subspace\n  Packings for Diversity and Anti-Sparsity</b></br>
Authors: , Yap, Dian Ang, Roberts, Nicholas, Prabhu, Vinay Uday</br>
  <font color="blue">Kernel</font> sparsity (&quot;dying ReLUs&quot;) and lack of diversity are commonly observed in CNN kernels, which decreases model capacity. Drawing inspiration from information <font color="blue">theor</font>y and wireless communications, we demonstrate the intersection of coding theory and deep learning through the Grassmannian subspace packing problem in CNNs. We propose Grassmannian packings for initial kernel layers to be initialized maximally far apart based on chordal or Fubini-Study distance. Convolutional kernels initialized with Grassmannian packings exhibit diverse features and obtain diverse representations. We show that Grassmannian packings, especially in the initial layers, address kernel sparsity and encourage diversity, while improving classification accuracy across shallow and deep CNNs with better convergence rates. </br></br>

<a href='http://arxiv.org/pdf/1911.08388.pdf'>1911.08388</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -1.0905баллов, №901</br>
<b>Multi-Resolution 3D CNN for MRI Brain Tumor Segmentation and Survival\n  Prediction</b></br>
Authors: , Amian, Mehdi, Soltaninejad, Mohammadreza</br>
  In this study, an automated three dimensional (3D) deep <font color="#be00be">segmentation</font> approach for detecting gliomas in 3D pre-operative<font color="#be00be"> MRI </font>scans is proposed. Then, a classi-fication algorithm based on <font color="blue">random forest</font>s, for survival prediction is presented. The objective is to segment the glioma area and produce segmentation labels for its different sub-regions, i.e. necrotic and the non-enhancing tumor core, the peri-tumoral edema, and enhancing tumor. The proposed deep architecture for the segmentation task encompasses two parallel streamlines with two different reso-lutions. One deep convolutional neural network is to learn local features of the input data while the other one is set to have a global observation on whole image. Deemed to be complementary, the outputs of each stream are then merged to pro-vide an ensemble complete learning of the input image. The proposed network takes the whole image as input instead of patch-based approaches in order to con-sider the semantic features throughout the whole volume. The algorithm is trained on BraTS 2019 which included 335 training cases, and validated on 127 unseen cases from the validation dataset using a blind testing approach. The proposed method was also evaluated on the BraTS 2019 challenge test dataset of 166 cases. The results show that the proposed methods provide promising segmentations as well as survival prediction. The mean Dice overlap measures of automatic <font color="#00be00">brain</font> tumor segmentation for validation set were 0.84, 0.74 and 0.71 for the whole tu-mor, core and enhancing tumor, respectively. The corresponding results for the challenge test dataset were 0.82, 0.72, and 0.70, respectively. The overall accura-cy of the proposed model for the survival prediction task is %52 for the valida-tion and %49 for the test dataset. </br></br>

<a href='http://arxiv.org/pdf/1910.00659.pdf'>1910.00659</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.0963баллов, №902</br>
<b>Forecasting Chaotic Systems with Very Low Connectivity Reservoir\n  Computers</b></br>
Authors: , Griffith, Aaron, Pomerance, Andrew, Gauthier, Daniel J.</br>
  We explore the hyperparameter space of <font color="#be00be">reservoir</font> computers used for forecasting of the chaotic Lorenz \'63 attractor with <font color="blue">Bayes</font>ian optimization. We use a new measure of reservoir performance, designed to emphasize learning the global <font color="#be00be">climate</font> of the forecasted system rather than short-term prediction. We find that optimizing over this measure more quickly excludes reservoirs that fail to reproduce the climate. The results of optimization are surprising: the optimized parameters often specify a reservoir network with very low connectivity. Inspired by this observation, we explore reservoir designs with even simpler structure, and find well-performing reservoirs that have zero spectral radius and no recurrence. These simple reservoirs provide counterexamples to widely used heuristics in the field, and may be useful for hardware implementations of reservoir computers. </br></br>

<a href='http://arxiv.org/pdf/1911.09217.pdf'>1911.09217</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -1.0996баллов, №903</br>
<b>REVAMP$^2$T: Real-time Edge Video Analytics for Multi-camera\n  Privacy-aware Pedestrian Tracking</b></br>
Authors: , Neff, Christopher, Mendieta, Mat&#xed;as, Mohan, Shrey, Baharani, Mohammadreza, Rogers, Samuel, Tabkhi, Hamed</br>
  This article presents REVAMP$^2$T, Real-time Edge Video Analytics for Multi-camera <font color="#be00be">Privacy</font>-aware <font color="#be00be">Pedestrian</font> <font color="#be00be">Tracking</font>, as an integrated end-to-end IoT system for privacy-built-in decentralized situational awareness. REVAMP$^2$T presents novel algorithmic and system constructs to push deep learning and video analytics next to IoT devices (i.e. video cameras). On the algorithm side, REVAMP$^2$T proposes a unified integrated computer vision pipeline for detection, <font color="blue">re-identification</font>, and tracking across multiple cameras without the need for storing the streaming data. At the same time, it avoids<font color="#be00be"> facial </font>recognition, and tracks and re-identifies pedestrians based on their key features at runtime. On the IoT system side, REVAMP$^2$T provides infrastructure to maximize hardware utilization on the edge, orchestrates global communications, and provides system-wide re-identification, without the use of personally identifiable information, for a distributed IoT network. For the results and evaluation, this article also proposes a new metric, Accuracy$\\cdot$Efficiency (\\AE), for holistic evaluation of IoT systems for real-time video analytics based on accuracy, performance, and power efficiency. REVAMP$^2$T <font color="#00be00">outperform</font>s current <font color="red">state-of-the-art</font> by as much as thirteen-fold \\AE~improvement. </br></br>

<a href='http://arxiv.org/pdf/1911.06919.pdf'>1911.06919</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp -1.1167баллов, №904</br>
<b>Improved Document Modelling with a Neural Discourse Parser</b></br>
Authors: , Koto, Fajri, Lau, Jey Han, Baldwin, Timothy</br>
  Despite the success of attention-based neural models for natural language generation and classification tasks, they are unable to capture the discourse structure of larger documents. We hypothesize that explicit discourse representations have utility for NLP tasks over longer documents or document sequences, which sequence-to-sequence models are unable to capture. For abstractive <font color="#be00be">summarization</font>, for instance, conventional neural models simply match source documents and the summary in a latent space without explicit representation of text structure or relations. In this paper, we propose to use neural discourse representations obtained from a rhetorical structure <font color="blue">theor</font>y (RST) <font color="#be00be">parser</font> to enhance document representations. Specifically, document representations are generated for discourse spans, known as the elementary discourse units (EDUs). We empirically investigate the benefit of the proposed approach on two different tasks: abstractive summarization and popularity prediction of online petitions. We find that the proposed approach leads to improvements in all cases. </br></br>

<a href='http://arxiv.org/pdf/1911.08716.pdf'>1911.08716</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -1.1169баллов, №905</br>
<b>DermGAN: Synthetic Generation of Clinical Skin Images with Pathology</b></br>
Authors: , Ghorbani, Amirata, Natarajan, Vivek, Coz, David, Liu, Yuan</br>
  Despite the recent success in applying supervised deep learning to <font color="blue">medic</font>al imaging tasks, the problem of obtaining large and diverse expert-annotated datasets required for the development of high performant models remains particularly challenging. In this work, we explore the possibility of using Generative Adverserial Networks (GAN) to synthesize <font color="blue">clinic</font>al images with skin condition. We propose DermGAN, an adaptation of the popular Pix2Pix architecture, to create synthetic images for a pre-specified skin condition while being able to vary its size, location and the underlying skin color. We demonstrate that the generated images are of high fidelity using objective GAN evaluation metrics. In a Human Turing test, we note that the synthetic images are not only visually similar to real images, but also embody the respective skin condition in dermatologists\' eyes. Finally, when using the synthetic images as a data augmentation technique for training a skin condition classifier, we observe that the model performs comparably to the baseline model overall while improving on rare but malignant conditions. </br></br>

<a href='http://arxiv.org/pdf/1911.08010.pdf'>1911.08010</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -1.1170баллов, №906</br>
<b>Convolutional Neural Network and decision support in medical imaging:\n  case study of the recognition of blood cell subtypes</b></br>
Authors: , Diouf, Daouda, Seck, Djibril, Diop, Mountaga, Ba, Abdoulye</br>
  Identifying and characterizing the <font color="blue">patient</font>\'s blood samples is indispensable in <font color="blue">diagnos</font>tics of malignance suspicious. A painstaking and sometimes subjective task is used in laboratories to manually classify white blood cells. Neural mathematical methods as deep learnings can be very useful in the automated recognition of blood cells. This study uses a particular type of deep learning i.e., convolutional neural networks (CNNs or ConvNets) for image recognition of the four (4) blood cell types (neutrophil, eosinophil, lymphocyte and monocyte) and to enable it to tag them employing a dataset of blood cells with labels for the corresponding cell types. The elements of the database are the input of our CNN and they allowed us to create learning models for the image recognition/classification of the blood cells. We evaluated the recognition performance and outputs learned by the networks in order to implement a neural image recognition model capable of distinguishing polynuclear cells (neutrophil and eosinophil) from those of mononuclear cells (lymphocyte and monocyte). The validation accuracy is 97.77%. </br></br>

<a href='http://arxiv.org/pdf/1911.08251.pdf'>1911.08251</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -1.1179баллов, №907</br>
<b>General $E(2)$-Equivariant Steerable CNNs</b></br>
Authors: , Weiler, Maurice, Cesa, Gabriele</br>
  The big empirical success of group equivariant networks has led in recent years to the sprouting of a great variety of equivariant network architectures. A particular focus has thereby been on rotation and reflection equivariant CNNs for planar images. Here we give a general description of $E(2)$-equivariant convolutions in the framework of Steerable CNNs. The <font color="blue">theor</font>y of Steerable CNNs thereby yields constraints on the convolution <font color="blue">kernel</font>s which depend on group representations describing the transformation laws of feature spaces. We show that these constraints for arbitrary group representations can be reduced to constraints under irreducible representations. A general solution of the kernel space constraint is given for arbitrary representations of the Euclidean group $E(2)$ and its subgroups. We implement a wide range of previously proposed and entirely new equivariant network architectures and extensively compare their performances. $E(2)$-steerable convolutions are further shown to yield remarkable gains on CIFAR-10, CIFAR-100 and STL-10 when used as a drop-in replacement for non-equivariant convolutions. </br></br>

<a href='http://arxiv.org/pdf/1911.06616.pdf'>1911.06616</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.1309баллов, №908</br>
<b>Detecting cutaneous basal cell carcinomas in ultra-high resolution and\n  weakly labelled histopathological images</b></br>
Authors: , Kimeswenger, Susanne, Rumetshofer, Elisabeth, Hofmarcher, Markus, Tschandl, Philipp, Kittler, Harald, Hochreiter, Sepp, H&#xf6;tzenecker, Wolfram, Klambauer, G&#xfc;nter</br>
  <font color="blue">Diagnos</font>ing basal cell carcinomas (BCC), one of the most common cutaneous malignancies in humans, is a task regularly performed by <font color="blue">patholog</font>ists and dermato-pathologists. Improving <font color="#be00be">histolog</font>ical diagnosis by providing diagnosis suggestions, i.e. computer-assisted diagnoses is actively researched to improve safety, quality and efficiency. Increasingly, machine learning methods are applied due to their superior performance. However, typical images obtained by scanning histological sections often have a resolution that is prohibitive for processing with current <font color="red">state-of-the-art</font> neural networks. Furthermore, the data pose a problem of weak labels, since only a tiny fraction of the image is indicative of the <font color="blue">diseas</font>e class, whereas a large fraction of the image is highly similar to the non-disease class. The aim of this study is to evaluate whether it is possible to detect basal cell carcinomas in histological sections using attention-based deep learning models and to overcome the ultra-high resolution and the weak labels of whole slide images. We demonstrate that attention-based models can indeed yield almost perfect classification performance with an AUC of 0.95. </br></br>

<a href='http://arxiv.org/pdf/1911.08654.pdf'>1911.08654</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.1370баллов, №909</br>
<b>Adversarial Robustness of Flow-Based Generative Models</b></br>
Authors: , Pope, Phillip, Balaji, Yogesh, Feizi, Soheil</br>
  Flow-based generative models leverage invertible generator functions to fit a distribution to the training data using maximum likelihood. Despite their use in several application domains, robustness of these models to <font color="blue">adversarial att</font>acks has hardly been explored. In this paper, we study adversarial robustness of flow-based generative models both <font color="blue">theor</font>etically (for some simple models) and empirically (for more complex ones). First, we consider a linear flow-based generative model and compute optimal sample-specific and universal adversarial perturbations that maximally decrease the likelihood scores. Using this result, we study the robustness of the well-known adversarial training procedure, where we characterize the fundamental trade-off between model robustness and accuracy. Next, we empirically study the robustness of two prominent deep, non-linear, flow-based generative models, namely GLOW and RealNVP. We design two types of adversarial attacks; one that minimizes the likelihood scores of in-distribution samples, while the other that maximizes the likelihood scores of out-of-distribution ones. We find that GLOW and RealNVP are extremely sensitive to both types of attacks. Finally, using a hybrid adversarial training procedure, we significantly boost the robustness of these generative models. </br></br>

<a href='http://arxiv.org/pdf/1910.08771.pdf'>1910.08771</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -1.1430баллов, №910</br>
<b>Convex Reconstruction of Structured Matrix Signals from Linear\n  Measurements (I): Theoretical Results</b></br>
Authors: , Tian, Yuan</br>
  We investigate the problem of reconstructing n-by-n structured matrix signal X via convex programming, where each column xj is a vector of s-sparsity and all columns have the same l1-norm. The regularizer is matrix norm |||X|||1=maxj|xj|1.The contribution in this paper has two parts. The first part is about conditions for stability and robustness in signal reconstruction via solving the convex programming from noise-free or noisy measurements.We establish uniform sufficient conditions which are very close to necessary conditions and non-uniform conditions are also discussed. Similar as the traditional compressive sensing <font color="blue">theor</font>y for reconstructing vector signals, a related RIP condition is established. In addition, stronger conditions are investigated to guarantee the reconstructed signal\'s support stability, sign stability and approximation-error robustness. The second part is to establish upper and lower bounds on number of measurements for robust reconstruction in noise. We take the convex geometric approach in random measurement setting and one of the critical ingredients in this approach is to estimate the related widths bounds in case of <font color="blue">Gaussi</font>an and non-Gaussian distributions. These bounds are explicitly controlled by signal\'s structural parameters r and s which determine matrix signal\'s column-wise sparsity and l1-column-flatness respectively. </br></br>

<a href='http://arxiv.org/pdf/1911.07626.pdf'>1911.07626</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.1440баллов, №911</br>
<b>Convex Formulation of Overparameterized Deep Neural Networks</b></br>
Authors: , Fang, Cong, Gu, Yihong, Zhang, Weizhong, Zhang, Tong</br>
  Analysis of over-parameterized neural networks has drawn significant attention in recentyears. It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent <font color="blue">kernel</font> space around specialized initializations. However, there areno <font color="blue">theor</font>etical techniques that can analyze fully trained deep neural networks encountered inpractice. This paper solves this fundamental problem by investigating such overparameterizeddeep neural networks when fully trained. We generalize a new technique called neural feature repopulation, originally introduced in (Fang et al., 2019a) for two-level neural networks, to analyze deep neural networks. It is shown that under suitable representations, overparameterized deep neural networks are inherently convex, and when optimized, the system can learn effective features suitable for the underlying learning task under mild conditions. This new analysis is consistent with empirical observations that deep neural networks are capable of learning efficient feature representations. Therefore, the highly unexpected result of this paper can satisfactorily explain the practical success of deep neural networks. Empirical studies confirm that predictions of our theory are consistent with results observed in practice. </br></br>

<a href='http://arxiv.org/pdf/1911.07923.pdf'>1911.07923</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -1.1459баллов, №912</br>
<b>Cluster-wise Unsupervised Hashing for Cross-Modal Similarity Search</b></br>
Authors: , Wang, Lu, Yang, Jie</br>
  In this paper, we present a new cluster-wise unsupervised hashing (CUH) approach to learn compact binary codes for cross-modal similarity retrieval. We develop a discrete optimization method to jointly learn binary codes and the corresponding hash functions for each modality which can improve the performance, unlike existing cross-modal hashing methods that often drop the binary constraints to obtain the binary codes. Moreover, considering the semantic consistency between observed modalities, our CUH generates one unified hash code for all observed modalities of any instance. Specifically, we construct a co-training framework for learning to hash, in which we simultaneously realize the multi-view <font color="#be00be">clustering</font> and the learning of hash. Firstly, our CUH utilize the re-weighted discriminatively embedded <font color="blue">K-mean</font>s for multi-view clustering to learn the corresponding dimension reduced data and the cluster centroid points in the low-dimensional common subspaces, which are used as the approximation to the corresponding hash codes of original data and the cluster-wise code-prototypes respectively. Secondly, in the process for learning of hash, these cluster-wise code-prototypes can guide the learning of the codes to further improve the performance of the binary codes. The reasonableness and effectiveness of CUH is well demonstrated by comprehensive experiments on diverse benchmark datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.09358.pdf'>1911.09358</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -1.1491баллов, №913</br>
<b>Gliding vertex on the horizontal bounding box for multi-oriented object\n  detection</b></br>
Authors: , Xu, Yongchao, Fu, Mingtao, Wang, Qimeng, Wang, Yukang, Chen, Kai, Xia, Gui-Song, Bai, Xiang</br>
  <font color="#be00be">Object detection</font> has recently experienced substantial progress. Yet, the widely adopted horizontal bounding box representation is not appropriate for ubiquitous oriented objects such as objects in aerial images and scene texts. In this paper, we propose a simple yet effective framework to detect multi-oriented objects. Instead of directly regressing the four vertices, we glide the vertex of the horizontal bounding box on each corresponding side to accurately describe a multi-oriented object. Specifically, We regress four length ratios characterizing the relative gliding offset on each corresponding side. This may facilitate the offset learning and avoid the confusion issue of sequential label points for oriented objects. To further remedy the confusion issue for nearly horizontal objects, we also introduce an obliquity factor based on area ratio between the object and its horizontal bounding box, guiding the selection of horizontal or oriented detection for each object. We add these five extra target variables to the <font color="#be00be">regression</font> head of fast R-CNN, which requires ignorable extra computation time. Extensive experimental results demonstrate that without bells and whistles, the proposed method achieves superior performances on multiple multi-oriented object detection benchmarks including object detection in aerial images, scene text detection, <font color="#be00be">pedestrian</font> detection in fisheye images. </br></br>

<a href='http://arxiv.org/pdf/1911.09660.pdf'>1911.09660</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -1.1514баллов, №914</br>
<b>Estimating uncertainty of earthquake rupture using Bayesian neural\n  network</b></br>
Authors: , Ahamed, Sabber</br>
  <font color="blue">Bayes</font>ian neural networks (BNN) are the probabilistic model that combines the strengths of both neural network (NN) and stochastic processes. As a result, BNN can combat overfitting and perform well in applications where data is limited. Earthquake rupture study is such a problem where data is insufficient, and scientists have to rely on many trial and error numerical or physical models. Lack of resources and computational expenses, often, it becomes hard to determine the reasons behind the earthquake rupture. In this work, a BNN has been used (1) to combat the small data problem and (2) to find out the parameter combinations responsible for earthquake rupture and (3) to estimate the uncertainty associated with earthquake rupture. Two thousand rupture simulations are used to train and test the model. A simple 2D rupture geometry is considered where the fault has a <font color="blue">Gaussi</font>an geometric heterogeneity at the center, and eight parameters vary in each simulation. The test F1-score of BNN (0.8334), which is 2.34% higher than plain NN score. Results show that the parameters of rupture propagation have higher uncertainty than the rupture arrest. Normal stresses play a vital role in determining rupture propagation and are also the highest source of uncertainty, followed by the dynamic friction coefficient. Shear stress has a moderate role, whereas the geometric features such as the width and height of the fault are least significant and uncertain. </br></br>

<a href='http://arxiv.org/pdf/1911.07498.pdf'>1911.07498</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.1530баллов, №915</br>
<b>Online Adaptive Asymmetric Active Learning with Limited Budgets</b></br>
Authors: , Zhang, Yifan, Zhao, Peilin, Niu, Shuaicheng, Wu, Qingyao, Cao, Jiezhang, Huang, Junzhou, Tan, Mingkui</br>
  Online Active Learning (OAL) aims to manage unlabeled datastream by selectively querying the label of data. OAL is applicable to many <font color="#009600">real-world</font> problems, such as <font color="#be00be">anomal</font>y detection in health-care and <font color="#be00be">financ</font>e. In these problems, there are two key challenges: the query budget is often limited; the ratio between classes is highly imbalanced. In practice, it is quite difficult to handle imbalanced unlabeled datastream when only a limited budget of labels can be queried for training. To solve this, previous OAL studies adopt either asymmetric losses or queries (an isolated asymmetric strategy) to tackle the imbalance, and use first-order methods to optimize the cost-sensitive measure. However, the isolated strategy limits their performance in class imbalance, while first-order methods restrict their optimization performance. In this paper, we propose a novel Online Adaptive Asymmetric Active learning algorithm, based on a new asymmetric strategy (merging both asymmetric losses and queries strategies), and second-order optimization. We <font color="blue">theor</font>etically analyze its mistake bound and cost-sensitive metric bounds. Moreover, to better balance performance and efficiency, we enhance our algorithm via a sketching technique, which significantly accelerates the computational speed with quite slight performance degradation. Promising results demonstrate the effectiveness and efficiency of the proposed methods. </br></br>

<a href='http://arxiv.org/pdf/1911.07922.pdf'>1911.07922</a> &nbsp&nbsp (cs:CV, cs:ML, stat:ML) &nbsp&nbsp -1.1602баллов, №916</br>
<b>Patch augmentation: Towards efficient decision boundaries for neural\n  networks</b></br>
Authors: , Bloice, Marcus D., Holzinger, Andreas</br>
  In this paper we propose a new augmentation technique, called patch augmentation, that, in our experiments, improves model accuracy and makes networks more robust to <font color="blue">adversarial att</font>acks. In brief, this data-independent approach creates new image data based on image/label pairs, where a patch from one of the two images in the pair is superimposed on to the other image, creating a new augmented sample. The new image\'s label is a linear combination of the image pair\'s corresponding labels. Initial experiments show a several percentage point increase in accuracy on CIFAR-10, from a baseline of 80.6% to 86.8%. CIFAR-100 sees better improvements still. Networks trained using patch augmentation are also more robust to adversarial attacks, which we demonstrate using the Fast Gradient Sign Method. An adversarial misclassification, or adversarial attack, occurs when an image that should seemingly be easily classified correctly by the network is suddenly classified as belonging to a completely different class-and with high confidence. Such occurrences are difficult to <font color="blue">diagnos</font>e and are a cause of much concern in artificial intelligence research, as any model trained with empirical risk minimisation seems to be vulnerable to such attacks. The ease at which neural networks are susceptible to adversarial perturbations are partially the result of images lying close to the decision boundaries that are typically learned by neural networks during their training. Patch augmentation is an attempt to train more efficient decision boundaries. </br></br>

<a href='http://arxiv.org/pdf/1911.07227.pdf'>1911.07227</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -1.1611баллов, №917</br>
<b>Iterative Construction of Gaussian Process Surrogate Models for Bayesian\n  Inference</b></br>
Authors: , Alawieh, Leen, Goodman, Jonathan, Bell, John B.</br>
  A new algorithm is developed to tackle the issue of sampling non-<font color="blue">Gaussi</font>an model parameter posterior probability distributions that arise from solutions to <font color="blue">Bayes</font>ian inverse problems. The algorithm aims to mitigate some of the hurdles faced by traditional Markov Chain Monte Carlo (MCMC) samplers, through constructing proposal probability densities that are both, easy to sample and that provide a better approximation to the target density than a simple Gaussian proposal distribution would. To achieve that, a Gaussian proposal distribution is augmented with a Gaussian Process (GP) surface that helps capture non-linearities in the log-likelihood function. In order to train the GP surface, an iterative approach is adopted for the optimal selection of points in parameter space. Optimality is sought by maximizing the information gain of the GP surface using a minimum number of forward model simulation runs. The accuracy of the GP-augmented surface approximation is assessed in two ways. The first consists of comparing predictions obtained from the approximate surface with those obtained through running the actual simulation model at hold-out points in parameter space. The second consists of a measure based on the relative variance of sample weights obtained from sampling the approximate posterior probability distribution of the model parameters. The efficacy of this new algorithm is tested on inferring reaction rate parameters in a 3-node and 6-node network toy problems, which imitate idealized reaction networks in combustion applications. </br></br>

<a href='http://arxiv.org/pdf/1911.07623.pdf'>1911.07623</a> &nbsp&nbsp (cs:RO) &nbsp&nbsp -1.1662баллов, №918</br>
<b>Machine Vision for Improved Human-Robot Cooperation in Adverse\n  Underwater Conditions</b></br>
Authors: , Islam, Md Jahidul</br>
  Visually-guided underwater robots are widely used in numerous autonomous exploration and <font color="#be00be">surveillance</font> applications alongside humans for cooperative task execution. However, underwater visual perception is challenging due to marine artifacts such as poor visibility, lighting variation, scattering, etc. Additionally, chromatic distortions and scarcity of salient visual features make it harder for an underwater robot to visually <font color="#be00be">interpret</font> its surroundings to effectively assist its companion diver during an underwater mission. In this paper, we delineate our attempts to address these challenges by designing novel and improved vision-based solutions. Specifically, we present robust methodologies for autonomous diver following, human-robot communication, automatic image enhancement, and image <font color="#be00be">super-resolution</font>. We depict their algorithmic details and describe relevant design choices to meet the real-time operating constraints on single-board embedded machines. Moreover, through extensive simulation and field experiments, we demonstrate how an autonomous robot can exploit these solutions to understand human motion and hand gesture-based instructions even in adverse visual conditions. As an immediate next step, we want to focus on relative pose estimation and visual attention modeling of an underwater robot based on its companion humans\' body-pose and temporal activity recognition. We believe that these autonomous capabilities will facilitate a faster and better interpretation of visual scenes and enable more effective underwater human-robot cooperation. </br></br>

<a href='http://arxiv.org/pdf/1911.08378.pdf'>1911.08378</a> &nbsp&nbsp (cs:ML, cs:AI, stat:ML) &nbsp&nbsp -1.1848баллов, №919</br>
<b>PRINCE: Provider-side Interpretability with Counterfactual Explanations\n  in Recommender Systems</b></br>
Authors: , Ghazimatin, Azin, Balalau, Oana, Roy, Rishiraj Saha, Weikum, Gerhard</br>
  <font color="#be00be">Interpret</font>able explanations for recommender systems and other machine learning models are crucial to gain user trust. Prior works that have focused on paths connecting users and items in a heterogeneous network have several limitations, such as discovering relationships rather than true explanations, or disregarding other users\' <font color="#be00be">privacy</font>. In this work, we take a fresh perspective, and present PRINCE: a provider-side mechanism to produce tangible explanations for end-users, where an explanation is defined to be a set of minimal actions performed by the user that, if removed, changes the <font color="blue">recommendat</font>ion to a different item. Given a recommendation, PRINCE uses a polynomial-time optimal algorithm for finding this minimal set of a user\'s actions from an exponential search space, based on random walks over dynamic graphs. Experiments on two <font color="#009600">real-world</font> datasets show that PRINCE provides more compact explanations than intuitive baselines, and insights from a crowdsourced user-study demonstrate the viability of such action-based explanations. We thus posit that PRINCE produces scrutable, actionable, and concise explanations, owing to its use of counterfactual evidence, a user\'s own actions, and minimal sets, respectively. </br></br>

<a href='http://arxiv.org/pdf/1911.07128.pdf'>1911.07128</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.2087баллов, №920</br>
<b>An Empirical and Comparative Analysis of Data Valuation with Scalable\n  Algorithms</b></br>
Authors: , Jia, Ruoxi, Sun, Xuehui, Xu, Jiacen, Zhang, Ce, Li, Bo, Song, Dawn</br>
  This paper focuses on valuating training data for supervised learning tasks and studies the Shapley value, a data value notion originated in cooperative game <font color="blue">theor</font>y. The Shapley value defines a unique value distribution scheme that satisfies a set of appealing properties desired by a data value notion. However, the Shapley value requires exponential complexity to calculate exactly. Existing approximation algorithms, although achieving great improvement over the exact algorithm, relies on retraining models for multiple times, thus remaining limited when applied to larger-scale learning tasks and <font color="#009600">real-world</font> datasets.   In this work, we develop a simple and efficient heuristic for data valuation based on the Shapley value with complexity independent with the model size. The key idea is to approximate the model via a $K$-<font color="#be00be">nearest neighbo</font>r ($K$NN) classifier, which has a locality structure that can lead to efficient Shapley value calculation. We evaluate the utility of the values produced by the $K$NN proxies in various settings, including label noise correction, watermark detection, data <font color="#be00be">summarization</font>, active data acquisition, and domain adaption. Extensive experiments demonstrate that our algorithm achieves at least comparable utility to the values produced by existing algorithms while significant efficiency improvement. Moreover, we theoretically analyze the Shapley value and justify its advantage over the leave-one-out error as a data value measure. </br></br>

<a href='http://arxiv.org/pdf/1911.07925.pdf'>1911.07925</a> &nbsp&nbsp (cs:CV, cs:ML, cs:NE) &nbsp&nbsp -1.2372баллов, №921</br>
<b>WaveletKernelNet: An Interpretable Deep Neural Network for Industrial\n  Intelligent Diagnosis</b></br>
Authors: , Li, Tianfu, Zhao, Zhibin, Sun, Chuang, Cheng, Li, Chen, Xuefeng, Yan, Ruqaing, Gao, Robert X.</br>
  Convolutional neural network (CNN), with ability of feature learning and nonlinear mapping, has demonstrated its effectiveness in prognostics and health management (PHM). However, explanation on the physical meaning of a CNN architecture has rarely been studied. In this paper, a novel wavelet driven deep neural network termed as Wavelet<font color="blue">Kernel</font>Net (WKN) is presented, where a continuous wavelet convolutional (CWConv) layer is designed to replace the first convolutional layer of the standard CNN. This enables the first CWConv layer to discover more meaningful filters. Furthermore, only the scale parameter and translation parameter are directly learned from raw data at this CWConv layer. This provides a very effective way to obtain a customized filter bank, specifically tuned for extracting defect-related impact component embedded in the vibration signal. In addition, three experimental verification using data from laboratory environment are carried out to verify effectiveness of the proposed method for mechanical fault <font color="blue">diagnos</font>is. The results show the importance of the designed CWConv layer and the output of CWConv layer is <font color="#be00be">interpret</font>able. Besides, it is found that WKN has fewer parameters, higher fault classification accuracy and faster convergence speed than standard CNN. </br></br>

<a href='http://arxiv.org/pdf/1911.07223.pdf'>1911.07223</a> &nbsp&nbsp (cs:CL, cs:ML) &nbsp&nbsp -1.2463баллов, №922</br>
<b>Deep Learning versus Traditional Classifiers on Vietnamese Students\'\n  Feedback Corpus</b></br>
Authors: , Nguyen, Phu X. V., Hong, Tham T. T., Van Nguyen, Kiet, Nguyen, Ngan Luu-Thuy</br>
  Student\'s feedback is an important source of collecting students\' opinions to improve the quality of training activities. Implementing <font color="#be00be">sentiment</font> analysis into student feedback data, we can determine sentiments polarities which express all problems in the institution since changes necessary will be applied to improve the quality of teaching and learning. This study focused on machine learning and natural language processing techniques (Naive<font color="blue">Bayes</font>, Maximum Entropy, Long Short-Term Memory, Bi-Directional Long Short-Term Memory) on the <font color="#be00be">Vietnamese</font>Students\' Feedback Corpus collected from a university. The final results were compared and evaluated to find the most effective model based on different evaluation criteria. The experimental results show that the Bi-Directional LongShort-Term Memory algorithm <font color="#00be00">outperform</font>ed than three other algorithms in terms of the F1-score measurement with 92.0% on the sentiment classification task and 89.6% on the topic classification task. In addition, we developed a sentiment analysis application analyzing student feedback. The application will help the institution to recognize students\' opinions about a problem and identify shortcomings that still exist. With the use of this application, the institution can propose an appropriate method to improve the quality of training activities in the future. </br></br>

<a href='http://arxiv.org/pdf/1911.06813.pdf'>1911.06813</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.2714баллов, №923</br>
<b>Transfer Learning of fMRI Dynamics</b></br>
Authors: , Mahmood, Usman, Rahman, Md Mahfuzur, Fedorov, Alex, Fu, Zening, Plis, Sergey</br>
  As a mental <font color="blue">disorder</font> progresses, it may affect <font color="#00be00">brain</font> structure, but brain function expressed in brain dynamics is affected much earlier. Capturing the moment when brain dynamics express the disorder is crucial for early <font color="blue">diagnos</font>is. The traditional approach to this problem via training classifiers either proceeds from handcrafted features or requires large datasets to combat the $m&gt;&gt;n$ problem when a high dimensional <font color="#be00be">fMRI</font> volume only has a single label that carries learning signal. Large datasets may not be available for a study of each disorder, or rare disorder types or sub-populations may not warrant for them. In this paper, we demonstrate a self-supervised pre-training method that enables us to pre-train directly on fMRI dynamics of healthy control subjects and transfer the learning to much smaller datasets of schizophrenia. Not only we enable classification of disorder directly based on fMRI dynamics in small data but also significantly speed up the learning when possible. This is encouraging evidence of informative transfer learning across datasets and diagnostic categories. </br></br>

<a href='http://arxiv.org/pdf/1911.07963.pdf'>1911.07963</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.3280баллов, №924</br>
<b>Can You Really Backdoor Federated Learning?</b></br>
Authors: , Sun, Ziteng, Kairouz, Peter, Suresh, Ananda Theertha, McMahan, H. Brendan</br>
  The decentralized nature of <font color="#be00be">federated</font> learning makes detecting and defending against <font color="blue">adversarial att</font>acks a challenging task. This paper focuses on backdoor attacks in the federated learning setting, where the goal of the adversary is to reduce the performance of the model on targeted tasks while maintaining good performance on the main task. Unlike existing works, we allow non-malicious clients to have correctly labeled samples from the targeted tasks. We conduct a comprehensive study of backdoor attacks and defenses for the EMNIST dataset, a real-life, user-partitioned, and non-iid dataset. We observe that in the absence of defenses, the performance of the attack largely depends on the fraction of adversaries present and the &quot;complexity\'\' of the targeted task. Moreover, we show that norm clipping and &quot;weak\'\' differential <font color="#be00be">privacy</font> mitigate the attacks without hurting the overall performance. We have implemented the attacks and defenses in TensorFlow Federated (TFF), a TensorFlow framework for federated learning. In open-sourcing our code, our goal is to encourage researchers to contribute new attacks and defenses and evaluate them on standard federated datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.07014.pdf'>1911.07014</a> &nbsp&nbsp (cs:ML, cs:CV) &nbsp&nbsp -1.3441баллов, №925</br>
<b>What Will Your Child Look Like? DNA-Net: Age and Gender Aware Kin Face\n  Synthesizer</b></br>
Authors: , Gao, Pengyu, Xia, Siyu, Robinson, Joseph, Zhang, Junkang, Xia, Chao, Shao, Ming, Fu, Yun</br>
  Visual kinship recognition aims to identify blood relatives from<font color="#be00be"> facial </font>images. Its practical application-- like in law-enforcement, video <font color="#be00be">surveillance</font>, automatic family album management, and more-- has motivated many researchers to put forth effort on the topic as of recent. In this paper, we focus on a new view of visual kinship technology: kin-based<font color="#be00be"> face </font>generation. Specifically, we propose a two-stage kin-face generation model to predict the appearance of a child given a pair of parents. The first stage includes a deep generative adversarial autoencoder conditioned on ages and genders to map between facial appearance and high-level features. The second stage is our proposed DNA-Net, which serves as a transformation between the deep and <font color="#be00be">genetic</font> features based on a random selection process to fuse genes of a parent pair to form the genes of a child. We demonstrate the effectiveness of the proposed method quantitatively and qualitatively: quantitatively, pre-trained models and human subjects perform kinship verification on the generated images of children; qualitatively, we show photo-realistic face images of children that closely resemble the given pair of parents. In the end, experiments validate that the proposed model synthesizes convincing kin-faces using both subjective and objective standards. </br></br>

<a href='http://arxiv.org/pdf/1911.07984.pdf'>1911.07984</a> &nbsp&nbsp (cs:ML, cs:CV, stat:ML) &nbsp&nbsp -1.3497баллов, №926</br>
<b>Learning Permutation Invariant Representations using Memory Networks</b></br>
Authors: , Kalra, Shivam, Adnan, Mohammed, Taylor, Graham, Tizhoosh, Hamid</br>
  Many <font color="#009600">real world</font> tasks such as 3D <font color="#be00be">object detection</font> and high-resolution image classification involve learning from a set of instances. In these cases, only a group of instances, a set, collectively contains meaningful information and therefore only the sets have labels, and not individual data instances. In this work, we present a permutation invariant neural network called a \\textbf{Memory-based Exchangeable Model (MEM)} for learning set functions. The model consists of memory units that embed an input sequence to high-level features (memories) enabling the model to learn inter-dependencies among instances of the set in the form of attention vectors. To demonstrate its learning ability, we evaluated our model on test datasets created using MNIST, <font color="#be00be">point cloud</font> classification, and population estimation. We also tested the model for classifying histo<font color="blue">patholog</font>y whole slide images to discriminate between two subtypes of Lung <font color="#be00be">cancer</font>---Lung Adenocarcinoma, and Lung Squamous Cell Carcinoma. We systematically extracted patches from lung cancer images from The Cancer Genome Atlas~(TCGA) dataset, the largest public repository of histopathology images. The proposed method achieved a <font color="#960096">competitive</font> classification accuracy of 84.84\\%. The results on other datasets are promising and demonstrate the efficacy of our model. </br></br>

<a href='http://arxiv.org/pdf/1911.08147.pdf'>1911.08147</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -1.3514баллов, №927</br>
<b>Learning Weighted Submanifolds with Variational Autoencoders and\n  Riemannian Variational Autoencoders</b></br>
Authors: , Miolane, Nina, Holmes, Susan</br>
  Manifold-valued data naturally arises in <font color="blue">medic</font>al imaging. In cognitive neuroscience, for instance, <font color="#00be00">brain</font> connectomes base the analysis of coactivation patterns between different brain regions on the analysis of the correlations of their functional <font color="blue">Magnetic Resonance</font> Imaging (<font color="#be00be">fMRI</font>) time series - an object thus constrained by construction to belong to the manifold of symmetric positive definite matrices. One of the challenges that naturally arises consists of finding a lower-dimensional subspace for representing such manifold-valued data. Traditional techniques, like principal component analysis, are ill-adapted to tackle non-Euclidean spaces and may fail to achieve a lower-dimensional representation of the data - thus potentially pointing to the absence of lower-dimensional representation of the data. However, these techniques are restricted in that: (i) they do not leverage the assumption that the connectomes belong on a pre-specified manifold, therefore discarding information; (ii) they can only fit a linear subspace to the data. In this paper, we are interested in variants to learn potentially highly curved submanifolds of manifold-valued data. Motivated by the brain connectomes example, we investigate a latent variable generative model, which has the added benefit of providing us with uncertainty estimates - a crucial quantity in the medical applications we are considering. While latent variable models have been proposed to learn linear and nonlinear spaces for Euclidean data, or geodesic subspaces for manifold data, no intrinsic latent variable model exists to learn nongeodesic subspaces for manifold data. This paper fills this gap and formulates a Riemannian variational autoencoder with an intrinsic generative model of manifold-valued data. We evaluate its performances on synthetic and real datasets by introducing the formalism of weighted Riemannian submanifolds. </br></br>

<a href='http://arxiv.org/pdf/1911.09449.pdf'>1911.09449</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -1.3639баллов, №928</br>
<b>Heuristic Black-box Adversarial Attacks on Video Recognition Models</b></br>
Authors: , Wei, Zhipeng, Chen, Jingjing, Wei, Xingxing, Jiang, Linxi, Chua, Tat-Seng, Zhou, Fengfeng, Jiang, Yu-Gang</br>
  We study the problem of attacking video recognition models in the black-box setting, where the model information is unknown and the adversary can only make queries to detect the predicted top-1 class and its probability. Compared with the <font color="#be00be">black-box attack</font> on images, attacking videos is more challenging as the computation cost for searching the adversarial perturbations on a video is much higher due to its high dimensionality. To overcome this challenge, we propose a heuristic black-box attack model that generates adversarial perturbations only on the selected frames and regions. More specifically, a heuristic-based algorithm is proposed to measure the importance of each frame in the video towards generating the adversarial examples. Based on the frames\' importance, the proposed algorithm heuristically searches a subset of frames where the generated adversarial example has strong <font color="blue">adversarial att</font>ack ability while keeps the perturbations lower than the given bound. Besides, to further boost the attack efficiency, we propose to generate the perturbations only on the salient regions of the selected frames. In this way, the generated perturbations are sparse in both temporal and spatial domains. Experimental results of attacking two mainstream video recognition methods on the UCF-101 dataset and the <font color="#be00be">HMDB-51</font> dataset demonstrate that the proposed heuristic black-box adversarial attack method can significantly reduce the computation cost and lead to more than 28\\% reduction in query numbers for the untargeted attack on both datasets. </br></br>

<a href='http://arxiv.org/pdf/1911.08805.pdf'>1911.08805</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -1.3707баллов, №929</br>
<b>Segmentation of Defective Skulls from CT Data for Tissue Modelling</b></br>
Authors: , Kodym, Old&#x159;ich, &#x160;pan&#x11b;l, Michal, Herout, Adam</br>
  In this work we present a method of automatic <font color="#be00be">segmentation</font> of defective skulls for custom cranial implant design and 3D printing purposes. Since such tissue models are usually required in <font color="blue">patient</font> cases with complex anatomical defects and variety of external objects present in the acquired data, most deep learning-based approaches fall short because it is not possible to create a sufficient training dataset that would encompass the spectrum of all possible structures. Because CNN segmentation experiments in this application domain have been so far limited to simple patch-based CNN architectures, we first show how the usage of the encoder-decoder architecture can substantially improve the segmentation accuracy. Then, we show how the number of segmentation artifacts, which usually require manual corrections, can be further reduced by adding a boundary term to CNN training and by globally optimizing the segmentation with graph-cut. Finally, we show that using the proposed method, 3D segmentation accurate enough for <font color="blue">clinic</font>al application can be achieved with 2D CNN architectures as well as their 3D counterparts. </br></br>

<a href='http://arxiv.org/pdf/1911.09559.pdf'>1911.09559</a> &nbsp&nbsp (stat:ML) &nbsp&nbsp -1.3943баллов, №930</br>
<b>Generalizing Information to the Evolution of Rational Belief</b></br>
Authors: , Duersch, Jed A., Catanach, Thomas A.</br>
  Information <font color="blue">theor</font>y provides a mathematical foundation to measure uncertainty in belief. Belief is represented by a probability distribution that captures our understanding of an outcome\'s plausibility. Information measures based on Shannon\'s concept of entropy include realization information, Kullback-Leibler divergence, Lindley\'s information in experiment, cross entropy, and mutual information.   We derive a general theory of information from first principles that accounts for evolving belief and recovers all of these measures. Rather than simply gauging uncertainty, information is understood in this theory to measure change in belief. We may then regard entropy as the information we expect to gain upon realization of a discrete latent random variable.   This theory of information is compatible with the <font color="blue">Bayes</font>ian paradigm in which rational belief is updated as evidence becomes available. Furthermore, this theory admits novel measures of information with well-defined properties, which we explore in both analysis and experiment. This view of information illuminates the study of machine learning by allowing us to quantify information captured by a predictive model and distinguish it from residual information contained in training data. We gain related insights regarding feature selection, <font color="#be00be">anomal</font>y detection, and novel Bayesian approaches. </br></br>

<a href='http://arxiv.org/pdf/1911.07328.pdf'>1911.07328</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -1.4033баллов, №931</br>
<b>The Potential of the Confluence of Theoretical and Algorithmic Modeling\n  in Music Recommendation</b></br>
Authors: , Bauer, Christine</br>
  The task of a<font color="#be00be"> music </font>recommender system is to predict what music item a particular user would like to listen to next. This position paper discusses the main challenges of the music preference prediction task: the lack of information on the many contextual factors influencing a user\'s music preferences in existing open datasets, the lack of clarity of what the right choice of music is and whether a right choice exists at all; the multitude of criteria (beyond accuracy) that have to be met for a &quot;good&quot; music item <font color="blue">recommendat</font>ion; and the need for explanations on relationships to identify (and potentially counteract) unwanted biases in recommendation approaches. The paper substantiates the position that the confluence of <font color="blue">theor</font>etical modeling (which seeks to explain behaviors) and algorithmic modeling (which seeks to predict behaviors) seems to be an effective avenue to take in computational modeling for music recommender systems. </br></br>

<a href='http://arxiv.org/pdf/1911.08121.pdf'>1911.08121</a> &nbsp&nbsp (cs:CV, stat:ML) &nbsp&nbsp -1.4144баллов, №932</br>
<b>Estimation of Orientation and Camera Parameters from Cryo-Electron\n  Microscopy Images with Variational Autoencoders and Generative Adversarial\n  Networks</b></br>
Authors: , Miolane, Nina, Poitevin, Fr&#xe9;d&#xe9;ric, Li, Yee-Ting, Holmes, Susan</br>
  Cryo-electron microscopy (cryo-EM) is capable of producing reconstructed 3D images of bio<font color="#be00be">molecule</font>s at near-atomic resolution. As such, it represents one of the most promising imaging techniques in structural biology. However, raw cryo-EM images are only highly corrupted - noisy and band-pass filtered - 2D projections of the target 3D biomolecules. Reconstructing the 3D molecular shape starts with the removal of image <font color="#be00be">outlier</font>s, the estimation of the orientation of the biomolecule that has produced the given 2D image, and the estimation of camera parameters to correct for intensity defects. Current techniques performing these tasks are often computationally expensive, while the dataset sizes keep growing. There is a need for next-generation algorithms that preserve accuracy while improving speed and scalability. In this paper, we combine variational autoencoders (VAEs) and generative adversarial networks (GANs) to learn a low-dimensional latent representation of cryo-EM images. We perform an exploratory analysis of the obtained latent space, that is shown to have a structure of &quot;orbits&quot;, in the sense of Lie group <font color="blue">theor</font>y, consistent with the acquisition procedure of cryo-EM images. This analysis leads us to design an estimation method for orientation and camera parameters of single-particle cryo-EM images, together with an outliers detection procedure. As such, it opens the door to geometric approaches for unsupervised estimations of orientations and camera parameters, making possible fast cryo-EM biomolecule reconstruction. </br></br>

<a href='http://arxiv.org/pdf/1911.08729.pdf'>1911.08729</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.4189баллов, №933</br>
<b>Response Transformation and Profit Decomposition for Revenue Uplift\n  Modeling</b></br>
Authors: , Gubela, Robin M., Lessmann, Stefan, Jaroszewicz, Szymon</br>
  Uplift models support decision-making in <font color="#be00be">market</font>ing campaign planning. Estimating the causal effect of a marketing treatment, an uplift model facilitates targeting communication to responsive <font color="#be00be">customer</font>s and efficient allocation of marketing budgets. Research into uplift models focuses on conversion models to maximize incremental sales. The paper introduces uplift modeling strategies for maximizing incremental revenues. If customers differ in their spending behavior, revenue maximization is a more plausible business objective compared to maximizing conversions. The proposed methodology entails a transformation of the prediction target, customer-level revenues, that facilitates implementing a causal uplift model using standard machine learning algorithms. The distribution of campaign revenues is typically zero-inflated because of many non-buyers. Remedies to this modeling challenge are incorporated in the proposed revenue uplift strategies in the form of two-stage models. Empirical experiments using <font color="#009600">real-world</font> <font color="#be00be">e-commerce</font> data confirm the merits of the proposed revenue uplift strategy over relevant alternatives including uplift models for conver-sion and recently developed causal machine learning algorithms. To quantify the degree to which improved targeting decisions raise return on marketing, the paper develops a decomposition of campaign profit. Applying the decomposition to a digital coupon targeting campaign, the paper provides evidence that revenue uplift modeling, as well as causal machine learning, can improve cam-paign profit substantially. </br></br>

<a href='http://arxiv.org/pdf/1911.06903.pdf'>1911.06903</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.4316баллов, №934</br>
<b>Query Complexity of Bayesian Private Learning</b></br>
Authors: , Xu, Kuang</br>
  We study the query complexity of <font color="blue">Bayes</font>ian <font color="#be00be">Private</font> Learning: a learner wishes to locate a random target within an interval by submitting queries, in the presence of an adversary who observes all of her queries but not the responses. How many queries are necessary and sufficient in order for the learner to accurately estimate the target, while simultaneously concealing the target from the adversary?   Our main result is a query complexity lower bound that is tight up to the first order. We show that if the learner wants to estimate the target within an error of $\\varepsilon$, while ensuring that no adversary estimator can achieve a constant additive error with probability greater than $1/L$, then the query complexity is on the order of $L\\log(1/\\varepsilon)$, as $\\varepsilon \\to 0$. Our result demonstrates that increased <font color="#be00be">privacy</font>, as captured by $L$, comes at the expense of a {multiplicative} increase in query complexity.   Our proof method builds on Fano\'s inequality and a family of proportional-sampling estimators. As an illustration of the method\'s wider applicability, we generalize the complexity lower bound to settings involving high-dimensional linear query learning and partial adversary observation. </br></br>

<a href='http://arxiv.org/pdf/1911.07936.pdf'>1911.07936</a> &nbsp&nbsp (cs:CV, cs:ML, stat:ML) &nbsp&nbsp -1.4586баллов, №935</br>
<b>Privacy Preserving Gaze Estimation using Synthetic Images via a\n  Randomized Encoding Based Framework</b></br>
Authors: , Bozkir, Efe, &#xdc;nal, Ali Burak, Akg&#xfc;n, Mete, Kasneci, Enkelejda, Pfeifer, Nico</br>
  Eye <font color="#be00be">tracking</font> is handled as one of the key technologies for applications which assess and evaluate human attention, behavior and biometrics, especially using gaze, pupillary and blink behaviors. One of the main challenges with regard to the social acceptance of eye-tracking technology is however the preserving of sensitive and personal information. To tackle this challenge, we employed a <font color="#be00be">privacy</font>-preserving framework based on randomized encoding to train a Support Vector <font color="#be00be">Regression</font> model on synthetic eye images <font color="#be00be">private</font>ly to estimate human gaze. During the computation, none of the parties learns about the data or the result that any other party has. Furthermore, the party that trains the model cannot reconstruct pupil, blink or visual scanpath. The experimental results showed that our privacy preserving framework is also capable of working in real-time, as accurate as a non-private version of it and could be extended to other eye-tracking related problems. </br></br>

<a href='http://arxiv.org/pdf/1911.07409.pdf'>1911.07409</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.4702баллов, №936</br>
<b>Online Learning and Matching for Resource Allocation Problems</b></br>
Authors: , Boskovic, Andrea, Chen, Qinyi, Kufel, Dominik, Zhou, Zijie</br>
  In order for an <font color="#be00be">e-commerce</font> platform to maximize its revenue, it must recommend <font color="#be00be">customer</font>s items they are most likely to purchase. However, the company often has business constraints on these items, such as the number of each item in stock. In this work, our goal is to recommend items to users as they arrive on a webpage sequentially, in an online manner, in order to maximize reward for a company, but also satisfy budget constraints. We first approach the simpler online problem in which the customers arrive as a stationary Poisson process, and present an integrated algorithm that performs online optimization and online learning together. We then make the model more complicated but more realistic, treating the arrival processes as non-stationary Poisson processes. To deal with heterogeneous customer arrivals, we propose a time <font color="#be00be">segmentation</font> algorithm that converts a non-stationary problem into a series of stationary problems. Experiments conducted on large-scale synthetic data demonstrate the effectiveness and efficiency of our proposed approaches on solving constrained resource allocation problems. </br></br>

<a href='http://arxiv.org/pdf/1911.09471.pdf'>1911.09471</a> &nbsp&nbsp (cs:AI, cs:ML, stat:ML) &nbsp&nbsp -1.4797баллов, №937</br>
<b>TrueLearn: A Family of Bayesian Algorithms to Match Lifelong Learners to\n  Open Educational Resources</b></br>
Authors: , Bulathwela, Sahan, Perez-Ortiz, Maria, Yilmaz, Emine, Shawe-Taylor, John</br>
  The recent advances in computer-assisted learning systems and the availability of open educational resources today promise a pathway to providing cost-efficient, high-quality education to large masses of learners. One of the most ambitious use cases of computer-assisted learning is to build a <font color="#00be00">lifelong</font> learning <font color="blue">recommendat</font>ion system. Unlike short-term courses, lifelong learning presents unique challenges, requiring sophisticated recommendation models that account for a wide range of factors such as background knowledge of learners or novelty of the material while effectively maintaining knowledge states of masses of learners for significantly longer periods of time (ideally, a lifetime). This work presents the foundations towards building a dynamic, scalable and transparent recommendation system for education, modelling learner\'s knowledge from implicit data in the form of engagement with open educational resources. We i) use a text ontology based on Wikipedia to automatically extract knowledge components of educational resources and, ii) propose a set of online <font color="blue">Bayes</font>ian strategies inspired by the well-known areas of item response <font color="blue">theor</font>y and knowledge tracing. Our proposal, TrueLearn, focuses on recommendations for which the learner has enough background knowledge (so they are able to understand and learn from the material), and the material has enough novelty that would help the learner improve their knowledge about the subject and keep them engaged. We further construct a large open educational video lectures dataset and test the performance of the proposed algorithms, which show clear promise towards building an effective educational recommendation system. </br></br>

<a href='http://arxiv.org/pdf/1911.08434.pdf'>1911.08434</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -1.4861баллов, №938</br>
<b>LNDb: A Lung Nodule Database on Computed Tomography</b></br>
Authors: , Pedrosa, Jo&#xe3;o, Aresta, Guilherme, Ferreira, Carlos, Rodrigues, M&#xe1;rcio, Leit&#xe3;o, Patr&#xed;cia, Carvalho, Andr&#xe9; Silva, Rebelo, Jo&#xe3;o, Negr&#xe3;o, Eduardo, Ramos, Isabel, Cunha, Ant&#xf3;nio, Campilho, Aur&#xe9;lio</br>
  Lung <font color="#be00be">cancer</font> is the deadliest type of cancer worldwide and late detection is the major factor for the low survival rate of <font color="blue">patient</font>s. Low dose computed <font color="#be00be">tomography</font> has been suggested as a potential screening tool but manual screening is costly, time-consuming and prone to variability. This has fueled the development of automatic methods for the detection, <font color="#be00be">segmentation</font> and characterisation of pulmonary nodules but its application to <font color="blue">clinic</font>al routine is challenging. In this study, a new database for the development and testing of pulmonary nodule computer-aided strategies is presented which intends to complement current databases by giving additional focus to radiologist variability and local clinical reality. <font color="red">State-of-the-art</font> nodule detection, segmentation and characterization methods are tested and compared to manual annotations as well as collaborative strategies combining multiple radiologists and radiologists and computer-aided systems. It is shown that state-of-the-art methodologies can determine a patient\'s follow-up <font color="blue">recommendat</font>ion as accurately as a radiologist, though the nodule detection method used shows decreased performance in this database. </br></br>

<a href='http://arxiv.org/pdf/1911.06959.pdf'>1911.06959</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.5153баллов, №939</br>
<b>Learning Behavioral Representations from Wearable Sensors</b></br>
Authors: , Tavabi, Nazgol, Hosseinmardi, Homa, Villatte, Jennifer L., Abeliuk, Andr&#xe9;s, Narayanan, Shrikanth, Ferrara, Emilio, Lerman, Kristina</br>
  The ubiquity of<font color="#960096"> mobile </font>devices and wearable sensors offers unprecedented opportunities for continuous collection of multimodal physiological data. Such data enables temporal characterization of an individual\'s behaviors, which can provide unique insights into her physical and psychological health. Understanding the relation between different behaviors/activities and personality traits such as stress or work performance can help build strategies to improve the work environment. Especially in workplaces like hospitals where many employees are overworked, having such policies improves the quality of <font color="blue">patient</font> care by prioritizing mental and physical health of their caregivers. One challenge in analyzing physiological data is extracting the underlying behavioral states from the temporal sensor signals and <font color="#be00be">interpret</font>ing them. Here, we use a non-parametric <font color="blue">Bayes</font>ian approach, to model multivariate sensor data from multiple people and discover dynamic behaviors they share. We apply this method to data collected from sensors worn by a population of workers in a large urban hospital, capturing their physiological signals, such as breathing and heart rate, and activity patterns. We show that the learned states capture behavioral differences within the population that can help cluster participants into meaningful groups and better predict their cognitive and affective states. This method offers a practical way to learn compact behavioral representations from dynamic multivariate sensor signals and provide insights into the data. </br></br>

<a href='http://arxiv.org/pdf/1911.07368.pdf'>1911.07368</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.5189баллов, №940</br>
<b>Predicting colorectal polyp recurrence using time-to-event analysis of\n  medical records</b></br>
Authors: , Harrington, Lia X., Wei, Jason W., Suriawinata, Arief A., Mackenzie, Todd A., Hassanpour, Saeed</br>
  Identifying <font color="blue">patient</font> characteristics that influence the rate of colorectal polyp recurrence can provide important insights into which patients are at higher risk for recurrence. We used natural language processing to extract polyp morphological characteristics from 953 polyp-presenting patients\' electronic <font color="blue">medic</font>al records. We used subsequent colonoscopy reports to examine how the time to polyp recurrence (731 patients experienced recurrence) is influenced by these characteristics as well as anthropometric features using Kaplan-Meier curves, Cox proportional hazards modeling, and random survival forest models. We found that the rate of recurrence differed significantly by polyp size, number, and location and patient smoking status. Additionally, right-sided colon polyps increased recurrence risk by 30% compared to left-sided polyps. History of tobacco use increased polyp recurrence risk by 20% compared to never-users. A random survival forest model showed an AUC of 0.65 and identified several other predictive variables, which can inform development of personalized polyp <font color="#be00be">surveillance</font> plans. </br></br>

<a href='http://arxiv.org/pdf/1911.08163.pdf'>1911.08163</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -1.5603баллов, №941</br>
<b>Projection-to-Projection Translation for Hybrid X-ray and Magnetic\n  Resonance Imaging</b></br>
Authors: , Stimpel, Bernhard, Syben, Christopher, W&#xfc;rfl, Tobias, Breininger, Katharina, Hoelter, Philipp, D&#xf6;rfler, Arnd, Maier, Andreas</br>
  Hybrid X-ray and <font color="blue">magnetic resonance</font> (MR) imaging promises large potential in interventional <font color="blue">medic</font>al imaging applications due to the broad variety of contrast of<font color="#be00be"> MRI </font>combined with fast imaging of X-ray-based modalities. To fully utilize the potential of the vast amount of existing image enhancement techniques, the corresponding information from both modalities must be present in the same domain. For image-guided interventional procedures, X-ray fluoroscopy has proven to be the modality of choice. Synthesizing one modality from another in this case is an ill-posed problem due to ambiguous signal and overlapping structures in projective geometry. To take on these challenges, we present a learning-based solution to MR to X-ray projection-to-projection translation. We propose an image generator network that focuses on high representation capacity in higher resolution layers to allow for accurate synthesis of fine details in the projection images. Additionally, a weighting scheme in the loss computation that favors high-frequency structures is proposed to focus on the important details and contours in projection imaging. The proposed extensions prove valuable in generating X-ray projection images with natural appearance. Our approach achieves a deviation from the ground truth of only $6$% and structural similarity measure of $0.913\\,\\pm\\,0.005$. In particular the high frequency weighting assists in generating projection images with sharp appearance and reduces erroneously synthesized fine details. </br></br>

<a href='http://arxiv.org/pdf/1911.08264.pdf'>1911.08264</a> &nbsp&nbsp (cs:NE) &nbsp&nbsp -1.5688баллов, №942</br>
<b>Visualization approach to assess the robustness of neural networks for\n  medical image classification</b></br>
Authors: , Sutre, Elina Thibeau, Colliot, Olivier, Dormont, Didier, Burgos, Ninon</br>
  The use of neural networks for <font color="blue">diagnos</font>is classification is becoming more and more prevalent in the <font color="blue">medic</font>al imaging community. However, deep learning method outputs remain hard to explain. Another difficulty is to choose among the large number of techniques developed to analyze how networks learn, as all present different limitations. In this paper, we extended the framework of Fong and Vedaldi [IEEE International Conference on Computer Vision (ICCV), 2017] to visualize the training of convolutional neural networks (CNNs) on 3D quantitative neuroimaging data. Our application focuses on the detection of Alzheimer\'s <font color="blue">diseas</font>e with gray matter probability maps extracted from structural MRI. We first assessed the robustness of the visualization method by studying the coherence of the longitudinal patterns and regions identified by the network. We then studied the stability of the CNN training by computing visualization-based similarity indexes between different re-runs of the CNN. We demonstrated that the areas identified by the CNN were consistent with what is known of Alzheimer\'s disease and that the visualization approach extract coherent longitudinal patterns. We also showed that the CNN training is not stable and that the areas identified mainly depend on the initialization and the training process. This issue may exist in many other medical studies using deep learning methods on datasets in which the number of samples is too small and the data dimension is high. This means that it may not be possible to rely on deep learning to detect stable regions of interest in this field yet. </br></br>

<a href='http://arxiv.org/pdf/1911.09355.pdf'>1911.09355</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.5848баллов, №943</br>
<b>A Probabilistic Approach for Discovering Daily Human Mobility Patterns\n  with Mobile Data</b></br>
Authors: , Qian, Weizhu, Lauri, Fabrice, Gechter, Franck</br>
  Discovering human mobility patterns with geo-location data collected from smartphone users has been a hot research topic in recent years. In this paper, we attempt to discover daily<font color="#960096"> mobile </font>patterns based on GPS data. We view this problem from a probabilistic perspective in order to explore more information from the original GPS data compared to other conventional methods. A non-parameter <font color="blue">Bayes</font>ian modeling method, Infinite <font color="blue">Gaussi</font>an Mixture Model, is used to estimate the probability density for the daily mobility. Then, we use Kullback-Leibler divergence as the metrics to measure the similarity of different probability distributions. And combining Infinite Gaussian Mixture Model and Kullback-Leibler divergence, we derived an automatic <font color="#be00be">clustering</font> algorithm to discover mobility patterns for each individual user without setting the number of clusters in advance. In the experiments, the effectiveness of our method is validated on the real user data collected from different users. The results show that the IGMM-based algorithm <font color="#00be00">outperform</font>s the GMM-based algorithm. We also test our methods on the dataset with different lengths to discover the minimum data length for discovering mobility patterns. </br></br>

<a href='http://arxiv.org/pdf/1911.07293.pdf'>1911.07293</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.5963баллов, №944</br>
<b>Collaborative Unsupervised Domain Adaptation for Medical Image Diagnosis</b></br>
Authors: , Zhang, Yifan, Wei, Ying, Zhao, Peilin, Niu, Shuaicheng, Wu, Qingyao, Tan, Mingkui, Huang, Junzhou</br>
  Deep learning based <font color="blue">medic</font>al image <font color="blue">diagnos</font>is has shown great potential in <font color="blue">clinic</font>al medicine. However, it often suffers two major difficulties in practice: 1) only limited labeled samples are available due to expensive annotation costs over medical images; 2) labeled images may contain considerable label noises (e.g., mislabeling labels) due to diagnostic difficulties. In this paper, we seek to exploit rich labeled data from relevant domains to help the learning in the target task with unsupervised domain adaptation (UDA). Unlike most existing UDA methods which rely on clean labeled data or assume samples are equally transferable, we propose a novel Collaborative Unsupervised Domain Adaptation algorithm to conduct transferability-aware domain adaptation and conquer label noise in a cooperative way. Promising empirical results verify the superiority of the proposed method. </br></br>

<a href='http://arxiv.org/pdf/1911.06913.pdf'>1911.06913</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -1.6065баллов, №945</br>
<b>Granular Motor State Monitoring of Free Living Parkinson\'s Disease\n  Patients via Deep Learning</b></br>
Authors: , Yuksel, Kamer A., Goschenhofer, Jann, Varma, Hridya V., Fietzek, Urban, Pfister, Franz M. J.</br>
  Parkinson\'s <font color="blue">diseas</font>e (PD) is the second most common neurodegenerative disease worldwide and affects around 1% of the (60+ years old) elderly population in industrial nations. More than 80% of PD <font color="blue">patient</font>s suffer from motor symptoms, which could be well addressed if a personalized <font color="blue">medic</font>ation schedule and dosage could be administered to them. However, such personalized medication schedule requires a continuous, objective and precise measurement of motor symptoms experienced by the patients during their regular daily activities. In this work, we propose the use of a wrist-worn smart-watch, which is equipped with 3D motion sensors, for estimating the motor fluctuation severity of PD patients in a free-living environment. We introduce a novel network architecture, a post-training scheme and a custom loss function that accounts for label noise to improve the results of our previous work in this domain and to establish a novel benchmark for nine-level PD motor state estimation. </br></br>

<a href='http://arxiv.org/pdf/1911.06944.pdf'>1911.06944</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.6482баллов, №946</br>
<b>$DC^2$: A Divide-and-conquer Algorithm for Large-scale Kernel Learning\n  with Application to Clustering</b></br>
Authors: , Wang, Ke Alexander, Bian, Xinran, Liu, Pan, Yan, Donghui</br>
  Divide-and-conquer is a general strategy to deal with large scale problems. It is typically applied to generate ensemble instances, which potentially limits the problem size it can handle. Additionally, the data are often divided by random sampling which may be suboptimal. To address these concerns, we propose the $DC^2$ algorithm. Instead of ensemble instances, we produce structure-preserving signature pieces to be assembled and conquered. $DC^2$ achieves the efficiency of sampling-based large scale <font color="blue">kernel</font> methods while enabling parallel multicore or clustered computation. The data partition and subsequent compression are unified by recursive random projections. Empirically dividing the data by random projections induces smaller mean squared approximation errors than conventional random sampling. The power of $DC^2$ is demonstrated by our <font color="#be00be">clustering</font> algorithm $rpfCluster^+$, which is as accurate as some fastest approximate spectral clustering algorithms while maintaining a running time close to that of <font color="blue">K-mean</font>s clustering. Analysis on $DC^2$ when applied to spectral clustering shows that the loss in clustering accuracy due to data division and reduction is upper bounded by the data approximation error which would vanish with recursive random projections. Due to its easy implementation and flexibility, we expect $DC^2$ to be applicable to general large scale learning problems. </br></br>

<a href='http://arxiv.org/pdf/1911.09288.pdf'>1911.09288</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -1.6773баллов, №947</br>
<b>Controversial stimuli: pitting neural networks against each other as\n  models of human recognition</b></br>
Authors: , Golan, Tal, Raju, Prashant C., Kriegeskorte, Nikolaus</br>
  Distinct scientific <font color="blue">theor</font>ies can make similar predictions. To adjudicate between theories, we must design experiments for which the theories make distinct predictions. Here we consider the problem of comparing deep neural networks as models of human visual recognition. To efficiently determine which models better explain human responses, we synthesize controversial stimuli: images for which different models produce distinct responses. We tested nine different models, which employed different architectures and recognition algorithms, including discriminative and generative models, all trained to recognize handwritten digits (from the MNIST set of digit images). We synthesized controversial stimuli to maximize the disagreement among the models. Human subjects viewed hundreds of these stimuli and judged the probability of presence of each digit in each image. We quantified how accurately each model predicted the human judgements. We found that the generative models (which learn the distribution of images for each class) better predicted the human judgments than the discriminative models (which learn to directly map from images to labels). The best performing model was the generative Analysis-by-Synthesis model (based on variational autoencoders). However, a simpler generative model (based on <font color="blue">Gaussi</font>an-<font color="blue">kernel</font>-density estimation) also performed better than each of the discriminative models. None of the candidate models fully explained the human responses. We discuss the advantages and limitations of controversial stimuli as an experimental paradigm and how they generalize and improve on adversarial examples as probes of discrepancies between models and human perception. </br></br>

<a href='http://arxiv.org/pdf/1911.09008.pdf'>1911.09008</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.8325баллов, №948</br>
<b>Learning Embeddings from Cancer Mutation Sets for Classification Tasks</b></br>
Authors: , Dubourg-Felonneau, Geoffroy, Kussad, Yasmeen, Kirkham, Dominic, Cassidy, John W, Patel, Nirmesh, Clifford, Harry W</br>
  Analysis of somatic mutation profiles from <font color="#be00be">cancer</font> <font color="blue">patient</font>s is essential in the development of cancer research. However, the low frequency of most mutations and the varying rates of mutations across patients makes the data extremely challenging to statistically analyze as well as difficult to use in classification problems, for <font color="#be00be">clustering</font>, visualization or for learning useful information. Thus, the creation of low dimensional representations of somatic mutation profiles that hold useful information about the DNA of cancer cells will facilitate the use of such data in applications that will progress precision <font color="blue">medic</font>ine. In this paper, we talk about the open problem of learning from somatic mutations, and present Flatsomatic: a solution that utilizes variational autoencoders (VAEs) to create latent representations of somatic profiles. The work done in this paper shows great potential for this method, with the VAE embeddings performing better than PCA for a clustering task, and performing equally well to the raw high dimensional data for a classification task. We believe the methods presented herein can be of great value in future research and in bringing data-driven models into precision oncology. </br></br>

<a href='http://arxiv.org/pdf/1911.08748.pdf'>1911.08748</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -1.8593баллов, №949</br>
<b>Yottixel -- An Image Search Engine for Large Archives of Histopathology\n  Whole Slide Images</b></br>
Authors: , Kalra, S., Choi, C., Shah, S., Pantanowitz, L., Tizhoosh, H. R.</br>
  With the emergence of digital <font color="blue">patholog</font>y, searching for similar images in large archives has gained considerable attention. Image retrieval can provide pathologists with unprecedented access to the evidence embodied in already <font color="blue">diagnos</font>ed and treated cases from the past. This paper proposes a search engine specialized for digital pathology, called Yottixel, a portmanteau for &quot;one yotta pixel,&quot; alluding to the big-data nature of histopathology images. The most impressive characteristic of Yottixel is its ability to represent whole slide images (WSIs) in a compact manner. Yottixel can perform millions of searches in real-time with a high search accuracy and low storage profile. Yottixel uses an intelligent indexing algorithm capable of representing WSIs with a mosaic of patches by converting them into a small number of methodically extracted barcodes, called &quot;Bunch of Barcodes&quot; (BoB), the most prominent performance enabler of Yottixel. The performance of the prototype platform is qualitatively tested using 300 WSIs from the University of Pittsburgh <font color="blue">Medic</font>al Center (UPMC) and 2,020 WSIs from The <font color="#be00be">Cancer</font> Genome Atlas Program (TCGA) provided by the National Cancer Institute. Both datasets amount to more than 4,000,000 patches of 1000x1000 pixels. We report three sets of experiments that show that Yottixel can accurately retrieve organs and malignancies, and its semantic ordering shows good agreement with the subjective evaluation of human observers. </br></br>

<a href='http://arxiv.org/pdf/1911.07116.pdf'>1911.07116</a> &nbsp&nbsp (cs:ML, cs:AI) &nbsp&nbsp -1.8969баллов, №950</br>
<b>Robust Anomaly Detection and Backdoor Attack Detection Via Differential\n  Privacy</b></br>
Authors: , Du, Min, Jia, Ruoxi, Song, Dawn</br>
  <font color="#be00be">Outlier</font> detection and novelty detection are two important topics for <font color="#be00be">anomal</font>y detection. Suppose the majority of a dataset are drawn from a certain distribution, outlier detection and novelty detection both aim to detect data samples that do not fit the distribution. Outliers refer to data samples within this dataset, while novelties refer to new samples. In the meantime, backdoor poisoning attacks for machine learning models are achieved through injecting poisoning samples into the training dataset, which could be regarded as &quot;outliers&quot; that are intentionally added by attackers. Differential <font color="#be00be">privacy</font> has been proposed to avoid leaking any individual\'s information, when aggregated analysis is performed on a given dataset. It is typically achieved by adding random noise, either directly to the input dataset, or to intermediate results of the aggregation mechanism. In this paper, we demonstrate that applying differential privacy can improve the utility of outlier detection and novelty detection, with an extension to detect poisoning samples in backdoor attacks. We first present a <font color="blue">theor</font>etical analysis on how differential privacy helps with the detection, and then conduct extensive experiments to validate the effectiveness of differential privacy in improving outlier detection, novelty detection, and backdoor attack detection. </br></br>

<a href='http://arxiv.org/pdf/1910.07842.pdf'>1910.07842</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.9472баллов, №951</br>
<b>Kernel density estimation based sampling for imbalanced class\n  distribution</b></br>
Authors: , Kamalov, Firuz</br>
  Imbalanced response variable distribution is a common occurrence in data science. In fields such as fraud detection, <font color="blue">medic</font>al <font color="blue">diagnos</font>tics, system intrusion detection and many others where abnormal behavior is rarely observed the data under study often features disproportionate target class distribution. One common way to combat class imbalance is through resampling the minority class to achieve a more balanced distribution. In this paper, we investigate the performance of the sampling method based on <font color="blue">kernel</font> density estimation (KDE). We believe that KDE offers a more natural way of generating new instances of minority class that is less prone to overfitting than other standard sampling techniques. It is based on a well established <font color="blue">theor</font>y of nonparametric statistical estimation. Numerical experiments show that KDE can <font color="#00be00">outperform</font> other sampling techniques on a range of real life datasets as measured by F1-score and G-mean. The results remain consistent across a number of classification algorithms used in the experiments. Furthermore, the proposed method outperforms the benchmark methods irregardless of the class distribution ratio. We conclude, based on the solid theoretical foundation and strong experimental results, that the proposed method would be a valuable tool in problems involving imbalanced class distribution. </br></br>

<a href='http://arxiv.org/pdf/1911.08551.pdf'>1911.08551</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -1.9663баллов, №952</br>
<b>Prediction Focused Topic Models for Electronic Health Records</b></br>
Authors: , Ren, Jason, Kunes, Russell, Doshi-Velez, Finale</br>
  Electronic Health Record (EHR) data can be represented as discrete counts over a high dimensional set of possible procedures, <font color="blue">diagnos</font>es, and <font color="blue">medic</font>ations. Supervised topic models present an attractive option for incorporating EHR data as features into a prediction problem: given a <font color="blue">patient</font>\'s record, we estimate a set of latent factors that are predictive of the response variable. However, existing methods for supervised topic modeling struggle to balance prediction quality and coherence of the latent factors. We introduce a novel approach, the prediction-focused topic model, that uses the supervisory signal to retain only features that improve, or do not hinder, prediction performance. By removing features with irrelevant signal, the topic model is able to learn task-relevant, <font color="#be00be">interpret</font>able topics. We demonstrate on a EHR dataset and a movie review dataset that compared to existing approaches, prediction-focused topic models are able to learn much more coherent topics while maintaining <font color="#960096">competitive</font> predictions. </br></br>

<a href='http://arxiv.org/pdf/1911.08105.pdf'>1911.08105</a> &nbsp&nbsp (cs:AI, cs:CV, cs:ML) &nbsp&nbsp -2.0162баллов, №953</br>
<b>Three-dimensional Generative Adversarial Nets for Unsupervised Metal\n  Artifact Reduction</b></br>
Authors: , Nakao, Megumi, Imanishi, Keiho, Ueda, Nobuhiro, Imai, Yuichiro, Kirita, Tadaaki, Matsuda, Tetsuya</br>
  The reduction of metal artifacts in computed <font color="#be00be">tomography</font> (CT) images, specifically for strong artifacts generated from multiple metal objects, is a challenging issue in <font color="blue">medic</font>al imaging research. Although there have been some studies on supervised metal artifact reduction through the learning of synthesized artifacts, it is difficult for simulated artifacts to cover the complexity of the real physical phenomena that may be observed in X-ray propagation. In this paper, we introduce metal artifact reduction methods based on an unsupervised volume-to-volume translation learned from <font color="blue">clinic</font>al CT images. We construct three-dimensional adversarial nets with a regularized loss function designed for metal artifacts from multiple dental fillings. The results of experiments using 915 CT volumes from real <font color="blue">patient</font>s demonstrate that the proposed framework has an outstanding capacity to reduce strong artifacts and to recover underlying missing voxels, while preserving the anatomical features of soft tissues and tooth structures from the original images. </br></br>

<a href='http://arxiv.org/pdf/1911.08136.pdf'>1911.08136</a> &nbsp&nbsp (cs:ML) &nbsp&nbsp -2.0673баллов, №954</br>
<b>Enhancing the Extraction of Interpretable Information for Ischemic\n  Stroke Imaging from Deep Neural Networks</b></br>
Authors: , Tjoa, Erico, Heng, Guo, Yuhao, Lu, Guan, Cuntai</br>
  When artificial intelligence is used in the <font color="blue">medic</font>al sector, <font color="#be00be">interpret</font>ability is a crucial factor to consider. <font color="blue">Diagnos</font>is based on machine decision produced by a black-box neural network, sometimes lacking clear rationale, is unlikely to be <font color="blue">clinic</font>ally adopted for fear of potentially dire consequences arising from unexplained misdiagnosis. In this work, we implement Layer-wise Relevance Propagation (LRP), a visual interpretability method applied on 3D U-Net for lesion <font color="#be00be">segmentation</font> using the small dataset of multi-modal images provided by ISLES 2017 competition. We demonstrate that LRP modifications could provide more sensible visual explanations to an otherwise highly noise-skewed output and quantify them using inclusivity coefficients. We also link amplitude of modified signals to useful information content. High amplitude signals appear to constitute the noise that undermines the interpretability capacity of LRP. Furthermore, mathematical framework for possible analysis of function approximation is developed by analogy. </br></br>

<a href='http://arxiv.org/pdf/1911.09006.pdf'>1911.09006</a> &nbsp&nbsp (stat:ML, cs:ML) &nbsp&nbsp -2.0817баллов, №955</br>
<b>Additive Bayesian Network Modelling with the R Package abn</b></br>
Authors: , Kratzer, Gilles, Lewis, Fraser Iain, Comin, Arianna, Pittavino, Marta, Furrer, Reinhard</br>
  The R package abn is designed to fit additive <font color="blue">Bayes</font>ian models to observational datasets. It contains routines to score Bayesian networks based on Bayesian or information <font color="blue">theor</font>etic formulations of generalized linear models. It is equipped with exact search and greedy search algorithms to select the best network. It supports a possible blend of continuous, discrete and count data and input of prior knowledge at a structural level. The Bayesian implementation supports random effects to control for one-layer <font color="#be00be">clustering</font>. In this paper, we give an overview of the methodology and illustrate the package\'s functionalities using a veterinary dataset about respiratory <font color="blue">diseas</font>es in commercial swine production. </br></br>

<a href='http://arxiv.org/pdf/1911.08736.pdf'>1911.08736</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -2.1560баллов, №956</br>
<b>Pan-Cancer Diagnostic Consensus Through Searching Archival\n  Histopathology Images Using Artificial Intelligence</b></br>
Authors: , Kalra, Shivam, Tizhoosh, H. R., Shah, Sultaan, Choi, Charles, Damaskinos, Savvas, Safarpoor, Amir, Shafiei, Sobhan, Babaie, Morteza, Diamandis, Phedias, Campbell, Clinton JV, Pantanowitz, Liron</br>
  The emergence of digital <font color="blue">patholog</font>y has opened new horizons for histopathology and cytology. Artificial-intelligence algorithms are able to operate on digitized slides to assist pathologists with <font color="blue">diagnos</font>tic tasks. Whereas machine learning involving classification and <font color="#be00be">segmentation</font> methods have obvious benefits for image analysis in pathology, image search represents a fundamental shift in computational pathology. Matching the pathology of new <font color="blue">patient</font>s with already diagnosed and curated cases offers pathologist a novel approach to improve diagnostic accuracy through visual inspection of similar cases and computational majority vote for consensus building. In this study, we report the results from searching the largest public repository (The <font color="#be00be">Cancer</font> Genome Atlas [TCGA] program by National Cancer Institute, USA) of whole slide images from almost 11,000 patients depicting different types of malignancies. For the first time, we successfully indexed and searched almost 30,000 high-resolution digitized slides constituting 16 terabytes of data comprised of 20 million 1000x1000 pixels image patches. The TCGA image database covers 25 anatomic sites and contains 32 cancer subtypes. High-performance storage and GPU power were employed for experimentation. The results were assessed with conservative &quot;majority voting&quot; to build consensus for subtype diagnosis through vertical search and demonstrated high accuracy values for both frozen sections slides (e.g., bladder urothelial carcinoma 93%, kidney renal clear cell carcinoma 97%, and ovarian serous cystadenocarcinoma 99%) and permanent histopathology slides (e.g., prostate adenocarcinoma 98%, skin cutaneous melanoma 99%, and thymoma 100%). The key finding of this validation study was that computational consensus appears to be possible for rendering diagnoses if a sufficiently large number of searchable cases are available for each cancer subtype. </br></br>

<a href='http://arxiv.org/pdf/1911.09339.pdf'>1911.09339</a> &nbsp&nbsp (cs:CL) &nbsp&nbsp -2.1748баллов, №957</br>
<b>Emotion Recognition for Vietnamese Social Media Text</b></br>
Authors: , Ho, Vong Anh, Nguyen, Duong Huynh-Cong, Nguyen, Danh Hoang, Pham, Linh Thi-Van, Nguyen, Duc-Vu, Van Nguyen, Kiet, Nguyen, Ngan Luu-Thuy</br>
  <font color="#be00be">Emotion</font> recognition or emotion prediction is a higher approach or a special case of <font color="#be00be">sentiment</font> analysis. In this task, the result is not produced in terms of either polarity: positive or negative or in the form of rating (from 1 to 5) but of a more detailed level of analysis in which the results are depicted in more expressions like sadness, enjoyment, anger, disgust, fear, and surprise. Emotion recognition plays a critical role in measuring the brand value of a product by recognizing specific emotions of <font color="#be00be">customer</font>s\' comments. In this study, we have achieved two targets. First and foremost, we built a standard <font color="#be00be">Vietnamese</font> Social Media Emotion Corpus (UIT-VSMEC) with exactly 6,927 emotion-annotated sentences, contributing to emotion recognition research in Vietnamese which is a <font color="#be00be">low-resource</font> language in natural language processing (NLP). Secondly, we assessed and measured machine learning and deep neural network models on our UIT-VSMEC corpus. As a result, the CNN model achieved the highest performance with the weighted F1-score of 59.74%. Our corpus is available at our research website. </br></br>

<a href='http://arxiv.org/pdf/1911.05439.pdf'>1911.05439</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -2.2389баллов, №958</br>
<b>Statistical Deformation Reconstruction Using Multi-organ Shape Features\n  for Pancreatic Cancer Localization</b></br>
Authors: , Nakao, Megumi, Nakamura, Mitsuhiro, Mizowaki, Takashi, Matsuda, Tetsuya</br>
  Respiratory motion and the associated deformations of abdominal organs and tumors are essential information in <font color="blue">clinic</font>al applications. However, inter- and intra-<font color="blue">patient</font> multi-organ deformations are complex and have not been statistically formulated, whereas single organ deformations have been widely studied. In this paper, we introduce a multi-organ deformation library and its application to deformation reconstruction based on the shape features of multiple abdominal organs. Statistical multi-organ motion/deformation models of the stomach, liver, left and right kidneys, and duodenum were generated by shape matching their region labels defined on four-dimensional computed <font color="#be00be">tomography</font> images. A total of 250 volumes were measured from 25 pancreatic <font color="#be00be">cancer</font> patients. This paper also proposes a per-region-based deformation learning using the reproducing <font color="blue">kernel</font> to predict the displacement of pancreatic cancer for adaptive radiotherapy. The experimental results show that the proposed concept estimates deformations better than general per-patient-based learning models and achieves a clinically acceptable estimation error with a mean distance of 1.2 $\\pm$ 0.7 mm and a Hausdorff distance of 4.2 $\\pm$ 2.3 mm throughout the respiratory motion. </br></br>

<a href='http://arxiv.org/pdf/1911.09332.pdf'>1911.09332</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -2.3491баллов, №959</br>
<b>Heart Segmentation From MRI Scans Using Convolutional Neural Network</b></br>
Authors: , Ibrahim, Shakeel Muhammad, Ibrahim, Muhammad Sohail, Usman, Muhammad, Naseem, Imran, Moinuddin, Muhammad</br>
  Heart is one of the vital organs of human body. A minor dysfunction of heart even for a short time interval can be fatal, therefore, efficient monitoring of its physiological state is essential for the <font color="blue">patient</font>s with cardiovascular <font color="blue">diseas</font>es. In the recent past, various computer assisted <font color="blue">medic</font>al imaging systems have been proposed for the <font color="#be00be">segmentation</font> of the organ of interest. However, for the segmentation of heart using MRI, only few methods have been proposed each with its own merits and demerits. For further advancement in this area of research, we analyze automated heart segmentation methods for <font color="blue">magnetic resonance</font> images. The analysis are based on deep learning methods that processes a full MR scan in a slice by slice fashion to predict desired mask for heart region. We design two encoder decoder type fully convolutional neural network models </br></br>

<a href='http://arxiv.org/pdf/1911.07088.pdf'>1911.07088</a> &nbsp&nbsp (cs:CV) &nbsp&nbsp -2.3561баллов, №960</br>
<b>Liver Steatosis Segmentation with Deep Learning Methods</b></br>
Authors: , Guo, Xiaoyuan, Wang, Fusheng, Teodorou, George, Farris, Alton B., Kong, Jun</br>
  Liver steatosis is known as the abnormal accumulation of lipids within cells. An accurate quantification of steatosis area within the liver histo<font color="blue">patholog</font>ical microscopy images plays an important role in liver <font color="blue">diseas</font>e <font color="blue">diagnos</font>is and trans-plantation assessment. Such a quantification analysis often requires a precise steatosis <font color="#be00be">segmentation</font> that is challenging due to abundant presence of highly overlapped steatosis droplets. In this paper, a deep learning model Mask-RCNN is used to segment the steatosis droplets in clumps. Extended from Faster R-CNN, Mask-RCNN can predict object masks in addition to bounding box detection. With transfer learning, the resulting model is able to segment overlapped steatosis regions at 75.87% by Average Precision, 60.66% by Recall,65.88% by F1-score, and 76.97% by Jaccard index, promising to support liver disease diagnosis and allograft rejection prediction in future <font color="blue">clinic</font>al practice. </br></br>

<a href='http://arxiv.org/pdf/1911.07015.pdf'>1911.07015</a> &nbsp&nbsp (cs:ML, stat:ML) &nbsp&nbsp -2.6402баллов, №961</br>
<b>Suspicion-Free Adversarial Attacks on Clustering Algorithms</b></br>
Authors: , Chhabra, Anshuman, Roy, Abhishek, Mohapatra, Prasant</br>
  <font color="#be00be">Clustering</font> algorithms are used in a large number of applications and play an important role in modern machine learning-- yet, <font color="blue">adversarial att</font>acks on clustering algorithms seem to be broadly overlooked unlike supervised learning. In this paper, we seek to bridge this gap by proposing a black-box adversarial attack for clustering models for linearly separable clusters. Our attack works by perturbing a single sample close to the decision boundary, which leads to the misclustering of multiple unperturbed samples, named spill-over adversarial samples. We <font color="blue">theor</font>etically show the existence of such adversarial samples for the <font color="blue">K-Mean</font>s clustering. Our attack is especially strong as (1) we ensure the perturbed sample is not an <font color="#be00be">outlier</font>, hence not detectable, and (2) the exact metric used for clustering is not known to the attacker. We theoretically justify that the attack can indeed be successful without the knowledge of the true metric. We conclude by providing empirical results on a number of datasets, and clustering algorithms. To the best of our knowledge, this is the first work that generates spill-over adversarial samples without the knowledge of the true metric ensuring that the perturbed sample is not an outlier, and theoretically proves the above. </br></br>

<a href='http://arxiv.org/pdf/1911.07515.pdf'>1911.07515</a> &nbsp&nbsp (cs:CV, cs:ML) &nbsp&nbsp -2.7863баллов, №962</br>
<b>Automated Human Claustrum Segmentation using Deep Learning Technologies</b></br>
Authors: , Albishri, Ahmed Awad, Shah, Syed Jawad Hussain, Schmiedler, Anthony, Kang, Seung Suk, Lee, Yugyung</br>
  In recent years, Deep Learning (DL) has shown promising results in conducting AI tasks such as computer vision and image <font color="#be00be">segmentation</font>. Specifically, Convolutional Neural Network (CNN) models in DL have been applied to prevention,detection, and <font color="blue">diagnos</font>is in predictive <font color="blue">medic</font>ine. Image segmentation plays a significant role in <font color="blue">diseas</font>e detection and prevention.However, there are enormous challenges in performing DL-based automatic segmentation due to the nature of medical images such as heterogeneous modalities and formats, insufficient labeled training data, and the high-class imbalance in the labeled data. Furthermore, automating segmentation of medical images,like <font color="blue">magnetic resonance</font> images (MRI), becomes a challenging task. The need for automated segmentation or annotation is what motivates our work. In this paper, we propose a fully automated approach that aims to segment the human claustrum for analytical purposes. We applied a U-Net CNN model to segment the claustrum (Cl) from a<font color="#be00be"> MRI </font>dataset. With this approach, we have achieved an average Dice per case score of 0.72 for Cl segmentation, with K=5 for cross-validation. The expert in the medical domain also evaluates these results. </br></br>

